[
  {
    "prompt": "summary: Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell–based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves highquality multi-view consistent renderings at a native resolution of 512 × 512 pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.",
    "chosen": "Gaussian Shell Maps (GSMs) are introduced to enhance 3D human generation efficiency. This framework combines generator network architectures with 3D Gaussian rendering primitives using an articulable multi-shell scaffold. A CNN generates a 3D texture stack, mapping features to shells representing inflated/deflated versions of a template surface. Instead of direct rasterization, 3D Gaussians are sampled on the shells, with attributes encoded in texture features. This method enables efficient, differentiable rendering, bypassing the need for view-inconsistent upsamplers and achieving high-quality multi-view consistent renderings at 512 × 512 pixels. GSMs successfully generate 3D humans when trained on single-view datasets like SHHQ and DeepFashion.",
    "rejected": "Efficient generation of 3D digital humans using Gaussian Shell Maps that connect generator networks with 3D Gaussian rendering primitives. This approach allows for fast, differentiable rendering and supports multi-view consistent outputs at high resolutions without requiring upsamplers."
  },
  {
    "prompt": "summary: Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-ﬁnders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to deﬁne and compute a notion of ‘conceptual similarity’ among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate ‘explanations’ (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be discriminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished. We operationalize this deﬁnition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity benchmarks. Beyond just providing a number, our method also offers interpretability by pointing to the speciﬁc level of granularity of the description where the source data are differentiated.",
    "chosen": "A novel method called Complexity-Constrained Descriptive Autoencoding (CC:DAE) is proposed to define and compute 'conceptual similarity' among images. This method uses a base multi-modal model to generate 'explanations' (captions) of visual data at increasing complexity levels. Similarity is measured by the caption length needed to discriminate between two images. CC:DAE correlates with subjective human assessments and outperforms existing baselines on image-to-image and text-to-text similarity benchmarks. It also offers interpretability by indicating the description granularity level where source data differentiation occurs.",
    "rejected": "This work introduces a method to quantify 'conceptual similarity' among images by generating explanations of increasing complexity. The length of these descriptions needed to differentiate between two images defines their similarity, providing an interpretable metric aligned with human assessments."
  },
  {
    "prompt": "summary: Vision-Language Models (VLMs) such as CLIP are trained on large amounts of image-text pairs, resulting in remarkable generalization across several data distributions. However, in several cases, their expensive training and data collection/curation costs do not justify the end application. This motivates a vendor-client paradigm, where a vendor trains a large-scale VLM and grants only input-output access to clients on a pay-per-query basis in a black-box setting. The client aims to minimize inference cost by distilling the VLM to a student model using the limited available task-specific data, and further deploying this student model in the downstream application. While naive distillation largely improves the In-Domain (ID) accuracy of the student, it fails to transfer the superior outof-distribution (OOD) generalization of the VLM teacher using the limited available labeled images. To mitigate this, we propose Vision-Language to Vision - Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and language modalities of the teacher model with the vision modality of a pre-trained student model, and further distills the aligned VLM representations to the student. This maximally retains the pre-trained features of the student, while also incorporating the rich representations of the VLM image encoder and the superior generalization of the text embeddings. The proposed approach achieves state-of-the-art results on the standard Domain Generalization benchmarks in a black-box teacher setting as well as a white-box setting where the weights of the VLM are accessible.",
    "chosen": "VL2V-ADiP is introduced to improve domain generalization in image classification by leveraging Vision-Language Models (VLMs). This method first aligns the vision and language modalities of a VLM teacher with a pre-trained student model's vision modality, then distills the aligned VLM representations to the student. This approach retains the student's pre-trained features while incorporating the rich representations of the VLM image encoder and superior generalization of text embeddings. VL2V-ADiP achieves state-of-the-art results on standard Domain Generalization benchmarks in both black-box and white-box teacher settings.",
    "rejected": "Vision-Language Models trained on large datasets achieve remarkable generalization but can be costly. A proposed distillation method called VL2V-ADiP aligns and transfers knowledge from a VLM teacher to a student model, maintaining pre-trained features while incorporating rich representations for improved out-of-distribution performance."
  },
  {
    "prompt": "summary: Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural radiance ﬁelds, i.e., the densities double when scene size is halved, and vice versa. We call this property alpha invariance. For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic initialization strategy to guarantee high ray transmittance. We revisit a few popular radiance ﬁeld models and ﬁnd that these systems use various heuristics to deal with issues arising from scene scaling. We test their behaviors and show our recipe to be more robust. Visit our",
    "chosen": "This paper addresses alpha invariance in Neural Radiance Fields (NeRFs), where scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities. To maintain alpha invariance, the authors recommend parameterizing both distance and volume densities in log space and using a discretization-agnostic initialization strategy for high ray transmittance. They revisit popular radiance field models, finding varied heuristics for handling scene scaling issues. Testing shows their approach is more robust across different NeRF methods and scene scalings.",
    "rejected": "Addressing scale-ambiguity in neural radiance fields, this paper recommends parameterizing distance and volume densities in log space for robustness. An initialization strategy ensuring high ray transmittance is also proposed, enhancing consistency across different scene scalings."
  },
  {
    "prompt": "summary: Unsupervised (US) video anomaly detection (VAD) in surveillance applications is gaining more popularity recently due to its practical real-world applications. As surveillance videos are privacy sensitive and the availability of large-scale video data may enable better US-VAD systems, collaborative learning can be highly rewarding in this setting. However, due to the extremely challenging nature of the US-VAD task, where learning is carried out without any annotations, privacy-preserving collaborative learning of US-VAD systems has not been studied yet. In this paper, we propose a new baseline for anomaly detection capable of localizing anomalous events in complex surveillance videos in a fully unsupervised fashion without any labels on a privacy-preserving participant-based distributed training configuration. Additionally, we propose three new evaluation protocols to benchmark anomaly detection approaches on various scenarios of collaborations and data availability. Based on these protocols, we modify existing VAD datasets to extensively evaluate our approach as well as existing US SOTA methods on two large-scale datasets including UCF-Crime and XD-Violence.",
    "chosen": "CLAP is proposed as a new baseline for unsupervised video anomaly detection in surveillance, capable of localizing anomalies without labels in a privacy-preserving distributed training setup. The authors also introduce three new evaluation protocols to benchmark anomaly detection approaches across various collaboration scenarios and data availabilities. They modify existing VAD datasets to extensively evaluate their approach and existing US SOTA methods on large-scale datasets like UCF-Crime and XD-Violence. All evaluation protocols, dataset splits, and codes are made available.",
    "rejected": "An unsupervised video anomaly detection technique suitable for privacy-preserving collaborative learning is introduced. It localizes anomalies in surveillance videos without labels and proposes new evaluation protocols tested on large-scale datasets including UCF-Crime and XD-Violence."
  },
  {
    "prompt": "summary: This paper addresses complex challenges in histopathological image analysis through three key contributions. Firstly, it introduces a fast patch selection method, FPS, for whole-slide image (WSI) analysis, significantly reducing computational cost while maintaining accuracy. Secondly, it presents PathDino, a lightweight histopathology feature extractor with a minimal configuration of five Transformer blocks and only ≈9 million parameters, markedly fewer than alternatives. Thirdly, it introduces a rotation-agnostic representation learning paradigm using self-supervised learning, effectively mitigating overfitting. We also show that our compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including both internal datasets spanning four sites (breast, liver, skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS, DigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training dataset of ≈6 million histopathology patches from The Cancer Genome Atlas (TCGA), our approach demonstrates an average 8.5% improvement in patch-level majority vote performance. These contributions provide a robust framework for enhancing image analysis in digital pathology, rigorously validated through extensive evaluation.",
    "chosen": "This paper presents three key contributions for histopathological image analysis: a fast patch selection method (FPS) for whole-slide image (WSI) analysis, PathDino (a lightweight histopathology feature extractor with five Transformer blocks and ≈9 million parameters), and a rotation-agnostic representation learning paradigm using self-supervised learning. The compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including internal and public datasets. It shows an average 8.5% improvement in patch-level majority vote performance with a training dataset of ≈6 million histopathology patches from TCGA.",
    "rejected": "A framework for histopathological image analysis through a fast patch selection method (FPS), PathDino feature extractor, and rotation-agnostic representation learning reduces computational costs and overfitting. The compact model outperforms state-of-the-art alternatives on various datasets, demonstrating significant improvements in accuracy."
  },
  {
    "prompt": "summary: Image and video analysis requires not only accurate object detection but also the understanding of relationships among detected objects. Common solutions to relation modeling typically resort to stand-alone object detectors followed by non-differentiable post-processing techniques. Recently introduced detection transformers (DETR) perform end-to-end object detection based on a bipartite matching loss. Such methods, however, lack the ability to jointly detect objects and resolve object associations. In this paper, we build on the DETR approach and extend it to the joint detection of objects and their relationships by introducing an approximated bipartite matching. While our method can generalize to an arbitrary number of objects, we here focus on the modeling of object pairs and their relations. In particular, we apply our method PairDETR to the problem of detecting human bodies and faces, and associating them for the same person. Our approach not only eliminates the need for hand-designed post-processing but also achieves excellent results for body-face associations. We evaluate PairDETR on the challenging CrowdHuman and CityPersons datasets and demonstrate a large improvement over the state of the art. Our training",
    "chosen": "PairDETR extends the DETR approach to jointly detect objects and resolve their associations by introducing an approximated bipartite matching. The method focuses on modeling object pairs and their relations, particularly for detecting human bodies and faces and associating them for the same person. PairDETR eliminates the need for hand-designed post-processing and achieves excellent results for body-face associations. It is evaluated on CrowdHuman and CityPersons datasets, demonstrating significant improvement over the state of the art.",
    "rejected": "Building upon DETR, PairDETR extends object detection to include relationship modeling via approximated bipartite matching. This end-to-end approach eliminates the need for post-processing steps and achieves superior results for detecting and associating human bodies and faces."
  },
  {
    "prompt": "summary: The reflective nature of the human eye is an underappreciated source of information about what the world around us looks like. By imaging the eyes of a moving person, we capture multiple views of a scene outside the camera’s direct line of sight through the reflections in the eyes. In this paper, we reconstruct a radiance field beyond the camera’s line of sight using portrait images containing eye reflections. This task is challenging due to 1) the difficulty of accurately estimating eye poses and 2) the entangled appearance of the iris textures and the scene reflections. To address these, our method jointly optimizes the cornea poses, the radiance field depicting the scene, and the observer’s eye iris texture. We further present a regularization prior on the iris texture to improve scene reconstruction quality. Through various experiments on synthetic and real-world captures featuring people with varied eye colors, and lighting conditions, we demonstrate the feasibility of our approach to recover the radiance field using cornea reflections. *Equal contribution",
    "chosen": "This paper reconstructs a radiance field beyond the camera’s line of sight using portrait images containing eye reflections. The method jointly optimizes cornea poses, the scene’s radiance field, and the observer’s eye iris texture, with a regularization prior on the iris texture for improved scene reconstruction quality. Experiments on synthetic and real-world captures show the approach’s feasibility in recovering the radiance field using cornea reflections, regardless of eye color and lighting conditions.",
    "rejected": "Reconstructing radiance fields beyond the camera's line of sight using eye reflections captured in portrait images. By optimizing cornea poses, radiance field, and iris texture, this method demonstrates feasibility in recovering scenes reflected in eyes under varied conditions."
  },
  {
    "prompt": "summary: We introduce FaceTalk1, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporallyoptimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the 1",
    "chosen": "FaceTalk is a novel generative approach for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio. It couples speech signals with the latent space of neural parametric head models, using a new latent diffusion model in the expression space of these models. In the absence of a dataset with NPHM expressions corresponding to audio, the authors optimize for these correspondences. FaceTalk generates plausible motion sequences for high-fidelity head animation, outperforming existing methods by 75% in perceptual user study evaluation.",
    "rejected": "FaceTalk1 synthesizes high-fidelity 3D motion sequences of talking heads from audio signals using a latent diffusion model in expression space. It couples speech with neural parametric head models to create realistic, temporally coherent sequences optimized from audio-video data."
  },
  {
    "prompt": "summary: Learning representations to capture the very fundamental understanding of the world is a key challenge in machine learning. The hierarchical structure of explanatory factors hidden in data is such a general representation and could be potentially achieved with a hierarchical VAE. However, training a hierarchical VAE always suffers from the “posterior collapse”, where the data information is hard to propagate to the higher-level latent variables, hence resulting in a bad hierarchical representation. To address this issue, we first analyze the shortcomings of existing methods for mitigating the posterior collapse from an information theory perspective, then highlight the necessity of regularization for explicitly propagating data information to higher-level latent variables while maintaining the dependency between different levels. This naturally leads to formulating the inference of the hierarchical latent representation as a sequential decision process, which could benefit from applying reinforcement learning (RL). Aligning RL’s objective with the regularization, we first introduce a skip-generative path to acquire a reward for evaluating the information content of an inferred latent representation, and then the developed Q-value function based on it could have a consistent optimization direction of the regularization. Finally, policy gradient, one of the typical RL methods, is employed to train a hierarchical VAE without introducing a gradient estimator. Experimental results firmly support our analysis and demonstrate that our proposed method effectively mitigates the posterior collapse issue, learns an informative hierarchy, acquires explainable latent representations, and significantly outperforms other hierarchical VAE-based methods in downstream tasks.",
    "chosen": "A method combining reinforcement learning (RL) is proposed to improve unsupervised hierarchical representation learning in hierarchical VAEs, addressing the posterior collapse issue. The approach involves analyzing existing methods' shortcomings, emphasizing regularization for data information propagation to higher-level latent variables, and formulating hierarchical latent representation inference as a sequential decision process. RL is applied using a skip-generative path for reward acquisition and a Q-value function for consistent optimization. Policy gradient trains the hierarchical VAE without introducing a gradient estimator. Experimental results show the method effectively mitigates posterior collapse, learns informative hierarchies, and significantly outperforms other hierarchical VAE-based methods in downstream tasks.",
    "rejected": "Innovative methods in generating 3D digital humans, measuring image similarity, improving Vision-Language Models, addressing scale ambiguities, detecting anomalies, analyzing histopathological images, extending object detection, reconstructing scenes, and synthesizing head motions are highlighted. Each addresses specific challenges within its domain."
  },
  {
    "prompt": "summary: Human perception and understanding is a major domain of computer vision which, like many other vision subdomains recently, stands to gain from the use of large models pre-trained on large datasets. We hypothesize that the most common pre-training strategy of relying on general purpose, object-centric image datasets such as ImageNet, is limited by an important domain shift. On the other hand, collecting domain-specific ground truth such as 2D or 3D labels does not scale well. Therefore, we propose a pre-training approach based on self-supervised learning that works on human-centric data using only images. Our method uses pairs of images of humans: the first is partially masked and the model is trained to reconstruct the masked parts given the visible ones and a second image. It relies on both stereoscopic (cross-view) pairs, and temporal (cross-pose) pairs taken from videos, in order to learn priors about 3D as well as human motion. We pre-train a model for body-centric tasks and one for hand-centric tasks. With a generic transformer architecture, these models outperform existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and obtain state-of-the-art performance for instance when fine-tuning for model-based and model-free human mesh recovery.",
    "chosen": "A self-supervised pre-training method for human-centric vision tasks is proposed, using pairs of human images with masking and reconstruction. It leverages stereoscopic and temporal pairs to learn 3D and motion priors, outperforming existing methods on human-centric downstream tasks.",
    "rejected": "We propose a self-supervised learning approach for human-centric tasks using transformer models. By training on image pairs of humans with one partially masked, our method learns from cross-view and cross-pose data, outperforming existing techniques in human mesh recovery."
  },
  {
    "prompt": "summary: We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video’s current path of execution in a certain way, the goal is to find a related “detour video” that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to’s using video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model’s significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%.",
    "chosen": "VidDetours is introduced to solve the video detours problem, retrieving targeted temporal segments from how-to videos using video-and-text conditioned queries. It creates weakly supervised training data from video narration text, significantly improving video retrieval and question answering in cooking videos.",
    "rejected": "Introducing VidDetours, a novel video-language model designed to navigate instructional videos by retrieving related segments based on user queries. This system leverages weakly supervised training data derived from how-to video narrations, significantly improving recall rates over state-of-the-art methods."
  },
  {
    "prompt": "summary: Generative AI (GenAI) is transforming creative workflows through the capability to synthesize and manipulate images via high-level prompts. Yet creatives are not well supported to receive recognition or reward for the use of their content in GenAI training. To this end, we propose ProMark, a causal attribution technique to attribute a synthetically generated image to its training data concepts like objects, motifs, templates, artists, or styles. The concept information is proactively embedded into the input training images using imperceptible watermarks, and the diffusion models (unconditional or conditional) are trained to retain the corresponding watermarks in generated images. We show that we can embed as many as 216 unique watermarks into the training data, and each training image can contain more than one watermark. ProMark can maintain image quality whilst outperforming correlation-based attribution. Finally, several qualitative examples are presented, providing the confidence that the presence of the watermark conveys a causative relationship between training data and synthetic images.",
    "chosen": "ProMark is a causal attribution technique for GenAI, embedding watermarks in training images to attribute synthetic images to their training data concepts. It outperforms correlation-based attribution while maintaining image quality.",
    "rejected": "ProMark is a causal attribution technique that embeds imperceptible watermarks into training images for generative AI models. These watermarks allow attributing synthetic images back to their original concepts, providing recognition or reward to creatives whose work contributed to the training dataset."
  },
  {
    "prompt": "summary: Determining the location of an image anywhere on Earth is a complex visual task, which makes it particularly relevant for evaluating computer vision algorithms. Yet, the absence of standard, large-scale, open-access datasets with reliably localizable images has limited its potential. To address this issue, we introduce OpenStreetView-5M, a largescale, open-access dataset comprising over 5.1 million georeferenced street view images, covering 225 countries and territories. In contrast to existing benchmarks, we enforce a strict train/test separation, allowing us to evaluate the relevance of learned geographical features beyond mere memorization. To demonstrate the utility of our dataset, we conduct an extensive benchmark of various state-of-theart image encoders, spatial representations, and training strategies. All associated codes and models can be found at github.com/gastruc/osv5m.",
    "chosen": "OpenStreetView-5M, a large-scale, open-access geolocation dataset with over 5.1 million geo-referenced street view images, is introduced. It enables evaluation of geographical feature learning beyond memorization, benchmarking various image encoders, spatial representations, and training strategies.",
    "rejected": "OpenStreetView-5M (OSV-5M) offers a large-scale open-access dataset of 5.1 million georeferenced street view images for global geolocation tasks. The dataset enables robust evaluation of computer vision algorithms beyond memorization, enhancing geographical feature learning."
  },
  {
    "prompt": "summary: We introduce SAOR, a novel approach for estimating the 3D shape, texture, and viewpoint of an articulated object from a single image captured in the wild. Unlike prior approaches that rely on pre-defined category-specific 3D templates or tailored 3D skeletons, SAOR learns to articulate shapes from single-view image collections with a skeleton-free part-based model without requiring any 3D object shape priors. To prevent ill-posed solutions, we propose a cross-instance consistency loss that exploits disentangled object shape deformation and articulation. This is helped by a new silhouette-based sampling mechanism to enhance viewpoint diversity during training. Our method only requires estimated object silhouettes and relative depth maps from off-the-shelf pre-trained networks during training. At inference time, given a single-view image, it efficiently outputs an explicit mesh representation. We obtain improved qualitative and quantitative results on challenging quadruped animals compared to relevant existing work.",
    "chosen": "SAOR is a novel approach for estimating 3D shape, texture, and viewpoint of articulated objects from a single image. It uses a skeleton-free part-based model with a cross-instance consistency loss and silhouette-based sampling, requiring only object silhouettes and depth maps during training.",
    "rejected": "SAOR introduces a novel approach for estimating 3D shape, texture, and viewpoint of articulated objects from single images. Without relying on predefined 3D templates, SAOR uses a skeleton-free part-based model, achieving improved results compared to existing methods on challenging categories like quadruped animals."
  },
  {
    "prompt": "summary: In this work, we study a novel problem which focuses on person identification while performing daily activities. Learning biometric features from RGB videos is challenging due to spatio-temporal complexity and presence of appearance biases such as clothing color and background. We propose ABNet, a novel framework which leverages disentanglement of biometric and non-biometric features to perform effective person identification from daily activities. ABNet relies on a bias-less teacher to learn biometric features from RGB videos and explicitly disentangle nonbiometric features with the help of biometric distortion. In addition, ABNet also exploits activity prior for biometrics which is enabled by joint biometric and activity learning. We perform comprehensive evaluation of the proposed approach across five different datasets which are derived from existing activity recognition benchmarks. Furthermore, we extensively compare ABNet with existing works in person identification and demonstrate its effectiveness for activitybased biometrics across all five datasets.",
    "chosen": "ABNet is a framework for person identification from daily activities, disentangling biometric and non-biometric features using a bias-less teacher and biometric distortion. It exploits activity priors through joint biometric and activity learning, evaluated across five datasets.",
    "rejected": "ABNet leverages disentanglement of biometric and non-biometric features to perform person identification from daily activities. It utilizes a bias-less teacher model and exploits activity priors for effective biometric feature learning, demonstrating superior performance across five datasets."
  },
  {
    "prompt": "summary: Despite the growing demand for accurate surface normal estimation models, existing methods use generalpurpose dense prediction models, adopting the same inductive biases as other tasks. In this paper, we discuss the inductive biases needed for surface normal estimation and propose to (1) utilize the per-pixel ray direction and (2) en",
    "chosen": "A method rethinking inductive biases for surface normal estimation is proposed, utilizing per-pixel ray direction and encoding relative rotation between neighboring surface normals. It shows stronger generalization ability despite being trained on a smaller dataset.",
    "rejected": "This paper discusses inductive biases necessary for surface normal estimation and proposes DSINE, a method utilizing per-pixel ray direction and relative rotation between neighboring normals. DSINE shows stronger generalization ability despite being trained on a smaller dataset compared to state-of-the-art models."
  },
  {
    "prompt": "summary: Recent breakthroughs in text-to-4D generation rely on pre-trained text-to-image and text-to-video models to generate dynamic 3D scenes. However, current text-to-4D methods face a three-way tradeoff between the quality of scene appearance, 3D structure, and motion. For example, textto-image models and their 3D-aware variants are trained on internet-scale image datasets and can be used to produce scenes with realistic appearance and 3D structure—but no motion. Text-to-video models are trained on relatively smaller video datasets and can produce scenes with motion, but poorer appearance and 3D structure. While these models have complementary strengths, they also have opposing weaknesses, making it difficult to combine them in a way that alleviates this three-way tradeoff. Here, we introduce hybrid score distillation sampling, an alternating optimization procedure that blends supervision signals from multiple pre-trained diffusion models and incorporates benefits of each for high-fidelity text-to-4D generation. Using hybrid SDS, we demonstrate synthesis of 4D scenes with compelling appearance, 3D structure, and motion.",
    "chosen": "4D-fy is a text-to-4D generation method using hybrid score distillation sampling, blending supervision signals from multiple pre-trained diffusion models. It synthesizes 4D scenes with compelling appearance, 3D structure, and motion.",
    "rejected": "Hybrid score distillation sampling combines supervision signals from multiple pre-trained diffusion models for high-fidelity text-to-4D generation. This method synthesizes dynamic scenes with compelling appearance, 3D structure, and motion, addressing tradeoffs faced by current text-to-4D methods."
  },
  {
    "prompt": "summary: Automating visual inspection in industrial production lines is essential for increasing product quality across various industries. Anomaly detection (AD) methods serve as robust tools for this purpose. However, existing public datasets primarily consist of images without anomalies, limiting the practical application of AD methods in production settings. To address this challenge, we present (1) the Valeo Anomaly Dataset (VAD), a novel real-world industrial dataset comprising 5000 images, including 2000 instances of challenging real defects across more than 20 subclasses. Acknowledging that traditional AD methods struggle with this dataset, we introduce (2) Segmentationbased Anomaly Detector (SegAD). First, SegAD leverages anomaly maps as well as segmentation maps to compute local statistics. Next, SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier, yielding the final anomaly score. Our SegAD achieves state-of-the-art performance on both VAD (+2.1% AUROC) and the VisA dataset (+0.4% AUROC).",
    "chosen": "SegAD is introduced for supervised anomaly detection in industrial images, using anomaly and segmentation maps with a Boosted Random Forest classifier. It achieves state-of-the-art performance on the Valeo Anomaly Dataset and VisA dataset.",
    "rejected": "The Valeo Anomaly Dataset (VAD) and Segmentation-based Anomaly Detector (SegAD) are introduced for industrial anomaly detection. SegAD computes local statistics from anomaly and segmentation maps, achieving state-of-the-art performance on VAD and VisA datasets."
  },
  {
    "prompt": "summary: Contrastive Vision-Language Pre-training, known as CLIP, has shown promising effectiveness in addressing downstream image recognition tasks. However, recent works revealed that the CLIP model can be implanted with a downstream-oriented backdoor. On downstream tasks, one victim model performs well on clean samples but predicts a specific target class whenever a specific trigger is present. For injecting a backdoor, existing attacks depend on a large amount of additional data to maliciously fine-tune the entire pre-trained CLIP model, which makes them inapplicable to data-limited scenarios. In this work, motivated by the recent success of learnable prompts, we address this problem by injecting a backdoor into the CLIP model in the prompt learning stage. Our method named BadCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP, i.e., influencing both the image and text encoders with the trigger. It consists of a learnable trigger applied to images and a trigger-aware context generator, such that the trigger can change text features via trigger-aware prompts, resulting in a powerful and generalizable attack. Extensive experiments conducted on 11 datasets verify that the clean accuracy of BadCLIP is similar to those of advanced prompt learning methods and the attack success rate is higher than 99% in most cases. BadCLIP is also generalizable to unseen classes, and shows a strong generalization capability under cross-dataset and cross-domain settings.",
    "chosen": "BadCLIP is a backdoor attack on CLIP models during prompt learning, influencing both image and text encoders with a trigger. It achieves high attack success rates while maintaining clean accuracy and generalizing to unseen classes and domains.",
    "rejected": "Our research focuses on person identification during routine activities, proposing ABNet to disentangle biometric and non-biometric features using RGB videos. This framework enhances identification accuracy beyond traditional gait or facial recognition methods, showing effectiveness across diverse datasets."
  },
  {
    "prompt": "summary: Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tuning without labor-intensive annotation. Extensive experiments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels.",
    "chosen": "DiPrompT is a novel federated learning approach for domain generalization that learns adaptive prompts in a distributed manner, eliminating the one-to-one mapping restriction between source domains and local clients. It introduces global and domain prompts to capture general and domain-specific knowledge, and a dynamic query metric to automatically search suitable domain labels. Extensive experiments show DiPrompT outperforms state-of-the-art FL methods without domain labels and even surpasses centralized methods using domain labels.",
    "rejected": "Federated learning (FL) has emerged as a method for decentralized data learning. DiPrompT proposes adaptive prompts for domain generalization in FL without explicit domain labels. It introduces global and domain-specific prompts, enhancing performance on unseen domains. The approach achieves superior results compared to centralized methods."
  },
  {
    "prompt": "summary: 3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining finegrained controllability and high rendering quality. Our key idea lies in the",
    "chosen": "A novel fast 3D neural implicit head avatar model is proposed, achieving real-time rendering while maintaining fine-grained controllability and high quality. It uses local hash table blendshapes attached to an underlying face parametric model, with per-vertex hash-tables merged via CNN-predicted weights. This enables efficient density and color predictions using a lightweight MLP accelerated by a hierarchical nearest neighbor search. Experiments show real-time performance with comparable rendering quality to state-of-the-arts.",
    "rejected": "A novel 3D neural implicit head avatar model is introduced, achieving real-time rendering while maintaining high quality and controllability. By using local hash table blendshapes and an efficient MLP, it ensures fine-grained expression control and fast performance suitable for dynamic facial performances in VR."
  },
  {
    "prompt": "summary: We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of ﬁxed point solving into the framework of diffusionbased generative modeling. Our approach embeds an implicit ﬁxed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related ﬁxed point problems. Combined with a new stochastic training method, this approach signiﬁcantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efﬁciency: reallocating computation across timesteps and reusing ﬁxed point solutions between timesteps. We conduct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in performance and efﬁciency. Compared to the state-of-the-art DiT model [38], FPDM contains 87% fewer parameters, consumes 60% less memory during training, and improves image generation *Equal Contribution. †Corresponding author. quality in situations where sampling computation or time is limited. Our",
    "chosen": "The Fixed Point Diffusion Model (FPDM) integrates fixed point solving into diffusion-based generative modeling, embedding an implicit fixed point layer in the denoising network. Combined with stochastic training, it reduces model size, memory usage, and accelerates training. It also improves sampling efficiency through computation reallocation and solution reuse. Experiments on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church show FPDM significantly improves performance and efficiency, with 87% fewer parameters and 60% less memory than DiT.",
    "rejected": "The Fixed Point Diffusion Model (FPDM) integrates fixed point solving into diffusion-based generative modeling, reducing model size and memory usage. This approach accelerates training and improves sampling efficiency, offering significant improvements over state-of-the-art models like DiT."
  },
  {
    "prompt": "summary: Out-of-distribution (OOD) detection methods often exploit auxiliary outliers to train model identifying OOD samples, especially discovering challenging outliers from auxiliary outliers dataset to improve OOD detection. However, they may still face limitations in effectively distinguishing between the most challenging OOD samples that are much like in-distribution (ID) data, i.e., ID-like samples. To this end, we propose a novel OOD detection framework that discovers ID-like outliers using CLIP [32] from the vicinity space of the ID samples, thus helping to identify these most challenging OOD samples. Then a prompt learning framework is proposed that utilizes the identified ID-like outliers to further leverage the capabilities of CLIP for OOD detection. Benefiting from the powerful CLIP, we only need a small number of ID samples to learn the prompts of the model without exposing other auxiliary outlier datasets. By focusing on the most challenging ID-like OOD samples and elegantly exploiting the capabilities of CLIP, our method achieves superior few-shot learning performance on various real-world image datasets (e.g., in 4-shot OOD detection on the ImageNet-1k dataset, our method reduces the average FPR95 by 12.16% and improves the average AUROC by 2.76%, compared to state-of-the-art methods).",
    "chosen": "A novel OOD detection framework discovers ID-like outliers using CLIP from the vicinity of ID samples, then uses prompt learning to leverage CLIP's capabilities for OOD detection. This method focuses on challenging ID-like OOD samples, requiring few ID samples to learn prompts without auxiliary outlier datasets. Extensive experiments demonstrate superior few-shot learning performance, reducing average FPR95 by 12.16% and improving AUROC by 2.76% on ImageNet-1k in 4-shot OOD detection.",
    "rejected": "A new OOD detection framework leveraging CLIP identifies ID-like outliers to improve detection accuracy. Prompt learning with these outliers enhances CLIP's capabilities, demonstrating superior few-shot learning performance on various datasets by significantly reducing FPR95 and improving AUROC."
  },
  {
    "prompt": "summary: We introduce a novel sequential modeling approach which enables learning a Large Vision Model (LVM) without making use of any linguistic data. To do this, we deﬁne a common format, “visual sentences”, in which we can represent raw images and videos as well as annotated data sources such as semantic segmentations and depth reconstructions without needing any meta-knowledge beyond the pixels. Once this wide variety of visual data (comprising 420 billion tokens) is represented as sequences, the model can be trained to minimize a cross-entropy loss for next token prediction. By training across various scales of model architecture and data diversity, we provide empirical evidence that our models scale effectively. Many different vision tasks can be solved by designing suitable visual prompts at test time.",
    "chosen": "A sequential modeling approach enables learning a Large Vision Model (LVM) using only visual data, defining \"visual sentences\" to represent diverse visual data as sequences. A 3 billion parameter transformer trained on 64 billion images minimizes cross-entropy loss for next token prediction. This model demonstrates effective scaling, solves various vision tasks via test-time prompts, benefits from unsupervised data, and shows potential for general visual reasoning.",
    "rejected": "Introducing a Large Vision Model (LVM) trained without linguistic data, this work defines \"visual sentences\" to represent diverse visual data formats. LVM scales effectively across different architectures and data diversities, solving many vision tasks through prompt design at test time."
  },
  {
    "prompt": "summary: This paper presents DriveTrack, a new benchmark and data generation framework for long-range  key point tracking in real-world videos. DriveTrack is motivated by the observation that the accuracy of state-of-the-art trackers depends strongly on visual attributes around the selected  key points, such as texture and lighting. The problem is that these artifacts are especially pronounced in real-world videos, but these trackers are unable to train on such scenes due to a dearth of annotations. DriveTrack bridges this gap by building a framework to automatically annotate point tracks on autonomous driving datasets. We release a dataset consisting of 1 billion point tracks across 24 hours of video, which is seven orders of magnitude greater than prior real-world benchmarks and on par with the scale of synthetic benchmarks. DriveTrack unlocks new use cases for point tracking in real-world videos. First, we show that finetuning  key point trackers on DriveTrack improves accuracy on real-world scenes by up to 7%. Second, we analyze the sensitivity of trackers to visual artifacts in real scenes and motivate the idea of running assistive  key point selectors alongside trackers.",
    "chosen": "DriveTrack is a benchmark and data generation framework for long-range  key point tracking in real-world videos. It automatically annotates point tracks on autonomous driving datasets, releasing 1 billion point tracks across 24 hours of video. This improves tracker accuracy on real-world scenes by up to 7% and analyzes tracker sensitivity to visual artifacts, motivating assistive  key point selectors.",
    "rejected": "DriveTrack provides a benchmark for long-range  key point tracking in real-world videos, addressing limitations of synthetic datasets. Fine-tuning trackers on DriveTrack improves real-world scene accuracy and analyzes tracker sensitivity to visual artifacts, proposing assistive  key point selectors."
  },
  {
    "prompt": "summary: Sign Languages (SL) serve as the primary mode of communication for the Deaf and Hard of Hearing communities. Deep learning methods for SL recognition and translation have achieved promising results. However, Sign Language Production (SLP) poses a challenge as the generated motions must be realistic and have precise semantic meaning. Most SLP methods rely on 2D data, which hinders their realism. In this work, a diffusion-based SLP model is trained on a curated large-scale dataset of 4D signing avatars and their corresponding text transcripts. The proposed method can generate dynamic sequences of 3D avatars from an unconstrained domain of discourse using a diffusion process formed on a novel and anatomically informed graph neural network defined on the SMPL-X body skeleton. Through quantitative and qualitative experiments, we show that the proposed method considerably outperforms previous methods of SLP. This work makes an important step towards realistic neural sign avatars, bridging the communication gap between Deaf and hearing communities.1 1",
    "chosen": "A diffusion-based Sign Language Production (SLP) model is trained on a large-scale 4D signing avatar dataset. It generates dynamic 3D avatar sequences using a diffusion process on a novel graph neural network defined on the SMPL-X body skeleton. Experiments show significant improvement over previous methods, advancing realistic neural sign avatars and bridging the communication gap between Deaf and hearing communities.",
    "rejected": "A diffusion-based Sign Language Production (SLP) model generates realistic 3D avatar motions from text transcripts. Using an anatomically informed graph neural network, it outperforms previous SLP methods, marking a significant step towards bridging communication gaps between Deaf and hearing communities."
  },
  {
    "prompt": "summary: Recent advances in large-scale pretraining have yielded visual foundation models with strong capabilities. Not only can recent models generalize to arbitrary images for their training task, their intermediate representations are useful for other visual tasks such as detection and segmentation. Given that such models can classify, delineate, and localize objects in 2D, we ask whether they also represent their 3D structure? In this work, we analyze the 3D awareness of visual foundation models. We posit that 3D awareness implies that representations (1) en",
    "chosen": "This work analyzes the 3D awareness of visual foundation models through task-specific probes and zero-shot inference. It evaluates whether representations encode 3D structure and maintain multiview consistency. Experiments reveal several limitations in current models, indicating further research is needed to enhance their 3D understanding.",
    "rejected": "This paper explores the 3D awareness of visual foundation models. Through experiments on task-specific probes, it reveals that current models lack consistent 3D representation across views, despite their strong 2D capabilities. The study highlights the need for improved 3D understanding in these models."
  },
  {
    "prompt": "summary: We propose SketchINR, to advance the representation of vector sketches with implicit neural models. A variable length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes. The learned function predicts the xy point coordinates in a sketch at each time and stroke. Despite its simplicity, SketchINR outperforms existing representations at multiple tasks: (i) Encoding an entire sketch dataset into a fixed size latent vector, SketchINR gives 60× and 10× data compression over raster and vector sketches, respectively. (ii) SketchINR’s auto-decoder provides a much higher-fidelity representation than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO. (iii) SketchINR supports parallelisation that can decode/render ∼100× faster than other learned vector representations such as SketchRNN. (iv) SketchINR, for the first time, emulates the human ability to reproduce a sketch with varying abstraction in terms of number and complexity of strokes. As a first look at implicit sketches, SketchINR’s compact high-fidelity representation will support future work in modelling long and complex sketches.",
    "chosen": "SketchINR advances vector sketch representation with implicit neural models, compressing sketches into a latent space encoding shape as a function of time and strokes. It outperforms existing representations in data compression, fidelity, scalability, and rendering speed, supporting future work in modeling complex sketches.",
    "rejected": "SketchINR advances vector sketch representations using implicit neural models. It compresses variable-length sketches into a fixed latent space, enabling the encoding of shape details over time. This approach facilitates more effective processing and manipulation of vector sketches for various applications."
  },
  {
    "prompt": "summary: Pre-trained Vision Language Models (VLMs) have demonstrated notable progress in various zero-shot tasks, such as classiﬁcation and retrieval. Despite their performance, because improving performance on new tasks requires task-speciﬁc knowledge, their adaptation is essential. While labels are needed for the adaptation, acquiring them is typically expensive. To overcome this challenge, active learning, a method of achieving a high performance by obtaining labels for a small number of samples from experts, has been studied. Active learning primarily focuses on selecting unlabeled samples for labeling and leveraging them to train models. In this study, we pose the question, “how can the pre-trained VLMs be adapted under the active learning framework?” In response to this inquiry, we observe that (1) simply applying a conventional active learning framework to pre-trained VLMs even may degrade performance compared to random selection because of the class imbalance in labeling candidates, and (2) the knowledge of VLMs can provide hints for achieving the balance before labeling. Based on these observations, we devise a novel active learning framework for VLMs, denoted as PCB. To assess the effectiveness of our approach, we conduct experiments on seven different real-world datasets, and the results demonstrate that PCB surpasses conventional active learning and random sampling methods.",
    "chosen": "A novel active learning framework for pre-trained Vision Language Models (VLMs) is proposed, addressing class imbalance issues in labeling candidates. This framework, called PCB, leverages VLM knowledge before labeling to achieve better balance. Experiments on seven real-world datasets show PCB surpasses conventional active learning and random sampling methods.",
    "rejected": "SketchINR advances the representation of vector sketches through implicit neural models. It compresses variable-length sketches into a fixed-dimension latent space, encoding the underlying shape as a function of time and structure. This method facilitates efficient processing and manipulation of vector sketches, enhancing their application in various domains."
  },
  {
    "prompt": "summary: Despite being (pre)trained on a massive amount of data, state-of-the-art video-language alignment models are not robust to semantically-plausible contrastive changes in the video captions. Our work addresses this by identifying a broad spectrum of contrast misalignments, such as replacing entities, actions, and flipping event order, which alignment models should be robust against. To this end, we introduce the VideoCon, a video-language alignment dataset constructed by a large language model that generates plausible contrast video captions and explanations for differences between original and contrast video captions. Then, a generative video-language model is finetuned with VideoCon to assess video-language entailment and generate explanations. Our VideoCon-based alignment model significantly outperforms current models. It exhibits a 12-point increase in AUC for the video-language alignment task on human-generated contrast captions. Finally, our model sets new state of the art zero-shot performance in temporally-extensive video-language tasks such as textto-video retrieval (SSv2-Temporal) and video question answering (ATP-Hard). Moreover, our model shows superior performance on novel videos and human-crafted captions and explanations.",
    "chosen": "VideoCon is a video-language alignment dataset constructed by a large language model, generating plausible contrast video captions and explanations for differences. A generative video-language model fine-tuned with VideoCon significantly outperforms current models, achieving a 12-point increase in AUC for the video-language alignment task and setting new state-of-the-art zero-shot performance in temporally-extensive video-language tasks.",
    "rejected": "In addressing video-language alignment robustness, VideoCon dataset is introduced, containing semantically plausible contrastive captions. Fine-tuned models on VideoCon show significant improvement in AUC and zero-shot performance on tasks like text-to-video retrieval and video question answering."
  },
  {
    "prompt": "summary: Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMMdriving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints.",
    "chosen": "GeneAvatar is a generic avatar editing approach for various 3DMM-driven volumetric head avatars. It uses an expression-aware modification generative model to lift 2D editing from a single image to a consistent 3D modification field. The method includes techniques like expression-dependent modification distillation, implicit latent space guidance, and segmentation-based loss reweighting, delivering high-quality and consistent results across multiple expressions and viewpoints.",
    "rejected": "GeneAvatar proposes a novel approach to edit 3D head avatars across various volumetric representations. It employs an expression-aware modification generative model that ensures consistency across expressions and viewpoints, supporting both intensive and delicate editing from single-perspective images."
  },
  {
    "prompt": "summary: Adversarial attacks aim to perturb images such that a predictor outputs incorrect results. Due to the limited research in structured attacks, imposing consistency checks on natural multi-object scenes is a practical defense against conventional adversarial attacks. More desired attacks should be able to fool defenses with such consistency checks. Therefore, we present the first approach GLOW that copes with various attack requests by generating global layout-aware adversarial attacks, in which both categorical and geometric layout constraints are explicitly established. Specifically, we focus on object detection tasks and given a victim image, GLOW first localizes victim objects according to target labels. And then it generates multiple attack plans, together with their context-consistency scores. GLOW, on the one hand, is capable of handling various types of requests, including single or multiple victim objects, with or without specified victim objects. On the other hand, it produces a consistency score for each attack plan, reflecting the overall contextual consistency that both semantic category and global scene layout are considered. We conduct our experiments on MS COCO and Pascal. Extensive experimental results demonstrate that we can achieve about 30% average relative improvement compared to state-of-the-art methods in conventional single object attack request; Moreover, such superiority is also valid across more generic attack requests, under both whitebox and zero-query black-box settings. Finally, we conduct comprehensive human analysis, which not only validates our claim further but also provides strong evidence that our evaluation metrics reflect human reviews well.",
    "chosen": "GLOW is the first approach to generate global layout-aware adversarial attacks on object detection, establishing categorical and geometric layout constraints. It localizes victim objects and generates multiple attack plans with context-consistency scores. Experiments on MS COCO and Pascal show GLOW achieves about 30% average relative improvement compared to state-of-the-art methods in conventional single object attack requests and is valid across more generic attack requests under both white-box and zero-query black-box settings.",
    "rejected": "GLOW introduces a global layout-aware adversarial attack method targeting object detection tasks. By generating attacks with categorical and geometric constraints, GLOW achieves about 30% average relative improvement over state-of-the-art methods, proving effective under white-box and black-box settings."
  },
  {
    "prompt": "summary: Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-ofthe-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.",
    "chosen": "FlowMDM is the first diffusion-based model generating seamless Human Motion Compositions without postprocessing. It uses Blended Positional Encodings for global motion coherence and smooth transitions, achieving state-of-the-art results on Babel and HumanML3D datasets. FlowMDM is robust against varying text descriptions at inference time due to its Pose-Centric Cross-ATtention.",
    "rejected": "FlowMDM introduces diffusion-based human motion generation driven by textual descriptions. Utilizing blended positional encodings, it achieves seamless transitions between actions, demonstrating state-of-the-art results in accuracy, realism, and smoothness on benchmarks like Babel and HumanML3D."
  },
  {
    "prompt": "summary: Training a linear classifier or lightweight model on top of pretrained vision model outputs, so-called ‘frozen features’, leads to impressive performance on a number of downstream few-shot tasks. Currently, frozen features are not modified during training. On the other hand, when networks are trained directly on images, data augmentation is a standard recipe that improves performance with no substantial overhead. In this paper, we conduct an extensive pilot study on few-shot image classification that explores applying data augmentations in the frozen feature space, dubbed ‘frozen feature augmentation (FroFA)’, covering twenty augmentations in total. Our study demonstrates that adopting a deceptively simple pointwise FroFA, such as brightness, can improve few-shot performance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets.",
    "chosen": "FroFA explores applying data augmentations in the frozen feature space for few-shot image classification. The study shows that simple pointwise augmentations like brightness can consistently improve few-shot performance across different network architectures, pre-training datasets, and transfer datasets.",
    "rejected": "Frozen feature augmentation (FroFA) applies data augmentations to frozen features from pretrained vision models, improving few-shot learning performance. Simple point-wise augmentations such as brightness consistently enhance performance across multiple architectures and datasets."
  },
  {
    "prompt": "summary: Non-isometric shape correspondence remains a fundamental challenge in computer vision. Traditional methods using Laplace-Beltrami operator (LBO) eigenmodes face limitations in characterizing high-frequency extrinsic shape changes like bending and creases. We propose a novel approach of combining the non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell hessian with the intrinsic ones of the LBO, creating a hybrid spectral space in which we construct functional maps. To this end, we present a theoretical framework to effectively integrate nonorthogonal basis functions into descriptor- and learningbased functional map methods. Our approach can be incorporated easily into existing functional map pipelines across varying applications and can handle complex deformations beyond isometries. We show extensive evaluations across various supervised and unsupervised settings and demonstrate significant improvements. Notably, our approach achieves up to 15% better mean geodesic error for non-isometric correspondence settings and up to 45% improvement in scenarios with topological noise.",
    "chosen": "This novel approach combines non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell hessian with intrinsic Laplace-Beltrami operator eigenmodes to create a hybrid spectral space for functional maps. It effectively integrates non-orthogonal basis functions into descriptor- and learning-based methods, handling complex deformations beyond isometries and showing significant improvements in non-isometric correspondence settings and scenarios with topological noise.",
    "rejected": "A novel shape correspondence approach integrates non-orthogonal extrinsic basis functions with intrinsic Laplace-Beltrami operator eigenmodes. This hybrid spectral space method shows up to 15% better mean geodesic error for non-isometric shapes and handles topological noise effectively."
  },
  {
    "prompt": "summary: Point cloud matching, a crucial technique in computer vision, medical and robotics ﬁelds, is primarily concerned with ﬁnding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights about the paired geometries. In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account ﬁne local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian1, that allows to easily generate aligned eigenspaces for multiple registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We ﬁrstly evaluate the performance of the proposed technique in a point-wise manner, focusing on the task of object anomaly localization on the MVTec 3D-AD dataset. Additionally, we deﬁne a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a benchmark collecting bone surface structures from various public datasets. Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks.",
    "chosen": "A new technique based on graph Laplacian eigenmaps is proposed for point cloud matching, introducing a Coupled Laplacian operator to generate aligned eigenspaces for multiple registered geometries. The similarity between aligned high-dimensional spaces provides a locally meaningful score for shape matching, outperforming other methods in object anomaly localization and automatic Bone Side Estimation tasks.",
    "rejected": "Using graph Laplacian eigenmaps, a new point cloud matching technique addresses local structure differences. The Coupled Laplacian operator aligns eigenspaces, providing meaningful similarity scores. It outperforms existing methods in object anomaly localization and introduces Bone Side Estimation for medical applications."
  },
  {
    "prompt": "summary: In recent years, automated Gallbladder Cancer (GBC) detection has gained the attention of researchers. Current state-of-the-art (SOTA) methodologies relying on ultrasound sonography (US) images exhibit limited generalization, emphasizing the need for transformative approaches. We observe that individual US frames may lack sufficient information to capture disease manifestation. This study advocates for a paradigm shift towards video-based GBC detection, leveraging the inherent advantages of spatiotemporal representations. Employing the Masked Autoencoder (MAE) for representation learning, we address shortcomings in conventional image-based methods. We propose a novel design called FocusMAE to systematically bias the selection of masking tokens from high-information regions, fostering a more refined representation of malignancy. Additionally, we contribute the most extensive US video dataset for GBC detection. We also note that, this is the first study on US video-based GBC detection. We validate the proposed methods on the curated dataset, and report a new SOTA accuracy of 96.4% for the GBC detection problem, against an accuracy of 84% by current Image-based SOTA – GBCNet and RadFormer, and 94.7% by Video-based SOTA – AdaMAE. We further demonstrate the generality of the proposed FocusMAE on a public CTbased Covid detection dataset, reporting an improvement in accuracy by 3.3% over current baselines.",
    "chosen": "FocusMAE is a novel design for video-based Gallbladder Cancer detection, systematically biasing masking token selection from high-information regions to refine malignancy representation. The method achieves a new state-of-the-art accuracy of 96.4% on the curated dataset and demonstrates generality on a public CT-based Covid detection dataset.",
    "rejected": "FocusMAE enhances masked autoencoders for ultrasound-based Gallbladder Cancer detection using a spatiotemporal approach. By biasing masking towards high-information regions, it achieves 96.4% accuracy, surpassing current image-based and video-based SOTA methods. Code and dataset are available."
  },
  {
    "prompt": "summary: We propose a novel method for 3D point cloud action recognition. Understanding human actions in RGB videos has been widely studied in recent years, however, its 3D point cloud counterpart remains under-explored despite the clear value that 3D information may bring. This is mostly due to the inherent limitation of the point cloud data modality—lack of structure, permutation invariance, and varying number of points—which makes it difﬁcult to learn a spatio-temporal representation. To address this limitation, we propose the 3DinAction pipeline that ﬁrst estimates patches moving in time (t-patches) as a key building block, alongside a hierarchical architecture that learns an informative spatio-temporal representation. We show that our method achieves improved performance on existing datasets, including DFAUST and IKEA ASM.",
    "chosen": "3DinAction is a novel method for 3D point cloud action recognition, proposing a pipeline that estimates patches moving in time (t-patches) and uses a hierarchical architecture to learn spatio-temporal representations. The method improves performance on existing datasets like DFAUST and IKEA ASM.",
    "rejected": "This study investigates non-isometric shape correspondence using a hybrid spectral space combining intrinsic and extrinsic basis functions. The proposed method significantly improves upon traditional approaches, particularly in handling complex deformations and topological noise."
  },
  {
    "prompt": "summary: Domain Generalized Semantic Segmentation (DGSS) deals with training a model on a labeled source domain with the aim of generalizing to unseen domains during inference. Existing DGSS methods typically effectuate robust features by means of Domain Randomization (DR). Such an approach is often limited as it can only account for style diversification and not content. In this work, we take an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In detail, CLOUDS is a framework that integrates Foundation Models of various kinds: (i) CLIP backbone for its robust feature representation, (ii) Diffusion Model to diversify the content, thereby covering various modes of the possible target distribution, and (iii) Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model. Extensive experiments show that our CLOUDS excels in adapting from synthetic to real DGSS benchmarks and under varying weather conditions, notably outperforming prior methods by 5.6% and 6.7% on averaged mIoU, respectively. Our",
    "chosen": "CLOUDS is a framework integrating various foundation models for Domain Generalized Semantic Segmentation, including CLIP for feature representation, Diffusion Model for content diversification, and Segment Anything Model for prediction refinement. It outperforms prior methods by 5.6% and 6.7% on averaged mIoU in adapting from synthetic to real DGSS benchmarks and under varying weather conditions.",
    "rejected": "To improve point cloud matching, a technique based on graph Laplacian eigenmaps is introduced, focusing on fine local structures. The Coupled Laplacian operator aligns eigenspaces, providing a locally meaningful score for shape matching. The method excels in tasks such as object anomaly localization and Bone Side Estimation."
  },
  {
    "prompt": "summary: Precise geospatial vegetation forecasting holds potential across diverse sectors, including agriculture, forestry, humanitarian aid, and carbon accounting. To leverage the vast availability of satellite imagery for this task, various works have applied deep neural networks for predicting multispectral images in photorealistic quality. However, the important area of vegetation dynamics has not been thoroughly explored. Our study introduces GreenEarthNet, the first dataset specifically designed for high-resolution vegetation forecasting, and Contextformer, a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images with fine resolution across Europe. Our multi-modal transformer model Contextformer leverages spatial context through a vision backbone and predicts the temporal dynamics on local context patches incorporating meteorological time series in a parameterefficient manner. The GreenEarthNet dataset features a learned cloud mask and an appropriate evaluation scheme for vegetation modeling. It also maintains compatibility with the existing satellite imagery forecasting dataset EarthNet2021, enabling cross-dataset model comparisons. Our extensive qualitative and quantitative analyses reveal that our methods outperform a broad range of baseline techniques. This includes surpassing previous state-of-theart models on EarthNet2021, as well as adapted models from time series forecasting and video prediction. To the best of our knowledge, this work presents the first models for continental-scale vegetation modeling at fine resolution able to capture anomalies beyond the seasonal cycle, thereby paving the way for predicting vegetation health and behaviour in response to climate variability and extremes. We provide open source",
    "chosen": "GreenEarthNet is the first dataset specifically designed for high-resolution vegetation forecasting, paired with Contextformer, a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images. This multi-modal transformer model leverages spatial context through a vision backbone and predicts temporal dynamics on local context patches incorporating meteorological time series efficiently. GreenEarthNet includes a learned cloud mask and an appropriate evaluation scheme, maintaining compatibility with EarthNet2021. Extensive analyses show Contextformer outperforms various baselines, including previous SOTA models on EarthNet2021 and adapted models from time series forecasting and video prediction.",
    "rejected": "GreenEarthNet introduces Contextformer, a novel deep learning model for predicting vegetation greenness from Sentinel 2 satellite images across Europe. This study presents the first continental-scale vegetation modeling at fine resolution, capable of capturing anomalies beyond seasonal cycles."
  },
  {
    "prompt": "summary: Astronaut photography, spanning six decades of human spaceflight, presents a unique Earth observations dataset with immense value for both scientific research and disaster response. Despite their significance, accurately localizing the geographical extent of these images, which is crucial for effective utilization, poses substantial challenges. Current, manual localization efforts are time-consuming, motivating the need for automated solutions. We propose a novel approach – leveraging image retrieval – to address this challenge efficiently. We introduce innovative training techniques which contribute to the development of a high performance model, EarthLoc. We develop six evaluation datasets and perform a comprehensive benchmark comparing EarthLoc to existing methods, showcasing its superior efficiency and accuracy. Our approach marks a significant advancement in automating the localization of astronaut photography, which will help bridge a critical gap in Earth observations data.",
    "chosen": "EarthLoc is a novel approach leveraging image retrieval to automate the localization of astronaut photography. This method efficiently addresses the challenge of identifying geographical areas in astronaut photographs, which is crucial for effective utilization. EarthLoc employs innovative training techniques to develop a high-performance model, supported by six evaluation datasets and comprehensive benchmark comparisons. The approach significantly advances automated localization of astronaut photography, bridging a critical gap in Earth observations data.",
    "rejected": "EarthLoc proposes an innovative approach for automating the localization of astronaut photography using image retrieval techniques. The method significantly advances the efficiency and accuracy in localizing these unique Earth observation datasets."
  },
  {
    "prompt": "summary: We describe a novel method, StyLitGAN, for relighting and resurfacing images in the absence of labeled data. StyLitGAN generates images with realistic lighting effects, including cast shadows, soft shadows, inter-reflections, and glossy effects, without the need for paired or CGI data. StyLitGAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained StyleGAN to identify a set of directions. By prompting the model to fix one component (e.g., albedo) and vary another (e.g., shading), we generate relighted images by adding the identified directions to the latent style codes. Quantitative metrics of change in albedo and lighting diversity allow us to choose effective directions using a forward selection process. Qualitative evaluation confirms the effectiveness of our method.",
    "chosen": "StyLitGAN is a novel method for relighting and resurfacing images without labeled data. It generates images with realistic lighting effects, including cast shadows, soft shadows, inter-reflections, and glossy effects. StyLitGAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained StyleGAN to identify effective directions for relighting. By fixing one component (e.g., albedo) and varying another (e.g., shading), it generates relighted images. Quantitative metrics and qualitative evaluation confirm the method's effectiveness.",
    "rejected": "StyLitGAN generates relighted images without paired or CGI data, utilizing intrinsic image methods and StyleGAN's latent space to manipulate lighting while preserving albedo and geometry, achieving realistic lighting effects."
  },
  {
    "prompt": "summary: Object detection in radar imagery with neural networks shows great potential for improving autonomous driving. However, obtaining annotated datasets from real radar images, crucial for training these networks, is challenging, especially in scenarios with long-range detection and adverse weather and lighting conditions where radar performance excels. To address this challenge, we present RadSimReal, an innovative physical radar simulation capable of generating synthetic radar images with accompanying annotations for various radar types and environmental conditions, all without the need for real data collection. Remarkably, our findings demonstrate that training object detection models on RadSimReal data and subsequently evaluating them on real-world data produce performance levels comparable to models trained and tested on real data from the same dataset, and even achieves better performance when testing across different real datasets. RadSimReal offers advantages over other physical radar simulations that it does not necessitate knowledge of the radar design details, which are often not disclosed by radar suppliers, and has faster run-time. This innovative tool has the potential to advance the development of computer vision algorithms for radar-based autonomous driving applications.",
    "chosen": "RadSimReal is an innovative physical radar simulation capable of generating synthetic radar images with annotations for various radar types and environmental conditions without real data collection. This simulation addresses the challenge of obtaining annotated radar datasets, crucial for training neural networks in object detection. RadSimReal demonstrates comparable performance to real-data-trained models when evaluated on real-world data and even outperforms them across different real datasets. The simulation offers advantages over other physical radar simulations, including faster run-time and no need for radar design details.",
    "rejected": "RadSimReal simulates synthetic radar images with annotations, offering an effective solution for training object detection models in radar imagery. It demonstrates comparable performance to real-world data-trained models, even surpassing them in cross-dataset evaluations."
  },
  {
    "prompt": "summary: Recent advancements in large vision-language models enabled visual object detection in open-vocabulary scenarios, where object classes are defined in free-text formats during inference. In this paper, we aim to probe the stateof-the-art methods for open-vocabulary object detection to determine to what extent they understand fine-grained properties of objects and their parts. To this end, we introduce an evaluation protocol based on dynamic vocabulary generation to test whether models detect, discern, and assign the correct fine-grained description to objects in the presence of hard-negative classes. We contribute with a benchmark suite of increasing difficulty and probing different properties like color, pattern, and material. We further enhance our investigation by evaluating several stateof-the-art open-vocabulary object detectors using the proposed protocol and find that most existing solutions, which shine in standard open-vocabulary benchmarks, struggle to accurately capture and distinguish finer object details. We conclude the paper by highlighting the limitations of current methodologies and exploring promising research directions to overcome the discovered drawbacks. Data and",
    "chosen": "This study evaluates state-of-the-art open-vocabulary object detection methods using a dynamic vocabulary generation protocol to test their ability to detect, discern, and assign fine-grained descriptions to objects. The benchmark suite increases in difficulty and probes properties like color, pattern, and material. Results show that most existing solutions struggle to accurately capture and distinguish finer object details, highlighting limitations and suggesting research directions for improvement.",
    "rejected": "This paper evaluates open-vocabulary object detection methods' ability to understand fine-grained properties of objects. Results show limitations in accurately distinguishing finer details, highlighting challenges and suggesting future research directions."
  },
  {
    "prompt": "summary: Learning compatible representations enables the interchangeable use of semantic features as models are updated over time. This is particularly relevant in search and retrieval systems where it is crucial to avoid reprocessing of the gallery images with the updated model. While recent research has shown promising empirical evidence, there is still a lack of comprehensive theoretical understanding about learning compatible representations. In this paper, we demonstrate that the stationary representations learned by the d-Simplex fixed classifier optimally approximate compatibility representation according to the two inequality constraints of its formal definition. This not only establishes a solid foundation for future works in this line of research but also presents implications that can be exploited in practical learning scenarios. An exemplary application is the nowstandard practice of downloading and fine-tuning new pretrained models. Specifically, we show the strengths and critical issues of stationary representations in the case in which a model undergoing sequential fine-tuning is asynchronously replaced by downloading a better-performing model pretrained elsewhere. Such a representation enables seamless delivery of retrieval service (i.e., no reprocessing of gallery images) and offers improved performance without operational disruptions during model replacement.",
    "chosen": "IAM-CL2R demonstrates that stationary representations learned by the d-Simplex fixed classifier optimally approximate compatible representations according to theoretical constraints. This establishes a foundation for learning compatible representations and has practical implications for retrieval systems. Stationary representations enable seamless retrieval services during model replacement without reprocessing gallery images, crucial for systems requiring minimal operational disruptions.",
    "rejected": "The paper explores learning compatible representations for search and retrieval systems, demonstrating that stationary representations optimally approximate compatibility, enabling seamless service delivery during model updates."
  },
  {
    "prompt": "summary: Significant progress has been made in scene text detection models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace. Previous works either treated text detection and grouping using separate models, or train a model from scratch while using a unified one. All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets. In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to assemble text instance features. To capture broader contextual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment. Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simultaneously inheriting generalized text detection ability from pre-training. In the case of full parameter fine-tuning, we can further improve layout analysis performance.",
    "chosen": "Text Grouping Adapter (TGA) is a module enabling the use of pre-trained text detectors for layout analysis. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as inputs to assemble text instance features. It predicts text group masks by one-to-many assignment to capture broader contextual information. Experiments show TGA enhances layout analysis performance even with frozen pre-trained models, inheriting generalized text detection ability and improving performance further with full parameter fine-tuning.",
    "rejected": "TGA enables pre-trained text detectors to learn layout analysis, improving scene text grouping without retraining. It uses detected text regions and image features to predict text group masks, enhancing layout analysis performance."
  },
  {
    "prompt": "summary: The increasing use of transformer-based large language models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page setting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pretrained single-page models to the multi-page setting, without requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document tokens, we propose a tailored bias adaptation method. For additional computational savings during decoding, we introduce an optional compression stage using our compressiontransformer(C-Former ),reducing the encoded sequence length, thereby allowing a tradeoff between quality and latency. Extensive experiments showcase GRAM’s stateof-the-art performance on the benchmarks for multi-page DocVQA, demonstrating the effectiveness of our approach.",
    "chosen": "GRAM extends pretrained single-page models to multi-page DocVQA without heavy pretraining. It uses a single-page encoder for local understanding and enhances it with document-level layers and learnable tokens for global reasoning. A compression stage using C-Former reduces sequence length, allowing a quality-latency tradeoff. GRAM achieves state-of-the-art performance on multi-page DocVQA benchmarks.",
    "rejected": "GRAM extends single-page document visual question answering (DocVQA) models to multi-page settings, facilitating global reasoning through document-level designated layers and learnable tokens. An optional compression stage allows quality-latency trade-offs."
  },
  {
    "prompt": "summary: Knowledge of lane topology is a core problem in autonomous driving. Aerial imagery can provide high resolution, quickly updatable lane source data but detecting lanes from such data has so far been an expensive manual process or, where automated solutions exist, undrivable and requiring of downstream processing. We propose a method for large-scale lane topology extraction from aerial imagery while ensuring that the resulting lanes are realistic and drivable by introducing a novel bezier Graph shared parameter is a tion of bezier curves. We develop a transformer-based model to predict these B´ezier Graphs from input aerial images, demonstrating competitive results on the UrbanLaneGraph dataset. We demonstrate that our method generates realistic lane graphs which require both minimal input, and minimal downstream processing. We make our",
    "chosen": "This method extracts large-scale lane topology from aerial imagery using a novel Bézier Graph parameterization of cubic Bézier curves. A transformer-based model predicts these graphs, generating realistic, drivable lane graphs with minimal input and downstream processing. The approach improves on existing methods by ensuring lane direction continuity at boundaries, demonstrated on the UrbanLaneGraph dataset.",
    "rejected": "This paper presents BGFormer, a transformer-based model for predicting Bezier Graphs from aerial images, generating drivable lane graphs with minimal downstream processing, ideal for HD map creation in autonomous driving."
  },
  {
    "prompt": "summary: Image  key point descriptions that are discriminative and matchable over large changes in viewpoint are vital for 3D reconstruction. However, descriptions output by learned descriptors are typically not robust to camera rotation. While they can be made more robust by, e.g., data augmentation, this degrades performance on upright images. Another approach is test-time augmentation, which incurs a significant increase in runtime. Instead, we learn a linear transform in description space that encodes rotations of the input image. We call this linear transform a steerer since it allows us to transform the descriptions as if the image was rotated. From representation theory, we know all possible steerers for the rotation group. Steerers can be optimized (A) given a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize a descriptor given a fixed steerer. We perform experiments in these three settings and obtain state-of-the-art results on the rotation invariant image matching benchmarks AIMS and Roto-360. We publish",
    "chosen": "Steerers are learned linear transforms in description space that encode image rotations for  key point descriptors. This framework allows rotation-equivariant matching without sacrificing distinctiveness for small rotations or incurring significant runtime costs. Experiments show state-of-the-art results on rotation-invariant benchmarks, with code and model weights published.",
    "rejected": "The study focuses on creating discriminative image  key point descriptors robust over large viewpoint changes for 3D reconstruction, addressing the lack of robustness in current learned descriptors."
  },
  {
    "prompt": "summary: Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts, and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability.",
    "chosen": "Gaussian Shadow Casting for Neural Characters presents a new shadow model using a Gaussian density proxy for efficient shadow computation in neural character models. This method improves reconstructions by better separating albedo, shading, and shadows, enabling realistic relighting and novel pose generation without user input.",
    "rejected": "The paper introduces Gaussian shadows, a new model for accurate lighting and shadow computation in neural avatars reconstructed from video. This method supports dynamic motion and integrates with deferred neural rendering to improve shading and shadow casting. It enables realistic relighting in novel scenes without user input on light direction."
  },
  {
    "prompt": "summary: In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photorealistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work.",
    "chosen": "This work explores end-to-end training of agents in simulation to minimize the sim2real gap for autonomous robot navigation. The agent predicts velocity commands and is trained with a modified Habitat simulator modeling real robot behavior and noise. Evaluations show significant performance and robustness gains in real navigation scenarios.",
    "rejected": "This work addresses minimizing the sim2real gap for terrestrial robots by end-to-end training in simulations that closely mimic real-world sensing and actuation dynamics. By using a modified Habitat simulator with noise models for odometry and localization, it achieves significant improvements in navigation performance and robustness compared to prior methods."
  },
  {
    "prompt": "summary: Local Interpretable Model-agnostic Explanations (LIME) - a widely used post-ad-hoc model agnostic explainable AI (XAI) technique. It works by training a simple transparent (surrogate) model using random samples drawn around the neighborhood of the instance (image) to be explained (IE). Explanations are then extracted for a black-box model and a given IE, using the surrogate model. However, the explanations of LIME suffer from inconsistency across different runs for the same model and the same IE. We identify two main types of inconsistencies: variance in the sign and importance ranks of the segments (superpixels). These factors hinder LIME from obtaining consistent explanations. We analyze these inconsistencies and propose a new method, Stabilized LIME for Consistent Explanations (SLICE). The proposed method handles the stabilization problem in two aspects: using a novel feature selection technique to eliminate spurious superpixels and an adaptive perturbation technique to generate perturbed images in the neighborhood of IE. Our results demonstrate that the explanations from SLICE exhibit signiﬁcantly better consistency and ﬁdelity than LIME (and its variant BayLime).",
    "chosen": "SLICE is a new method enhancing LIME's consistency for image classification explanations. It uses feature selection to eliminate spurious superpixels and adaptive perturbation to generate neighborhood images, yielding more reliable and consistent explanations than LIME and its variants.",
    "rejected": "SLICE is proposed to stabilize explanations generated by LIME, addressing inconsistencies due to variance in segment importance ranks. SLICE uses feature selection and adaptive perturbation techniques to produce more consistent and faithful explanations for black-box models than LIME, enhancing reliability in explainable AI."
  },
  {
    "prompt": "summary: We study the underexplored but fundamental problem of machine understanding of abstract freehand scene sketches. We introduce a sketch encoder that ensures a semantically aware feature space, which we evaluate by testing its performance on a semantic sketch segmentation task. To train our model, we rely only on bitmap sketches accompanied by brief captions, avoiding the need for pixel-level annotations. To generalize to a large set of sketches and categories, we build upon a vision transformer encoder pretrained with the CLIP model. We freeze the text encoder and perform visual-prompt tuning of the visual encoder branch while introducing a set of critical modifications. First, we augment the classical key-query (k-q) self-attention blocks with value-value (v-v) self-attention blocks. Central to our model is a two-level hierarchical training that enables efficient semantic disentanglement: The first level ensures holistic scene sketch encoding, and the second level focuses on individual categories. In the second level of the hierarchy, we introduce cross-attention between the text and vision branches. Our method outperforms zero-shot CLIP segmentation results by 37 points, reaching a pixel accuracy of 85.5% on the FS-COCO sketch dataset. Finally, we conduct a user study that allows us to identify further improvements needed over our method to reconcile machine and human understanding of freehand scene sketches.",
    "chosen": "This paper introduces a sketch encoder for understanding abstract freehand scene sketches, evaluated on semantic sketch segmentation. The model, built on a CLIP-pretrained ViT, uses two-level hierarchical training and cross-attention to achieve high pixel accuracy on the FS-COCO dataset.",
    "rejected": "A sketch encoder is introduced that leverages a vision transformer pre-trained with CLIP for semantic segmentation of freehand scene sketches. The hierarchical training approach allows efficient disentanglement at both holistic and category levels, achieving superior zero-shot segmentation accuracy compared to CLIP on the FS-COCO dataset."
  },
  {
    "prompt": "summary: Vision-language foundation models have shown remarkable performance in various zero-shot settings such as image retrieval, classification, or captioning. But so far, those models seem to fall behind when it comes to zero-shot localization of referential expressions and objects in images. As a result, they need to be fine-tuned for this task. In this paper, we show that pretrained vision-language (VL) models allow for zero-shot open-vocabulary object localization without any fine-tuning. To leverage those capabilities, we propose a Grounding Everything Module (GEM) that generalizes the idea of value-value attention introduced by CLIPSurgery [17] to a self-self attention path. We show that the concept of self-self attention corresponds to clustering, thus enforcing groups of tokens arising from the same object to be similar while preserving the alignment with the language space. To further guide the group formation, we propose a set of regularizations that allows the model to finally generalize across datasets and backbones. We evaluate the proposed GEM framework on various benchmark tasks and datasets for semantic segmentation. GEM not only outperforms other training-free open-vocabulary localization methods, but also achieves state-of-the-art results on the recently proposed OpenImagesV7 large-scale segmentation benchmark. 1",
    "chosen": "GEM enables zero-shot open-vocabulary object localization using pretrained VL models. The proposed Grounding Everything Module uses self-self attention and regularizations to improve token grouping and alignment, achieving state-of-the-art results on segmentation benchmarks without fine-tuning.",
    "rejected": "GEM enhances zero-shot object localization capabilities of pretrained vision-language models through self-self attention paths, promoting clustering of tokens representing the same object. GEM outperforms existing methods on various benchmarks and achieves state-of-the-art results on OpenImagesV7 large-scale segmentation."
  },
  {
    "prompt": "summary: Text-to-image diffusion models have recently received increasing interest for their astonishing ability to produce high-ﬁdelity images from solely text inputs. Subsequent research efforts aim to exploit and apply their capabilities to real image editing. However, existing image-toimage methods are often inefﬁcient, imprecise, and of limited versatility. They either require time-consuming ﬁnetuning, deviate unnecessarily strongly from the input image, and/or lack support for multiple, simultaneous edits. To address these issues, we introduce LEDITS++, an efﬁcient yet versatile and precise textual image manipulation technique. LEDITS++’s novel inversion approach requires no tuning nor optimization and produces high-ﬁdelity results with a few diffusion steps. Second, our methodology supports multiple simultaneous edits and is architecture-agnostic. Third, we use a novel implicit masking technique that limits changes to relevant image regions. We propose the novel TEdBench++ benchmark as part of our exhaustive evaluation. Our results demonstrate the capabilities of LEDITS++ and its improvements over previous methods.",
    "chosen": "LEDITS++ is an efficient textual image manipulation technique using a novel inversion approach and implicit masking. It requires no tuning, supports multiple edits, and produces high-fidelity results with few diffusion steps, outperforming previous methods.",
    "rejected": "LEDITS++ presents an efficient textual image manipulation technique requiring no fine-tuning, supporting multiple simultaneous edits, and using implicit masking to limit changes to relevant regions. It provides high-fidelity results with fewer diffusion steps, making it suitable for versatile and precise image editing applications."
  },
  {
    "prompt": "summary: Neural approaches have shown a significant progress on camera-based reconstruction. But they require either a fairly dense sampling of the viewing sphere, or pre-training on an existing dataset, thereby limiting their generalizability. In contrast, photometric stereo (PS) approaches have shown great potential for achieving high-quality reconstruction under sparse viewpoints. Yet, they are impractical because they typically require tedious laboratory conditions, are restricted to dark rooms, and often multi-staged, making them subject to accumulated errors. To address these shortcomings, we propose an end-to-end uncalibrated multi-view PS framework for reconstructing high-resolution shapes acquired from sparse viewpoints in a real-world environment. We relax the dark room assumption, and allow a combination of static ambient lighting and dynamic near LED lighting, thereby enabling easy data capture outside the lab. Experimental validation confirms that it outperforms existing baseline approaches in the regime of sparse viewpoints by a large margin. This allows to bring highaccuracy 3D reconstruction from the dark room to the real world, while maintaining a reasonable data capture complexity.",
    "chosen": "This paper proposes an end-to-end uncalibrated multi-view photometric stereo framework for high-resolution 3D reconstruction from sparse viewpoints in real-world settings. It combines static ambient and dynamic LED lighting, outperforming baselines and enabling lab-quality reconstruction in practical conditions.",
    "rejected": "An end-to-end uncalibrated multi-view photometric stereo framework is introduced for 3D reconstruction from sparse viewpoints under ambient and near LED lighting conditions. The approach overcomes limitations of laboratory setups, enabling high-accuracy reconstructions in real-world environments without controlled lighting."
  },
  {
    "prompt": "summary: This paper introduces a versatile paradigm for integrating multi-view reﬂectance (optional) and normal maps acquired through photometric stereo. Our approach employs a pixel-wise joint re-parameterization of reﬂectance and normal, considering them as a vector of radiances rendered under simulated, varying illumination. This reparameterization enables the seamless integration of reﬂectance and normal maps as input data in neural volume rendering-based 3D reconstruction while preserving a single optimization objective. In contrast, recent multi-view photometric stereo (MVPS) methods depend on multiple, potentially conﬂicting objectives. Despite its apparent simplicity, our proposed approach outperforms state-of-the-art approaches in MVPS benchmarks across F-score, Chamfer distance, and mean angular error metrics. Notably, it signiﬁcantly improves the detailed 3D reconstruction of areas with high curvature or low visibility.",
    "chosen": "RNb-NeuS integrates multi-view reflectance and normal maps into neural volume rendering via a joint re-parameterization. This approach outperforms state-of-the-art methods in MVPS benchmarks, improving 3D reconstruction of detailed areas.",
    "rejected": "The paper proposes a joint re-parameterization of reflectance and normal maps as radiances under simulated illumination for integration into neural volume rendering-based 3D reconstruction. This method simplifies optimization and improves reconstruction quality, especially in areas with high curvature or low visibility, outperforming state-of-the-art approaches in MVPS benchmarks."
  },
  {
    "prompt": "summary: In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provide an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D  key points detected across multiple views, and outputs the corresponding camera poses and 3D  key point coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime. Our",
    "chosen": "This work presents GASFM, a graph attention network for learning Structure-from-Motion. The model takes 2D key points as input and outputs camera poses and 3D coordinates, offering faster reconstruction than COLMAP with competitive accuracy.",
    "rejected": "Using graph attention networks, this work replaces traditional sub-problems in Structure-from-Motion (SfM) with a learned model for fast inference of camera poses and 3D  key points. Experimental results demonstrate superior performance compared to competing learning-based methods and challenge COLMAP while reducing runtime."
  },
  {
    "prompt": "summary: Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geographically diverse descriptive knowledge of categories can enhance robustness. For this purpose, we explore the feasibility of probing a large language model for geography-based object knowledge, and we examine the effects of integrating knowledge into zero-shot and learnable soft prompting with CLIP. Within this exploration, we propose geography knowledge regularization to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set. Accuracy gains over prompting baselines on DollarStreet while training only on Europe data are up to +2.8/1.2/1.6 on target data from Africa/Asia/Americas, and +4.6 overall on the hardest classes. Competitive performance is shown vs. few-shot target training, and analysis is provided to direct future study of geographical robustness.",
    "chosen": "The paper explores incorporating geo-diverse knowledge into prompting for geographical robustness in object recognition. Using a large language model for geography-based knowledge and integrating it with CLIP via soft prompting and regularization improves accuracy on target data from diverse regions.",
    "rejected": "The research develops an advanced framework combining photometric stereo and neural rendering for reconstructing textureless objects from sparse viewpoints. By integrating ambient and point-light illumination, it achieves detailed 3D reconstructions outside controlled environments, overcoming limitations associated with conventional photometric stereo techniques."
  },
  {
    "prompt": "summary: Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training, in this paper, we show that the full potential of improving the training process by addressing such issues is yet to be realized. Specifically, we firstly study and analyze two issues affecting training: incorrect assignment of negative pairs, and low caption quality and diversity. Then, we devise effective solutions for addressing both problems, which essentially require training with multiple true positive pairs. Finally, we propose training with sigmoid loss to address such a requirement. We show very large gains over the current state-of-the-art for both image recognition (∼+6% on average over 11 datasets) and image retrieval (∼+19% on Flickr30k and ∼+15% on MSCOCO).",
    "chosen": "FFF addresses noise and caption quality in vision-language contrastive pre-training by studying incorrect negative pair assignments and low caption quality/diversity. It proposes training with multiple true positive pairs using a sigmoid loss, achieving large gains over the state-of-the-art in image recognition and retrieval.",
    "rejected": "This study addresses two issues affecting vision-language pre-training: incorrect assignment of negative pairs and low caption quality. By introducing an algorithm that mines new positive pairs and proposes training with sigmoid loss, significant improvements in image recognition and retrieval are achieved. The approach enhances model accuracy by addressing false negatives and improving caption quality."
  },
  {
    "prompt": "summary: We consider a critical issue of false negatives in VisionLanguage Pre-training (VLP), a challenge that arises from the inherent many-to-many correspondence of image-text pairs in large-scale web-crawled datasets. The presence of false negatives can impede achieving optimal performance and even lead to a significant performance drop. To address this challenge, we propose MAFA (MAnaging FAlse negatives), which consists of two pivotal components building upon the recently developed GRouped mIni-baTch sampling (GRIT) strategy: 1) an efficient connection mining process that identifies and converts false negatives into positives, and 2) label smoothing for the image-text contrastive (ITC) loss. Our comprehensive experiments verify the effectiveness of MAFA across multiple downstream tasks, emphasizing the crucial role of addressing false negatives in VLP, potentially even surpassing the importance of addressing false positives. In addition, the compatibility of MAFA with the recent BLIP-family model is also demonstrated.",
    "chosen": "MAFA tackles false negatives in Vision-Language Pre-training (VLP) through an efficient connection mining process and label smoothing for the image-text contrastive loss, building on the GRIT strategy. It demonstrates significant improvements across downstream tasks, highlighting the importance of addressing false negatives in VLP.",
    "rejected": "The paper introduces MAFA, designed to tackle false negatives in vision-language pre-training. It utilizes a connection mining process and label smoothing for ITC loss. Experiments show MAFA's effectiveness across multiple tasks, demonstrating its compatibility with recent models like BLIP, achieving superior performance by managing false negatives."
  },
  {
    "prompt": "summary: Nonlinearities are decisive in neural representation learning. Traditional Activation (Act) functions impose fixed inductive biases on neural networks with oriented biological intuitions. Recent methods leverage selfgated curves to compensate for the rigid traditional Act paradigms in fitting flexibility. However, substantial improvements are still impeded by the norm-induced mismatched feature re-calibrations (see Section 1), i.e., the actual importance of a feature can be inconsistent with its explicit intensity such that violates the basic intention of a direct self-gated feature re-weighting. To address this problem, we propose to learn discriminative neural feature Act with a novel prototype, namely, AdaShift, which enhances typical self-gated Act by incorporating an adaptive shift factor into the re-weighting function of Act. AdaShift casts dynamic translations on the inputs of a re-weighting function by exploiting comprehensive feature-filter context cues of different ranges in a simple yet effective manner. We obtain the new intuitions of AdaShift by rethinking the feature-filter relationships from a common Softmax-based classification and by generalizing the new observations to a common learning layer that encodes features with updatable filters. Our practical AdaShifts, built upon the new Act prototype, demonstrate significant improvements to the popular/SOTA Act functions on different vision benchmarks. By simply replacing ReLU with AdaShifts, ResNets can match advanced Transformer counterparts (e.g., ResNet-50 vs. Swin-T) with lower cost and fewer parameters.",
    "chosen": "AdaShift enhances traditional self-gated activation functions by incorporating an adaptive shift factor into the re-weighting function. This approach, which exploits comprehensive feature-filter context cues, demonstrates significant improvements over popular activation functions on various vision benchmarks when integrated with ResNet backbones.",
    "rejected": "To address feature recalibration mismatches, AdaShift is proposed, incorporating an adaptive shift factor into activation functions. This method enhances neural networks' discriminative power without significantly increasing parameters or costs, leading to improved performance on various benchmarks."
  },
  {
    "prompt": "summary: Representing signals using coordinate networks dominates the area of inverse problems recently, and is widely applied in various scientific computing tasks. Still, there exists an issue of spectral bias in coordinate networks, limiting the capacity to learn high-frequency components. This problem is caused by the pathological distribution of the neural tangent kernel’s (NTK’s) eigenvalues of coordinate networks. We find that, this pathological distribution could be improved using the classical batch normalization (BN), which is a common deep learning technique but rarely used in coordinate networks. BN greatly reduces the maximum and variance of NTK’s eigenvalues while slightly modifies the mean value, considering the max eigenvalue is much larger than the most, this variance change results in a shift of eigenvalues’ distribution from a lower one to a higher one, therefore the spectral bias could be alleviated (see Fig. 1). This observation is substantiated by the significant improvements of applying BN-based coordinate networks to various tasks, including the image compression, computed tomography reconstruction, shape representation, magnetic resonance imaging and novel view synthesis.",
    "chosen": "This paper shows that batch normalization can alleviate the spectral bias in coordinate networks by shifting the distribution of the neural tangent kernel's eigenvalues. This results in significant improvements in tasks like image compression and tomography reconstruction, where coordinate networks are widely applied.",
    "rejected": "Batch normalization (BN) is explored to alleviate spectral bias in coordinate networks, enhancing learning of high-frequency components. BN modifies the distribution of NTK eigenvalues, improving performance in tasks such as image compression and computed tomography reconstruction."
  },
  {
    "prompt": "summary: In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character’s digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 582",
    "chosen": "The Digital Life Project presents a framework for autonomous 3D characters with social intelligence, featuring SocioMind (a digital brain modeling personalities) and MoMat-MoGen (a text-driven motion synthesis paradigm). This enables characters to engage in autonomous social interactions and express with articulated body motions.",
    "rejected": "The Digital Life Project framework uses language to build autonomous 3D characters capable of social interaction and articulated body motions. SocioMind models personalities and initiates dialogues, while MoMat-MoGen synthesizes motion. Together, they enable virtual characters to autonomously engage in dialogue and evolve socio-psychological states, demonstrating state-of-the-art performance in their domains."
  },
  {
    "prompt": "summary: Traditional 3D content creation tools empower users to bring their imagination to life by giving them direct control over a scene’s geometry, appearance, motion, and camera path. Creating computer-generated videos, however, is a tedious manual process, which can be automated by emerging text-to-video diffusion models. Despite great promise, video diffusion models are difficult to control, hindering a user to apply their own creativity rather than amplifying it. To address this challenge, we present a novel approach that combines the controllability of dynamic 3D meshes with the expressivity and editability of emerging diffusion models. For this purpose, our approach takes an animated, low-fidelity rendered mesh as input and injects the ground truth correspondence information obtained from the dynamic mesh into various stages of a pre-trained text-to-image generation model to output high-quality and temporally consistent frames. We demonstrate our approach on various examples where motion can be obtained by animating rigged assets or changing the camera path.",
    "chosen": "This novel approach combines the controllability of dynamic 3D meshes with the expressivity of text-to-image diffusion models. By injecting ground truth correspondence information from dynamic meshes into pre-trained text-to-image models, it generates high-quality, temporally consistent frames, enabling contextually relevant character motions.",
    "rejected": "A novel approach combining dynamic 3D meshes with text-to-image generation models is presented. This method injects ground truth correspondence information into stages of a pre-trained model, producing high-quality, temporally consistent frames from animated low-fidelity rendered meshes, enhancing control and creativity in video generation."
  },
  {
    "prompt": "summary: In the current era of generative AI breakthroughs, generating panoramic scenes from a single input image remains a key challenge. Most existing methods use diffusion-based iterative or simultaneous multi-view inpainting. However, the lack of global scene layout priors leads to subpar outputs with duplicated objects (e.g., multiple beds in a bedroom) or requires time-consuming human text inputs for each view. We propose L-MAGIC, a novel method leveraging large language models for guidance while diffusing multiple coherent views of 360◦panoramic scenes. L-MAGIC harnesses pre-trained diffusion and language models without ﬁne-tuning, ensuring zero-shot performance. The output quality is further enhanced by super-resolution and multiview fusion techniques. Extensive experiments demonstrate that the resulting panoramic scenes feature better scene layouts and perspective view rendering quality compared to related works, with >70% preference in human evaluations. Combined with conditional diffusion models, L-MAGIC can accept various input modalities, including but not limited to text, depth maps, sketches, and colored scripts. Applying depth estimation further enables 3D point cloud generation and dynamic scene exploration with ﬂuid camera motion.",
    "chosen": "L-MAGIC leverages large language models to guide the diffusion of multiple coherent views for generating 360° panoramic scenes. It ensures zero-shot performance by utilizing pre-trained models without fine-tuning and enhances output quality with super-resolution and multi-view fusion techniques.",
    "rejected": "L-MAGIC leverages large language models to guide diffusion models for generating coherent views of 360-degree panoramic scenes. It achieves enhanced scene layouts and perspective rendering quality compared to related methods. L-MAGIC supports various input modalities and enables 3D point cloud generation, offering fluid camera motion and immersive experiences."
  },
  {
    "prompt": "summary: Camera-parameter-free multi-view pose estimation is an emerging technique for 3D human pose estimation (HPE). They can infer the camera settings implicitly or explicitly to mitigate the depth uncertainty impact, showcasing significant potential in real applications. However, due to the limited camera setting diversity in the available datasets, the inferred camera parameters are always simply hardcoded into the model during training and not adaptable to the input in inference, making the learned models cannot generalize well under unseen camera settings. A natural solution is to artificially synthesize some samples, i.e., 2D3D pose pairs, under massive new camera settings. Unfortunately, to prevent over-fitting the existing camera setting, the number of synthesized samples for each new camera setting should be comparable with that for the existing one, which multiplies the scale of training and even makes it computationally prohibitive. In this paper, we propose a novel HPE approach under the invariant risk minimization (IRM) paradigm. Precisely, we first synthesize 2D poses from myriad camera settings. We then train our model under the IRM paradigm, which targets at learning a common optimal model across all camera settings and thus enforces the model to automatically learn the camera parameters based on the input data. This allows the model to accurately infer 3D poses on unseen data by training on only a handful of samples from each synthesized setting and thus avoid the unbearable training cost increment. Another appealing feature of our method is that benefited from the capability of IRM in identifying the invariant features, its performance on the seen camera settings is enhanced as well. Comprehensive experiments verify the superiority of our approach.",
    "chosen": "PoseIRM enhances 3D human pose estimation on unseen camera settings using invariant risk minimization. It synthesizes 2D poses from various camera settings and trains a model to learn a common optimal solution, allowing accurate 3D pose inference on unseen data with minimal samples per setting.",
    "rejected": "A novel HPE approach under the invariant risk minimization (IRM) paradigm is introduced. It synthesizes 2D poses from myriad camera settings and trains the model to learn common optimal parameters across all settings. This allows accurate inference of 3D poses on unseen data, enhancing generalization capabilities and performance on both seen and unseen camera settings."
  },
  {
    "prompt": "summary: Synthesizing realistic videos of talking faces under custom lighting conditions and viewing angles benefits various downstream applications like video conferencing. However, most existing relighting methods are either time-consuming or unable to adjust the viewpoints. In this paper, we present the first real-time 3D-aware method for relighting in-thewild videos of talking faces based on Neural Radiance Fields (NeRF). Given an input portrait video, our method can synthesize talking faces under both novel views and novel lighting conditions with a photo-realistic and disentangled 3D representation. Specifically, we infer an albedo tri-plane, as well as a shading tri-plane based on a desired lighting condition for each video frame with fast dualencoders. We also leverage a temporal consistency network to ensure smooth transitions and reduce flickering artifacts. Our method runs at 32.98 fps on consumer-level hardware and achieves state-of-the-art results in terms of reconstruction quality, lighting error, lighting instability, temporal consistency and inference speed. We demonstrate the effectiveness and interactivity of our method on various portrait videos with diverse lighting and viewing conditions.",
    "chosen": "This real-time 3D-aware method for portrait video relighting uses Neural Radiance Fields (NeRF) to synthesize talking faces under novel views and lighting conditions. It infers albedo and shading tri-planes with fast dual-encoders and ensures temporal consistency, achieving state-of-the-art results in reconstruction quality and inference speed.",
    "rejected": "A real-time 3D-aware method for relighting talking face videos based on Neural Radiance Fields (NeRF) is proposed. It synthesizes realistic talking faces under novel views and lighting conditions, ensuring smooth transitions and reducing flickering artifacts. Running at 32.98 fps, it achieves state-of-the-art results in reconstruction quality, temporal consistency, and speed."
  },
  {
    "prompt": "summary: While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary (free-form) visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a “red bounding box” or “pointed arrow”. Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.",
    "chosen": "ViP-LLaVA introduces a multimodal model capable of decoding arbitrary visual prompts, allowing users to interact with the model using natural cues like arrows or bounding boxes. This approach achieves state-of-the-art performance on region-understanding tasks and includes ViP-Bench, a benchmark for assessing visual prompt understanding.",
    "rejected": "This paper introduces a novel multimodal model capable of decoding free-form visual prompts, enabling intuitive image interaction using natural cues like \"red bounding boxes\" or \"pointed arrows.\" By overlaying visual markers directly onto RGB images, the method simplifies user interaction without complex region encodings. It achieves state-of-the-art performance on region-understanding tasks such as Visual7W, PointQA, and Visual Commonsense Reasoning benchmarks. Additionally, the authors present ViP-Bench, a benchmark for evaluating models' capabilities in understanding visual prompts across multiple dimensions. This facilitates future research and advancements in user-friendly visual prompting techniques."
  },
  {
    "prompt": "summary: Neural Radiance Fields (NeRFs) have shown great potential in novel view synthesis. However, they struggle to render sharp images when the data used for training is affected by motion blur. On the other hand, event cameras excel in dynamic scenes as they measure brightness changes with microsecond resolution and are thus only marginally affected by blur. Recent methods attempt to enhance NeRF reconstructions under camera motion by fusing frames and events. However, they face challenges in recovering accurate color content or constrain the NeRF to a set of predefined camera poses, harming reconstruction quality in challenging conditions. This paper proposes a novel formulation addressing these issues by leveraging both modeland learning-based modules. We explicitly model the blur formation process, exploiting the event double integral as an additional model-based prior. Additionally, we model the event-pixel response using an end-to-end learnable response function, allowing our method to adapt to nonidealities in the real event-camera sensor. We show, on synthetic and real data, that the proposed approach outperforms existing deblur NeRFs that use only frames as well as those that combine frames and events by +6.13dB and +2.48dB, respectively.",
    "chosen": "Ev-DeblurNeRF addresses motion blur in NeRF reconstructions by fusing frames and events. It models the blur formation process using event double integral as a prior and employs a learnable event-pixel response function. This method outperforms existing deblur NeRFs, achieving significant improvements on synthetic and real data.",
    "rejected": "This paper addresses the issue of motion blur in Neural Radiance Fields (NeRFs) by leveraging both model- and learning-based modules, explicitly modeling the blur formation process using event cameras. The proposed method outperforms existing techniques by significant margins."
  },
  {
    "prompt": "summary: Radiologists highly desire fully automated versatile AI for medical imaging interpretation. However, the lack of extensively annotated large-scale multi-disease datasets has hindered the achievement of this goal. In this paper, we explore the feasibility of leveraging language as a naturally high-quality supervision for chest CT imaging. In light of the limited availability of image-report pairs, we bootstrap the understanding of 3D chest CT images by distilling chest-related diagnostic knowledge from an extensively pre-trained 2D X-ray expert model. Speciﬁcally, we propose a language-guided retrieval method to match each 3D CT image with its semantically closest 2D X-ray image, and perform pair-wise and semantic relation knowledge distillation. Subsequently, we use contrastive learning to align images and reports within the same patient while distinguishing them from the other patients. However, the challenge arises when patients have similar semantic diagnoses, such as healthy patients, potentially confusing if treated as negatives. We introduce a robust contrastive learning that identiﬁes and corrects these false negatives. We train our model with over 12K pairs of chest CT images and radiology reports. Extensive experiments across multiple scenarios, including zero-shot learning, report generation, and ﬁne-tuning processes, demonstrate the model’s feasibility in interpreting chest CT images.",
    "chosen": "This paper explores using language as supervision for chest CT imaging by leveraging knowledge from a 2D X-ray expert model. A language-guided retrieval method matches CT images with semantically closest X-ray images, performing knowledge distillation and contrastive learning. The model is trained on over 12K pairs of chest CT images and radiology reports, demonstrating feasibility in interpreting chest CT images across multiple scenarios.",
    "rejected": "This work explores using language as supervision for interpreting chest CT images, distilling knowledge from 2D X-ray images via contrastive learning to understand 3D chest CT scans without extensive annotations. It demonstrates feasibility across various scenarios."
  },
  {
    "prompt": "summary: Diffusion model-based image restoration (IR) aims to use diffusion models to recover high-quality (HQ) images from degraded images, achieving promising performance. Due to the inherent property of diffusion models, most existing methods need long serial sampling chains to restore HQ images step-by-step, resulting in expensive sampling time and high computation costs. Moreover, such long sampling chains hinder understanding the relationship between inputs and restoration results since it is hard to compute the gradients in the whole chains. In this work, we aim to rethink the diffusion model-based IR models through a different perspective, i.e., a deep equilibrium (DEQ) fixed point system, called DeqIR. Specifically, we derive an analytical solution by modeling the entire sampling chain in these IR models as a joint multivariate fixed point system. Based on the analytical solution, we can conduct parallel sampling and restore HQ images without training. Furthermore, we compute fast gradients via DEQ inversion and found that initialization optimization can boost image quality and control the generation direction. Extensive experiments on benchmarks demonstrate the effectiveness of our method on typical IR tasks and real-world settings.",
    "chosen": "DeqIR rethinks diffusion model-based image restoration through a deep equilibrium fixed point system. It models the sampling chain as a joint multivariate fixed point system, enabling parallel sampling and restoring high-quality images without training. The method computes fast gradients via DEQ inversion and optimizes initialization, showing effectiveness on various IR tasks.",
    "rejected": "DeqIR is introduced as a deep equilibrium fixed point system for diffusion model-based image restoration. It enables parallel sampling and restores high-quality images efficiently, showing better performance than traditional methods on multiple image restoration tasks."
  },
  {
    "prompt": "summary: We present DreamAvatar, a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. While encouraging results have been reported by recent methods on text-guided 3D common object generation, generating high-quality human avatars remains an open challenge due to the complexity of the human body’s shape, pose, and appearance. We propose DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for predicting density and color for 3D points and pretrained text-to-image diffusion models for providing 2D self-supervision. Specifically, we leverage the SMPL model to provide shape and pose guidance for the generation. We introduce a dual-observation-space design that involves the joint optimization of a canonical space and a posed space that are related by a learnable deformation field. This facilitates the generation of more complete textures and geometry faithful to the target pose. We also jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the common multi-face “Janus” problem and improve facial details in the generated avatars. Extensive evaluations demonstrate that DreamAvatar significantly outperforms existing methods, establishing a new state-of-the-art for text-and-shape guided 3D human avatar generation.",
    "chosen": "DreamAvatar is a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. It uses a trainable NeRF and pretrained text-to-image diffusion models, leveraging the SMPL model for shape and pose guidance. The dual-observation-space design and joint optimization of losses improve texture completeness and facial details, outperforming existing methods.",
    "rejected": "DreamAvatar generates high-quality 3D human avatars guided by text and shape priors, utilizing trainable NeRF and pretrained text-to-image diffusion models. It achieves complete textures and geometry faithful to target poses, significantly improving facial details."
  },
  {
    "prompt": "summary: We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based priors enable more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent sets instead of using global latent codes. This novel 4D representation allows us to learn local shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporallycoherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid computational overhead, we designed *Equal Contribution. †Corresponding author. ‡Work done during master’s thesis. an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations.",
    "chosen": "Motion2VecSets is a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. It learns shape and motion distribution through an iterative denoising process of compressed latent representations, parameterized with latent sets for local shape and deformation patterns. This approach improves generalizability to unseen motions and identities, demonstrating superiority in 4D reconstruction from imperfect observations.",
    "rejected": "Motion2VecSets, a 4D diffusion model, learns shape and motion distribution through iterative denoising processes. By parameterizing dynamics with latent sets, it leads to accurate reconstructions and improved generalizability to unseen motions and identities."
  },
  {
    "prompt": "summary: Generalized Category Discovery (GCD) aims to identify a mix of known and novel categories within unlabeled data sets, providing a more realistic setting for image recognition. Essentially, GCD needs to remember existing patterns thoroughly to recognize novel categories. Recent state-of-the-art method SimGCD transfers the knowledge from known-class data to the learning of novel classes through debiased learning. However, some patterns are catastrophically forgot during adaptation and thus lead to poor performance in novel categories classiﬁcation. To address this issue, we propose a novel learning approach, LegoGCD, which is seamlessly integrated into previous methods to enhance the discrimination of novel classes while maintaining performance on previously encountered known classes. Speciﬁcally, we design two types of techniques termed as Local Entropy Regularization (LER) and Dualviews Kullback–Leibler divergence constraint (DKL). The LER optimizes the distribution of potential known class samples in unlabeled data, thus ensuring the preservation of knowledge related to known categories while learning novel classes. Meanwhile, DKL introduces Kullback–Leibler divergence to encourage the model to produce a similar prediction distribution of two view samples from the same image. In this way, it successfully avoids mismatched prediction and generates more reliable potential known class samples simultaneously. Extensive experiments validate that the proposed LegoGCD effectively addresses the known category forgetting issue across all datasets, e.g., delivering a 7.74% and 2.51% accuracy boost on known and novel classes in CUB, respectively. Our",
    "chosen": "LegoGCD addresses catastrophic forgetting in Generalized Category Discovery by integrating Local Entropy Regularization and Dual-views Kullback-Leibler divergence constraint. These techniques optimize the distribution of known class samples and encourage similar prediction distributions from different views, effectively solving the forgetting issue and improving accuracy on both known and novel classes.",
    "rejected": "LegoGCD introduces Local Entropy Regularization (LER) and Dual-views Kullback–Leibler divergence constraint (DKL) to address catastrophic forgetting in generalized category discovery. These techniques enhance novel class discrimination while preserving known class performance, boosting accuracy."
  },
  {
    "prompt": "summary: We present SuperNormal, a fast, high-ﬁdelity approach to multi-view 3D reconstruction using surface normal maps. With a few minutes, SuperNormal produces detailed surfaces on par with 3D scanners. We harness volume rendering to optimize a neural signed distance function (SDF) powered by multi-resolution hash encoding. To accelerate training, we propose directional ﬁnite difference and patchbased ray marching to approximate the SDF gradients numerically. While not compromising reconstruction quality, this strategy is nearly twice as efﬁcient as analytical gradients and about three times faster than axis-aligned ﬁnite difference. Experiments on the benchmark dataset demonstrate the superiority of SuperNormal in efﬁciency and accuracy compared to existing multi-view photometric stereo methods. On our captured objects, SuperNormal produces more ﬁne-grained geometry than recent neural 3D reconstruction methods. Our",
    "chosen": "SuperNormal is a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. It optimizes a neural signed distance function powered by multi-resolution hash encoding, using directional finite difference and patch-based ray marching. This method is more efficient and accurate than existing MVPS methods, producing detailed surfaces comparable to 3D scanners.",
    "rejected": "SuperNormal presents a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. It optimizes a neural signed distance function with multi-resolution hash encoding, producing detailed surfaces comparable to 3D scanners efficiently."
  },
  {
    "prompt": "summary: High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patching and windowing operations of the SWIN transformer, enabling the network to process spherical representations with minimal computational overhead. We demonstrate the superior performance of our model on both synthetic and real automotive datasets, as well as a selection of other image datasets, for semantic segmentation, depth regression and classification tasks. Our",
    "chosen": "HEAL-SWIN is a transformer model combining the Hierarchical Equal Area iso-Latitude Pixelation grid with the Hierarchical Shifted-Window transformer. This enables efficient processing of high-resolution, distortion-free spherical data, demonstrating superior performance on automotive datasets for semantic segmentation, depth regression, and classification tasks.",
    "rejected": "HEAL-SWIN transformer combines the HEALPix grid used in astrophysics with SWIN transformer to handle high-resolution fisheye images effectively. It processes spherical representations with minimal computational overhead, showing superior performance in various computer vision tasks."
  },
  {
    "prompt": "summary: Advances in camera-based physiological monitoring have enabled the robust, non-contact measurement of respiration and the cardiac pulse, which are known to be indicative of the sleep stage. This has led to research into camera-based sleep monitoring as a promising alternative to “gold-standard” polysomnography, which is cumbersome, expensive to administer, and hence unsuitable for longer-term clinical studies. In this paper, we introduce SleepVST, a transformer model which enables state-of-theart performance in camera-based sleep stage classification (sleep staging). After pre-training on contact sensor data, SleepVST outperforms existing methods for cardiorespiratory sleep staging on the SHHS and MESA datasets, achieving total Cohen’s kappa scores of 0.75 and 0.77 respectively. We then show that SleepVST can be successfully transferred to cardio-respiratory waveforms extracted from video, enabling fully contact-free sleep staging. Using a video dataset of 50 nights, we achieve a total accuracy of 78.8% and a Cohen’s κ of 0.71 in four-class video-based sleep staging, setting a new state-of-the-art in the domain.",
    "chosen": "SleepVST is a transformer model for camera-based sleep stage classification. Pre-trained on contact sensor data, it outperforms existing methods for cardiorespiratory sleep staging and can be transferred to video-based staging. Using a video dataset of 50 nights, it achieves state-of-the-art performance in contact-free sleep staging.",
    "rejected": "This paper proposes Ev-DeblurNeRF, which combines blurry images and events to recover sharp NeRFs. It leverages event double integral as an additional prior and models the event-pixel response, enabling high-quality reconstructions under challenging conditions."
  },
  {
    "prompt": "summary: The landscape of deep learning research is moving towards innovative strategies to harness the true potential of data. Traditionally, emphasis has been on scaling model architectures, resulting in large and complex neural networks, which can be difficult to train with limited computational resources. However, independently of the model size, data quality (i.e. amount and variability) is still a major factor that affects model generalization. In this work, we propose a novel technique to exploit available data through the use of automatic data augmentation for the tasks of image classification and semantic segmentation. We introduce the first Differentiable Augmentation Search method (DAS) to generate variations of images that can be processed as videos. Compared to previous approaches, DAS is extremely fast and flexible, allowing the search on very large search spaces in less than a GPU day. Our intuition is that the increased receptive field in the temporal dimension provided by DAS could lead to benefits also to the spatial receptive field. More specifically, we leverage DAS to guide the reshaping of the spatial receptive field by selecting task-dependant transformations. As a result, compared to standard augmentation alternatives, we improve in terms of accuracy on ImageNet, Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when pluggingin our DAS over different light-weight video backbones.",
    "chosen": "DAS is a differentiable augmentation search method for image classification and semantic segmentation. It generates image variations processed as videos, guiding the reshaping of the spatial receptive field with task-dependent transformations. This improves accuracy on multiple datasets when integrated with light-weight video backbones.",
    "rejected": "Radiologists desire fully automated versatile AI for medical imaging interpretation. This paper leverages language as supervision by distilling diagnostic knowledge from pre-trained 2D X-ray models to interpret 3D chest CT images. It uses robust contrastive learning to align images and reports within patients, demonstrating feasibility in real-world applications."
  },
  {
    "prompt": "summary: Due to the high potential for abuse of GenAI systems, the task of detecting synthetic images has recently become of great interest to the research community. Unfortunately, existing image-space detectors quickly become obsolete as new high-fidelity text-to-image models are developed at blinding speed. In this work, we propose a new synthetic image detector that uses features obtained by inverting an open-source pre-trained Stable Diffusion model. We show that these inversion features enable our detector to generalize well to unseen generators of high visual fidelity (e.g., DALL·E 3) even when the detector is trained only on lower fidelity fake images generated via Stable Diffusion. This detector achieves new state-of-the-art across multiple training and evaluation setups. Moreover, we introduce a new challenging evaluation protocol that uses reverse image search to mitigate stylistic and thematic biases in the detector evaluation. We show that the resulting evaluation scores align well with detectors’ inthe-wild performance, and release these datasets as public benchmarks for future research. *Work done during an internship at Google Research.",
    "chosen": "FakeInversion is a synthetic image detector that uses features from an inverted Stable Diffusion model. This method generalizes well to unseen high-fidelity generators, even when trained on lower-fidelity images. It achieves state-of-the-art results across multiple training and evaluation setups and introduces a new evaluation protocol using reverse image search.",
    "rejected": "Due to the rapid development of high-fidelity text-to-image models, there is an increasing need for synthetic image detection. This research introduces a detector using inversion features from a Stable Diffusion model, proving effective against unseen generators. The evaluation protocol includes reverse image search to mitigate biases, aligning with real-world performance."
  },
  {
    "prompt": "summary: Detecting edges in images suffers from the problems of (P1) heavy imbalance between positive and negative classes as well as (P2) label uncertainty owing to disagreement between different annotators. Existing solutions address P1 using class-balanced cross-entropy loss and dice loss and P2 by only predicting edges agreed upon by most annotators. In this paper, we propose RankED, a uniﬁed ranking-based approach that addresses both the imbalance problem (P1) and the uncertainty problem (P2). RankED tackles these two problems with two components: One component which ranks positive pixels over negative pixels, and the second which promotes high conﬁdence edge pixels to have more label certainty. We show that RankED outperforms previous studies and sets a new state-of-the-art on NYUDv2, BSDS500 and Multi-cue datasets.",
    "chosen": "RankED is a unified ranking-based approach for edge detection that addresses both class imbalance and label uncertainty. It ranks positive pixels over negatives and promotes high-confidence edge pixels. RankED outperforms previous methods and sets a new state-of-the-art on NYUD-v2, BSDS500, and Multi-cue datasets.",
    "rejected": "This paper presents RankED, a ranking-based method addressing class imbalance and label uncertainty in edge detection. It ranks positive pixels over negatives and promotes high-confidence edge pixels, outperforming previous studies on datasets like NYUD-v2 and BSDS500."
  },
  {
    "prompt": "summary: Detecting objects in 3D under various (normal and adverse) weather conditions is essential for safe autonomous driving systems. Recent approaches have focused on employing weather-insensitive 4D radar sensors and leveraging them with other modalities, such as LiDAR. However, they fuse multi-modal information without considering the sensor characteristics and weather conditions, and lose some height information which could be useful for localizing 3D objects. In this paper, we propose a novel framework for robust LiDAR and 4D radar-based 3D object detection. Specifically, we propose a 3D-LRF module that considers the distinct patterns they exhibit in 3D space (e.g., precise 3D mapping of LiDAR and wide-range, weather insensitive measurement of 4D radar) and extract fusion features based on their 3D spatial relationship. Then, our weather-conditional radar-flow gating network modulates the information flow of fusion features depending on weather conditions, and obtains enhanced feature that effectively incorporates the strength of two domains under various weather conditions. The extensive experiments demonstrate that our model achieves SoTA performance for 3D object detection under various weather conditions.",
    "chosen": "This paper proposes a novel framework for robust LiDAR and 4D radar-based 3D object detection. It introduces a 3D-LRF module to extract fusion features based on the 3D spatial relationship of LiDAR and radar, and a weather-conditional radar-flow gating network to modulate information flow. The model achieves state-of-the-art performance under various weather conditions.",
    "rejected": "A novel framework for 3D object detection under diverse weather conditions is proposed, utilizing LiDAR and 4D radar. The approach, featuring a 3D-LRF module and weather-conditional radar-flow gating network, demonstrates state-of-the-art performance by effectively fusing sensor data based on their spatial relationships and weather conditions."
  },
  {
    "prompt": "summary: Group robustness strategies aim to mitigate learned biases in deep learning models that arise from spurious correlations present in their training datasets. However, most existing methods rely on the access to the label distribution of the groups, which is time-consuming and expensive to obtain. As a result, unsupervised group robustness strategies are sought. Based on the insight that a trained model’s classiﬁcation strategies can be inferred accurately based on explainability heatmaps, we introduce ExMap, an unsupervised two stage mechanism designed to enhance group robustness in traditional classiﬁers. ExMap utilizes a clustering module to infer pseudo-labels based on a model’s explainability heatmaps, which are then used during training in lieu of actual labels. Our empirical studies validate the efﬁcacy of ExMap - We demonstrate that it bridges the performance gap with its supervised counterparts and outperforms existing partially supervised and unsupervised methods. Additionally, ExMap can be seamlessly integrated with existing group robustness learning strategies. Finally, we demonstrate its potential in tackling the emerging issue of multiple shortcut mitigation1.",
    "chosen": "ExMap is an unsupervised two-stage mechanism enhancing group robustness in classifiers using explainability heatmaps for pseudo-label inference. It bridges the performance gap with supervised methods and outperforms existing unsupervised approaches, demonstrating effectiveness in mitigating spurious correlations.",
    "rejected": "ExMap, an unsupervised mechanism, enhances group robustness in classifiers without relying on labeled groups. By clustering explainability heatmaps, it infers pseudo-labels used during training, bridging the performance gap with supervised methods and outperforming existing partially supervised and unsupervised techniques."
  },
  {
    "prompt": "summary: Bird’s-eye View (BeV) representations have emerged as the de-facto shared space in driving applications, offering a unified space for sensor data fusion and supporting various downstream tasks. However, conventional models use grids with fixed resolution and range and face computational inefficiencies due to the uniform allocation of resources across all cells. To address this, we propose PointBeV, a novel sparse BeV segmentation model operating on sparse BeV cells instead of dense grids. This approach offers precise control over memory usage, enabling the use of long temporal contexts and accommodating memory constrained platforms. PointBeV employs an efficient two pass strategy for training, enabling focused computation on regions of interest. At inference time, it can be used with various memory/performance trade-offs and flexibly adjusts to new specific use cases. PointBeV achieves stateof-the-art results on the nuScenes dataset for vehicle, pedestrian, and lane segmentation, showcasing superior performance in static and temporal settings despite being trained solely with sparse signals. We release our",
    "chosen": "PointBeV is a sparse Bird’s-eye View (BeV) segmentation model operating on sparse BeV cells. It offers memory usage control, enabling long temporal contexts and efficient computation. PointBeV achieves state-of-the-art results on nuScenes for vehicle, pedestrian, and lane segmentation with sparse signals.",
    "rejected": "PointBeV is introduced as a sparse Bird’s-eye View (BeV) segmentation model that operates efficiently on sparse BeV cells instead of dense grids. This approach offers precise memory control and achieves state-of-the-art results on the nuScenes dataset, particularly in static and temporal settings, through its efficient two-pass training strategy."
  },
  {
    "prompt": "summary: Coordinate based implicit neural representations have gained rapid popularity in recent years as they have been successfully used in image, geometry and scene modeling tasks. In this work, we present a novel use case for such implicit representations in the context of learning anatomically constrained face models. Actor speciﬁc anatomically constrained face models are the state of the art in both facial performance capture and performance retargeting. Despite their practical success, these anatomical models are slow to evaluate and often require extensive data capture to be built. We propose the anatomical implicit face model; an ensemble of implicit neural networks that jointly learn to model the facial anatomy and the skin surface with high-ﬁdelity, and can readily be used as a drop in replacement to conventional blendshape models. Given an arbitrary set of skin surface meshes of an actor and only a neutral shape with estimated skull and jaw bones, our method can recover a dense anatomical substructure which constrains every point on the facial surface. We demonstrate the usefulness of our approach in several tasks ranging from shape ﬁtting, shape editing, and performance retargeting.",
    "chosen": "AIM is an ensemble of implicit neural networks modeling facial anatomy and skin surface with high-fidelity. It recovers dense anatomical substructures from skin surface meshes and neutral shapes, providing advantages in shape fitting, editing, and performance retargeting.",
    "rejected": "The anatomical implicit face model uses implicit neural networks to jointly learn facial anatomy and skin surface with high fidelity. Given skin surface meshes and neutral shapes with estimated bones, it recovers dense anatomical substructures, enabling applications ranging from shape fitting to performance retargeting."
  },
  {
    "prompt": "summary: Superpixels play a crucial role in image processing by partitioning an image into clusters of pixels with similar visual attributes. This facilitates subsequent image processing tasks, offering computational advantages over the manipulation of individual pixels. While numerous over-segmentation techniques have emerged in recent years, many rely on predefined initialization and termination criteria. In this paper, a novel top-down superpixel segmentation algorithm called Hierarchical Histogram Threshold Segmentation (HHTS) is introduced. It eliminates the need for initialization and implements auto-termination, outperforming state-of-the-art methods w.r.t. boundary recall. This is achieved by iteratively partitioning individual pixel segments into foreground and background and applying intensity thresholding across multiple color channels. The underlying iterative process constructs a superpixel hierarchy that adapts to local detail distributions until color information exhaustion. Experimental results demonstrate the superiority of the proposed approach in terms of boundary adherence, while maintaining competitive runtime performance on the BSDS500 and NYUV2 datasets. Furthermore, an application of HHTS in refining machine learning-based semantic segmentation masks produced by the Segment Anything Foundation Model (SAM) is presented.",
    "chosen": "HHTS is a novel top-down superpixel segmentation algorithm eliminating initialization and implementing auto-termination. It outperforms state-of-the-art methods in boundary recall by iteratively partitioning pixel segments and applying intensity thresholding, demonstrating superior boundary adherence on BSDS500 and NYUV2 datasets.",
    "rejected": "HHTS, a top-down superpixel segmentation algorithm, automatically terminates and outperforms state-of-the-art methods in boundary recall. It iteratively partitions pixel segments into foreground and background, applying intensity thresholding across multiple channels, demonstrating superior performance on BSDS500 and NYUV2 datasets."
  },
  {
    "prompt": "summary: The study of complex human interactions and group activities has become a focal point in human-centric computer vision. However, progress in related tasks is often hindered by the challenges of obtaining large-scale labeled datasets from real-world scenarios. To address the limitation, we introduce M3Act, a synthetic data generator for multi-view multi-group multi-person human atomic actions and group activities. Powered by Unity Engine, M3Act features multiple semantic groups, highly diverse and photorealistic images, and a comprehensive set of annotations, which facilitates the learning of human-centered tasks across single-person, multi-person, and multi-group conditions. We demonstrate the advantages of M3Act across three core experiments. The results suggest our synthetic dataset can significantly improve the performance of several downstream methods and replace real-world datasets to reduce cost. Notably, M3Act improves the state-of-the-art MOTRv2 on DanceTrack dataset, leading to a hop on the leaderboard from 10th to 2nd place. Moreover, M3Act opens new research for controllable 3D group activity generation. We define multiple metrics and propose a competitive baseline for the novel task. Our",
    "chosen": "M3Act is a synthetic data generator for multi-view, multi-group, multi-person human actions and activities. Powered by Unity Engine, it offers diverse, photorealistic images and comprehensive annotations, improving downstream methods and enabling controllable 3D group activity generation.",
    "rejected": "M3Act, a synthetic data generator for multi-view multi-group human activities, is presented. Utilizing Unity Engine, M3Act provides photorealistic images and comprehensive annotations, enhancing learning for human-centric tasks. It improves state-of-the-art MOTRv2 on DanceTrack, opening new research avenues for controllable 3D group activity generation."
  },
  {
    "prompt": "summary: 3D visual grounding involves matching natural language descriptions with their corresponding objects in 3D spaces. Existing methods often face challenges with accuracy in object recognition and struggle in interpreting complex linguistic queries, particularly with descriptions that involve multiple anchors or are view-dependent. In response, we present the MiKASA (Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model integrates a self-attention-based scene-aware object encoder and an original multi-key-anchor technique, enhancing object recognition accuracy and the understanding of spatial relationships. Furthermore, MiKASA improves the explainability of decision-making, facilitating error diagnosis. Our model achieves the highest overall accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets, particularly excelling by a large margin in categories that require viewpoint-dependent descriptions. The source",
    "chosen": "MiKASA is a transformer model for 3D visual grounding integrating a scene-aware object encoder and multi-key-anchor technique. It enhances object recognition accuracy and spatial relationship understanding, achieving the highest accuracy in the Referit3D challenge.",
    "rejected": "The paper discusses the challenges in detecting synthetic images due to advancements in generative AI systems. A new detector leveraging inversion features from a pre-trained model shows robust generalization to unseen generators. Additionally, a novel evaluation protocol using reverse image search helps mitigate biases in detector evaluations."
  },
  {
    "prompt": "summary: The spiking cameras offer the benefits of high dynamic range (HDR), high temporal resolution, and low data redundancy. However, reconstructing HDR videos in high-speed conditions using single-bit spikings presents challenges due to the limited bit depth. Increasing the bit depth of the spikings is advantageous for boosting HDR performance, but the readout efficiency will be decreased, which is unfavorable for achieving a high frame rate (HFR) video. To address these challenges, we propose a readout mechanism to obtain rolling-mixed-bit (RMB) spikings, which involves interleaving multi-bit spikings within the single-bit spikings in a rolling manner, thereby combining the characteristics of high bit depth and efficient readout. Furthermore, we introduce RMB-Net for reconstructing HDR and HFR videos. RMBNet comprises a cross-bit attention block for fusing mixed-bit spikings and a cross-time attention block for achieving temporal fusion. Extensive experiments conducted on synthetic and real-synthetic data demonstrate the superiority of our method. For instance, pure 3-bit spikings result in 3 times of data volume, whereas our method achieves comparable performance with less than 2% increase in data volume.",
    "chosen": "RMB-Net reconstructs HDR and HFR videos from rolling-mixed-bit spikings. It uses a cross-bit attention block for fusing mixed-bit spikings and a cross-time attention block for temporal fusion, achieving superior performance with minimal data volume increase.",
    "rejected": "A novel approach for robust 3D object detection under various weather conditions combines LiDAR and 4D radar data. By considering distinct patterns in 3D space and modulating information flow based on weather conditions, this method enhances fusion features, achieving state-of-the-art performance in adverse weather scenarios."
  },
  {
    "prompt": "summary: In subject-driven text-to-image synthesis, the synthesis process tends to be heavily inﬂuenced by the reference images provided by users, often overlooking crucial attributes detailed in the text prompt. In this work, we propose Subject-Agnostic Guidance (SAG), a simple yet effective solution to remedy the problem. We show that through constructing a subject-agnostic condition and applying our proposed dual classiﬁer-free guidance, one could obtain outputs consistent with both the given subject and input text prompts. We validate the efﬁcacy of our approach through both optimization-based and encoder-based methods. Additionally, we demonstrate its applicability in second-order customization methods, where an encoder-based model is ﬁne-tuned with DreamBooth. Our approach is conceptually simple and requires only minimal",
    "chosen": "Subject-Agnostic Guidance (SAG) is introduced to improve subject-driven text-to-image synthesis. SAG constructs a subject-agnostic condition and applies dual classifier-free guidance, ensuring outputs align with both the subject and text prompts. This method enhances quality in optimization-based, encoder-based, and second-order customization approaches.",
    "rejected": "In subject-driven text-to-image synthesis, existing methods often emphasize subject-specific details over attributes described in text prompts. This paper introduces Subject-Agnostic Guidance (SAG), which ensures outputs align with both the subject and input text descriptions. SAG uses dual classifier-free guidance to improve image synthesis quality significantly."
  },
  {
    "prompt": "summary: Single-photon Light Detection and Ranging (LiDAR) systems are often equipped with an array of detectors for improved spatial resolution and sensing speed. However, given a fixed amount of flux produced by the laser transmitter across the scene, the per-pixel Signal-to-Noise Ratio (SNR) will decrease when more pixels are packed in a unit space. This presents a fundamental trade-off between the spatial resolution of the sensor array and the SNR received at each pixel. Theoretical characterization of this fundamental limit is explored. By deriving the photon arrival statistics and introducing a series of new approximation techniques, the Mean Squared Error (MSE) of the maximum-likelihood estimator of the time delay is derived. The theoretical predictions align well with simulations and real data.",
    "chosen": "This paper explores the fundamental trade-off in single-photon LiDAR between spatial resolution and Signal-to-Noise Ratio (SNR). It derives the Mean Squared Error (MSE) of the maximum-likelihood estimator for time delay, providing theoretical insights into the sensor's performance limits.",
    "rejected": "Single-photon LiDAR systems face a trade-off between spatial resolution and per-pixel Signal-to-Noise Ratio (SNR). By analyzing photon arrival statistics, this paper derives the Mean Squared Error (MSE) of time delay estimations, providing theoretical insights into optimizing SNR versus spatial resolution in single-photon detectors."
  },
  {
    "prompt": "summary: We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a re-parameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field. Additional materials can be found on the project website. 1",
    "chosen": "pixelSplat is a feed-forward model reconstructing 3D radiance fields from image pairs using 3D Gaussian primitives. It offers real-time rendering and memory-efficient training, outperforming state-of-the-art methods in novel view synthesis with faster rendering and explicit 3D representations.",
    "rejected": "PixelSplat is introduced as a feed-forward model for reconstructing 3D radiance fields from pairs of images using 3D Gaussian primitives. It offers real-time rendering and efficient memory usage, making it suitable for scalable training and fast 3D reconstruction. Experiments show superior performance compared to state-of-the-art methods."
  },
  {
    "prompt": "summary: This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category, which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g., contacts and semantics) from text prompts. To address this challenge, we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation, a VAE-based network takes as input a text and an object mesh, and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local geometry structure of diverse objects that is independent of the objects’ category, and thus, it is applicable to general objects. For motion generation, a Transformer-based diffusion model utilizes this 3D contact map as a strong prior for generating physically This research was conducted when Jihyeon Kim was a graduate student (Master candidate) at UNIST†. Co-last authors∗. plausible hand-object motion as a function of text prompts by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally, we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the objecthand contacts and to suppress the penetration artifacts. In the experiments, we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research.",
    "chosen": "A method for generating 3D hand-object interaction sequences from text prompts is proposed. It decomposes the task into contact generation and motion generation, using a VAE-based network and a Transformer-based diffusion model. This approach generates diverse interactions applicable to unseen objects.",
    "rejected": "This paper presents a novel method for generating sequences of hand-object interactions in 3D guided by text. It decomposes the task into contact generation and motion generation subtasks, utilizing a VAE-based network and Transformer-based diffusion model respectively. The approach generates realistic interactions applicable to unseen objects, enhancing the potential for diverse applications."
  },
  {
    "prompt": "summary: We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then, in the following, we study unsupervised representation learning with such hierarchical correlation clustering. For this purpose, we first investigate embedding the respective hierarchy to be used for tree preserving embedding and feature extraction. Thereafter, we study the extension of minimax distance measures to correlation clustering, as another representation learning paradigm. Finally, we demonstrate the performance of our methods on several datasets.",
    "chosen": "Hierarchical correlation clustering is extended to handle positive and negative pairwise dissimilarities. The paper studies unsupervised representation learning using hierarchical correlation clustering, proposing methods for tree preserving embedding and feature extraction.",
    "rejected": "A hierarchical correlation clustering method is proposed that extends traditional correlation clustering to handle positive and negative pairwise dissimilarities. It explores unsupervised representation learning through embedding hierarchies and minimax distance measures. This approach provides an effective solution for clustering complex data structures."
  },
  {
    "prompt": "summary: Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to “think” from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate twenty-one popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics. *Equal contribution, ‡ Project lead, B Corresponding author",
    "chosen": "EgoThink is a visual question-answering benchmark assessing VLMs' ability to think from a first-person perspective. It includes six core capabilities with twelve detailed dimensions, evaluated using manually annotated ego-centric video clips and GPT-4 for answer grading.",
    "rejected": "EgoThink benchmarks vision-language models on their ability to understand first-person perspective tasks. With six core capabilities and twelve detailed dimensions, it evaluates twenty-one popular models using GPT-4 for grading. Results indicate significant room for improvement in first-person perspective understanding, highlighting the benchmark's importance for future research."
  },
  {
    "prompt": "summary: This paper proposes a novel task named ”3D part grouping”. Suppose there is a mixed set containing scattered parts from various shapes. This task requires algorithms to find out every possible combination among all the parts. To address this challenge, we propose the so called Gradient Field-based Auto-Regressive Sampling framework (GFARS) tailored specifically for the 3D part grouping task. In our framework, we design a gradient-field-based selection graph neural network (GNN) to learn the gradients of a log conditional probability density in terms of part selection, where the condition is the given mixed part set. This innovative approach, implemented through the gradient-field-based selection GNN, effectively captures complex relationships among all the parts in the input. Upon completion of the training process, our framework becomes capable of autonomously grouping 3D parts by iteratively selecting them from the mixed part set, leveraging the knowledge acquired by the trained gradient-field-based selection GNN. Our",
    "chosen": "G-FARS is a framework for 3D part grouping using a gradient-field-based selection graph neural network. It learns part relationships and groups 3D parts autonomously, applicable to mixed part sets with unknown group numbers.",
    "rejected": "Introducing \"3D part grouping,\" this paper proposes a Gradient Field-based Auto-Regressive Sampling framework (G-FARS) designed to identify all possible combinations among scattered parts. Using a gradient-field-based selection graph neural network, G-FARS autonomously groups parts from mixed sets, demonstrating its capability in handling complex part relationships."
  },
  {
    "prompt": "summary: Backdoor attack poses a significant security threat to Deep Learning applications. Existing attacks are often not evasive to established backdoor detection techniques. This susceptibility primarily stems from the fact that these attacks typically leverage a universal trigger pattern or transformation function, such that the trigger can cause mis-classification for any input. In response to this, recent papers have introduced attacks using sample-specific invisible triggers crafted through special transformation functions. While these approaches manage to evade detection to some extent, they reveal vulnerability to existing backdoor mitigation techniques. To address and enhance both evasiveness and resilience, we introduce a novel backdoor attack LOTUS. Specifically, it leverages a secret function to separate samples in the victim class into a set of partitions and applies unique triggers to different partitions. Furthermore, LOTUS incorporates an effective trigger focusing mechanism, ensuring only the trigger corresponding to the partition can induce the backdoor behavior. Extensive experimental results show that LOTUS can achieve high attack success rate across 4 datasets and 7 model structures, and effectively evading 13 backdoor detection and mitigation techniques.",
    "chosen": "LOTUS is a novel backdoor attack leveraging secret functions to partition victim-class samples and apply unique triggers. It incorporates a trigger focusing mechanism, achieving high attack success rates across datasets and model structures while evading detection and mitigation techniques.",
    "rejected": "LOTUS is a novel backdoor attack leveraging a secret function to partition victim class samples and apply unique triggers to different partitions. It includes a trigger focusing mechanism ensuring only designated triggers can induce backdoor behavior. LOTUS achieves high attack success rates across various datasets and models while evading numerous detection and mitigation techniques."
  },
  {
    "prompt": "summary: We present Cutie, a video object segmentation (VOS) network with object-level memory reading, which puts the object representation from memory back into the video object segmentation result. Recent works on VOS employ bottom-up pixel-level memory reading which struggles due to matching noise, especially in the presence of distractors, resulting in lower performance in more challenging data. In contrast, Cutie performs top-down object-level memory reading by adapting a small set of object queries. Via those, it interacts with the bottom-up pixel features iteratively with a query-based object transformer (qt, hence Cutie). The object queries act as a high-level summary of the target object, while high-resolution feature maps are retained for accurate segmentation. Together with foreground-background masked attention, Cutie cleanly separates the semantics of the foreground object from the background. On the challenging MOSE dataset, Cutie improves by 8.7 J &F over XMem with a similar running time and improves by 4.2 J &F over DeAOT while being three times faster.",
    "chosen": "Cutie is a video object segmentation network with object-level memory reading using object queries and a query-based transformer. It improves segmentation accuracy by separating foreground and background semantics, outperforming existing methods on challenging datasets.",
    "rejected": "We present Cutie, a video object segmentation method that leverages spatiotemporal context for accurate and efficient segmentation. Cutie integrates deep learning techniques with dynamic programming to refine object boundaries over time, achieving state-of-the-art performance on several benchmark datasets."
  },
  {
    "prompt": "summary: The problem of calibrating deep neural networks (DNNs) for multi-label learning is considered. It is wellknown that DNNs trained by cross-entropy for single-label, or one-hot, classification are poorly calibrated. Many calibration techniques have been proposed to address the problem. However, little attention has been paid to the calibration of multi-label DNNs. In this literature, the focus has been on improving labeling accuracy in the face of severe dataset unbalance. This is addressed by the",
    "chosen": "A new Strictly Proper Asymmetric (SPA) loss and Label Pair Regularizer (LPR) are proposed to calibrate multi-label DNNs. These methods address poor calibration in multi-label learning, significantly reducing calibration error while maintaining accuracy.",
    "rejected": "This paper addresses the calibration of deep neural networks (DNNs) for multi-label learning. While asymmetric losses improve labeling accuracy under dataset imbalance, they fail to produce well-calibrated classifiers. The authors show that these losses lack strictly proper properties, essential for accurate probability estimation. To solve this, they propose a Strictly Proper Asymmetric (SPA) loss and introduce a Label Pair Regularizer (LPR) to increase calibration constraints per training example. Extensive experiments validate the effectiveness of both contributions, significantly reducing calibration error while maintaining state-of-the-art accuracy. This work highlights the importance of calibration in safety-critical applications such as medical diagnosis and cost-sensitive domains like fraud detection."
  },
  {
    "prompt": "summary: Image denoising is a fundamental task in computer vision. While prevailing deep learning-based supervised and self-supervised methods have excelled in eliminating in distribution noise, their susceptibility to out-of-distribution (OOD) noise remains a significant challenge. The recent emergence of contrastive language-image pre-training (CLIP) model has showcased exceptional capabilities in open-world image recognition and segmentation. Yet, the potential for leveraging CLIP to enhance the robustness of low-level tasks remains largely unexplored. This paper uncovers that certain dense features extracted from the frozen ResNet image encoder of CLIP exhibit distortion-invariant and content-related properties, which are highly desirable for generalizable denoising. Leveraging these properties, we devise an asymmetrical encoder-decoder denoising network, which incorporates dense features including the noisy image and its multi-scale features from the frozen ResNet encoder of CLIP into a learnable image decoder to achieve generalizable denoising. The progressive feature augmentation strategy is further proposed to mitigate feature overfitting and improve the robustness of the learnable decoder. Extensive experiments and comparisons conducted across diverse OOD noises, including synthetic noise, real-world sRGB noise, and low-dose CT image noise, demonstrate the superior generalization ability of our method.",
    "chosen": "This paper proposes a method to enhance the generalization of image denoising models using features from the frozen ResNet encoder of CLIP. The authors devise an asymmetrical encoder-decoder network and introduce a progressive feature augmentation strategy. Experiments show superior generalization across diverse out-of-distribution noises.",
    "rejected": "Image denoising methods struggle with out-of-distribution (OOD) noise. This paper explores using dense features from CLIP's frozen ResNet encoder for generalizable denoising, proposing an asymmetrical encoder-decoder network. It incorporates a progressive feature augmentation strategy to mitigate overfitting, demonstrating superior performance across various OOD noises."
  },
  {
    "prompt": "summary: The Segment Anything Model (SAM) has garnered significant attention for its versatile segmentation abilities and intuitive prompt-based interface. However, its application in medical imaging presents challenges, requiring either substantial training costs and extensive medical datasets for full model fine-tuning or high-quality prompts for optimal performance. This paper introduces H-SAM: a prompt-free adaptation of SAM tailored for efficient fine-tuning of medical images via a two-stage hierarchical decoding procedure. In the initial stage, H-SAM employs SAM’s original decoder to generate a prior probabilistic mask, guiding a more intricate decoding process in the second stage. Specifically, we propose two key designs: 1) A class-balanced, mask-guided self-attention mechanism addressing the unbalanced label distribution, enhancing image embedding; 2) A learnable mask cross-attention mechanism spatially modulating the interplay among different image regions based on the prior mask. Moreover, the inclusion of a hierarchical pixel decoder in H-SAM enhances its proficiency in capturing fine-grained and localized details. This approach enables SAM to effectively integrate learned medical priors, facilitating enhanced adaptation for medical image segmentation with limited samples. Our H-SAM demonstrates a 4.78% improvement in average Dice compared to existing prompt-free SAM variants for multi-organ segmentation using only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM even outperforms state-of-the-art semisupervised models relying on extensive unlabeled training data across various medical datasets. Our",
    "chosen": "This paper introduces H-SAM, a prompt-free adaptation of SAM for efficient fine-tuning in medical image segmentation. It employs a two-stage hierarchical decoding procedure with novel attention mechanisms and a hierarchical pixel decoder. H-SAM achieves significant improvements in average Dice scores with limited training data and outperforms semi-supervised models without using unlabeled data.",
    "rejected": "H-SAM enhances Segment Anything Model (SAM) for medical image segmentation through a prompt-free adaptation using a two-stage hierarchical decoding procedure. By addressing unbalanced label distribution and spatial modulation among regions, H-SAM shows a 4.78% improvement in Dice score compared to existing SAM variants, even outperforming state-of-the-art semi-supervised models."
  },
  {
    "prompt": "summary: Driver’s eye gaze holds a wealth of cognitive and intentional cues crucial for intelligent vehicles. Despite its signiﬁcance, research on in-vehicle gaze estimation remains limited due to the scarcity of comprehensive and well an notated datasets in real driving scenarios. In this paper, we present three novel elements to advance in-vehicle gaze research. Firstly, we introduce IVGaze, a pioneering dataset capturing in-vehicle gaze, collected from 125 subjects and covering a large range of gaze and head poses within vehicles. In this dataset, we propose a new vision-based solution for in-vehicle gaze collection, introducing a reﬁned gaze target calibration method to tackle annotation challenges. Second, our research focuses on in-vehicle gaze estimation leveraging the IVGaze. In-vehicle face images often suffer from low resolution, prompting our",
    "chosen": "This paper presents IVGaze, a novel dataset for in-vehicle gaze estimation, along with a vision-based collection solution. The authors propose gaze pyramid transformers and a dual-stream gaze pyramid transformer (GazeDPTR) for accurate gaze estimation. They also explore gaze zone classification by projecting gaze onto tri-planes and leveraging positional features and visual attributes.",
    "rejected": "This paper introduces IVGaze, a dataset for in-vehicle gaze estimation, and proposes GazeDPTR, which utilizes transformer-based multilevel features integration and dual-stream processing. GazeDPTR achieves state-of-the-art performance on the IVGaze dataset, enhancing accuracy in gaze zone classification by defining tri-plane projections and leveraging positional features from these projections."
  },
  {
    "prompt": "summary: Large-scale visual-language pre-trained models have achieved significant success in various video tasks. However, most existing methods follow an “adapt then align” paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static images to complicated activity concepts. In this paper, we propose a novel “Align before Adapt” (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This paradigm reuses the visual-language alignment of VLP during adaptation and tries to explain an action by the underlying entities. This helps understand actions by bridging the gap with complex activity semantics, particularly when facing unfamiliar or unseen categories. ALT demonstrates competitive performance while maintaining remarkably low computational costs. In fully supervised experiments, it achieves 88.1% top-1 accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the previous state-of-the-art methods in both zero-shot and fewshot experiments, emphasizing its superior generalizability across various learning scenarios.",
    "chosen": "This paper proposes \"Align before Adapt\" (ALT), a novel paradigm for video action recognition that establishes entity-to-region alignments before adapting to video representation learning. ALT reuses visual-language alignments during adaptation and demonstrates competitive performance with low computational costs across fully supervised, zero-shot, and few-shot experiments.",
    "rejected": "ALT paradigm advances video action recognition by aligning entity-to-region before adapting to video-level representations. By grounding text embeddings of relevant entities into image regions, ALT bridges the gap between static images and complex activity semantics. It achieves competitive performance while maintaining low computational costs, outperforming previous methods in both zero-shot and few-shot scenarios."
  },
  {
    "prompt": "summary: We present the pioneering Large Visual Motion Model (LVMM), meticulously engineered to analyze the intrinsic dynamics encapsulated within real-world imagery. Our model, fortified with a wealth of prior knowledge extracted from billions of image pairs, demonstrates promising results in predicting a diverse spectrum of scene dynamics. As a result, it can infuse any generic image with authentic dynamic effects, enhancing its visual allure.",
    "chosen": "This paper presents LVMM, a large visual motion model designed to predict local motion within scenes. The model comprises a motion rendering model and a motion diffusion model, and it can infuse static images with dynamic effects. The authors demonstrate promising results in predicting diverse scene dynamics.",
    "rejected": "LVMM predicts scene dynamics to animate static images, enhancing visual allure. The model extracts latent motion vectors and generates motion trajectories using a diffusion model. LVMM accurately predicts local motion trajectory, producing realistic dynamic effects. Its contributions include pioneering the prediction of diverse scene dynamics and introducing mechanisms for motion rendering and diffusion."
  },
  {
    "prompt": "summary: Recent advancements in text-to-image technology have significantly advanced the field of image customization. Among various applications, the task of customizing diverse scenes for user-specified composited elements holds great application value but has not been extensively explored. Addressing this gap, we propose AnyScene, a specialized framework designed to create varied scenes from composited foreground using textual prompts. AnyScene addresses the primary challenges inherent in existing methods, particularly scene disharmony due to a lack of foreground semantic understanding and distortion of foreground elements. Specifically, we develop a foreground injection module that guides a pre-trained diffusion model to generate cohesive scenes in visual harmony with the provided foreground. To enhance robust generation, we implement a layout control strategy that prevents distortions of foreground elements. Furthermore, an efficient image blending mechanism seamlessly reintegrates foreground details into the generated scenes, producing outputs with overall visual harmony and precise foreground details. In addition, we propose a new benchmark and a series of quantitative metrics to evaluate this proposed image customization task. Extensive experimental results demonstrate the effectiveness of AnyScene, which confirms its potential in various applications.",
    "chosen": "This paper introduces AnyScene, a framework for creating customized scenes from composited foregrounds using textual prompts. It includes a foreground injection module, layout control strategy, and image blending mechanism. The framework addresses scene disharmony and foreground distortion issues and shows effectiveness in various applications.",
    "rejected": "AnyScene customizes scenes based on composited foreground elements using textual prompts. It addresses disharmony and distortion issues by developing a foreground injection module, layout control strategy, and efficient image blending mechanism. AnyScene ensures visual harmony and precise foreground details, demonstrating its effectiveness in various applications through extensive experiments."
  },
  {
    "prompt": "summary: This work introduces ArtAdapter, a transformative textto-image (T2I) style transfer framework that transcends traditional limitations of color, brushstrokes, and object shape, capturing high-level style elements such as composition and distinctive artistic expression. The integration of a multilevel style encoder with our proposed explicit adaptation mechanism enables ArtAdapter to achieve unprecedented fidelity in style transfer, ensuring close alignment with textual descriptions. Additionally, the incorporation of an Auxiliary Content Adapter (ACA) effectively separates content from style, alleviating the borrowing of content from style references. Moreover, our novel fast finetuning approach could further enhance zero-shot style representation while mitigating the risk of overfitting. Comprehensive evaluations confirm that ArtAdapter surpasses current state-ofthe-art methods.",
    "chosen": "This paper presents ArtAdapter, a text-to-image style transfer framework that captures high-level style elements. It integrates a multi-level style encoder with an explicit adaptation mechanism and an Auxiliary Content Adapter. The framework achieves unprecedented fidelity in style transfer and surpasses current state-of-the-art methods.",
    "rejected": "ArtAdapter transforms text descriptions into stylistically rich images, capturing high-level style elements beyond color and brushstrokes. Featuring a multi-level style encoder and explicit adaptation mechanism, it separates content from style and employs fast finetuning to enhance zero-shot style representation. Comprehensive evaluations confirm ArtAdapter's superiority over current state-of-the-art methods."
  },
  {
    "prompt": "summary: Understanding how attention varies across individuals has significant scientific and societal impacts. However, existing visual scanpath models treat attention uniformly, neglecting individual differences. To bridge this gap, this paper focuses on individualized scanpath prediction (ISP), a new attention modeling task that aims to accurately predict how different individuals shift their attention in diverse visual tasks. It proposes an ISP method featuring three novel technical components: (1) an observer encoder to characterize and integrate an observer’s unique attention traits, (2) an observer-centric feature integration approach that holistically combines visual features, task guidance, and observer-specific characteristics, and (3) an adaptive fixation prioritization mechanism that refines scanpath predictions by dynamically prioritizing semantic feature maps based on individual observers’ attention traits. These novel components allow scanpath models to effectively address the attention variations across different observers. Our method is generally applicable to different datasets, model architectures, and visual tasks, offering a comprehensive tool for transforming general scanpath models into individualized ones. Comprehensive evaluations using valuebased and ranking-based metrics verify the method’s effectiveness and generalizability.",
    "chosen": "This paper focuses on individualized scanpath prediction (ISP) to model individual variations in eye movements. The proposed method includes an observer encoder, observer-centric feature integration, and adaptive fixation prioritization. The approach effectively transforms general scanpath models into individualized ones and shows strong generalizability.",
    "rejected": "ISP method focuses on predicting individual scanpaths during visual tasks. It integrates observer-specific characteristics, combines task guidance with visual features, and refines predictions using an adaptive fixation prioritization mechanism. The approach effectively addresses attention variations across individuals, verified through comprehensive evaluations on various datasets and visual tasks."
  },
  {
    "prompt": "summary: How to effectively explore multi-scale representations of rain streaks is important for image deraining. In contrast to existing Transformer-based methods that depend mostly on single-scale rain appearance, we develop an end-to-end multi-scale Transformer that leverages the potentially useful features in various scales to facilitate high-quality image reconstruction. To better explore the common degradation representations from spatially-varying rain streaks, we incorporate intra-scale implicit neural representations based on pixel coordinates with the degraded inputs in a closed-loop design, enabling the learned features to facilitate rain removal and improve the robustness of the model in complex scenarios. To ensure richer collaborative representation from different scales, we embed a simple yet effective inter-scale bidirectional feedback operation into our multi-scale Transformer by performing coarse-to-ﬁne and ﬁne-to-coarse information communication. Extensive experiments demonstrate that our approach, named as NeRDRain, performs favorably against the state-of-the-art ones on both synthetic and real-world benchmark datasets. The source",
    "chosen": "This paper develops an end-to-end multi-scale Transformer for image deraining, incorporating intra-scale implicit neural representations and inter-scale bidirectional feedback. The method, named NeRD-Rain, explores multi-scale representations and demonstrates favorable performance against state-of-the-art approaches on various benchmark datasets.",
    "rejected": "This paper proposes an efficient data-driven solution for self-localization within a floorplan, which does not require retraining per map or location and avoids needing a large image database of the area. The method introduces a novel probabilistic model with an observation module (comprising single and multiview components) and a temporal filtering module. It operates using a ray-based representation internally and is designed to work on conventional consumer hardware, overcoming common limitations such as the need for upright images. The system meets real-time requirements and outperforms state-of-the-art methods significantly."
  },
  {
    "prompt": "summary: An effective pre-training framework with universal 3D representations is extremely desired in perceiving largescale dynamic scenes. However, establishing such an ideal framework that is both task-generic and label-efficient poses a challenge in unifying the representation of the same primitive across diverse scenes. The current contrastive 3D pre-training methods typically follow a framelevel consistency, which focuses on the 2D-3D relationships in each detached image. Such inconsiderate consistency greatly hampers the promising path of reaching an universal pre-training framework: (1) The cross-scene semantic self-conflict, i.e., the intense collision between primitive segments of the same semantics from different scenes; (2) Lacking a globally unified bond that pushes the cross-scene semantic consistency into 3D representation learning. To This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 19925",
    "chosen": "This paper addresses the challenge of establishing a universal pre-training framework for large-scale dynamic scene perception. The authors propose a method to unify cross-scene semantic consistency in 3D representation learning, tackling issues like cross-scene semantic self-conflict and the lack of a globally unified bond in current contrastive 3D pre-training methods.",
    "rejected": "This research explores Handwritten Mathematical Expression Generation (HMEG) from symbolic sequences, framing it as a graph-to-image (G2I) generation problem. To address the lack of real masks for supervision, the authors propose an end-to-end G2I pipeline consisting of graph to layout, layout to mask, and mask to image stages. They introduce a Less-is-More (LiM) learning strategy to enhance prediction accuracy and a differentiable layout refinement module to clarify ambiguous areas. Their model, optimized in an end-to-end manner, generates high-quality HME images, surpassing previous generative methods. Additionally, the generated images improve HME recognition models through data augmentation."
  },
  {
    "prompt": "summary: The rebroadcasting of screen-recaptured document images introduces a signiﬁcant risk to the conﬁdential documents processed in government departments and commercial companies. However, detecting recaptured document images subjected to distortions from online social networks (OSNs) is challenging since the common forensics cues, such as moir´e pattern, are weakened during transmission. In this work, we ﬁrst devise a pixel-level distortion model of the screen-recaptured document image to identify the robust features of color artifacts. Then, we extract a chromaticity map from the recaptured image to highlight the presence of color artifacts even under low-quality samples. Based on the prior understanding, we design a chromaticity map adapter (CMA) to efﬁciently extract the chromaticity map, and feed it into the transformer backbone as multi-modal prompt tokens. To evaluate the performance of the proposed method, we collect a recaptured ofﬁce document image dataset with over 10K diverse samples. Experimental results demonstrate that the proposed CMA method outperforms a SOTA approach (with RGB modality only), reducing the average EER from 26.82% to 16.78%. Robustness evaluation shows that our method achieves 0.8688 and 0.7554 AUCs under samples with JPEG compression (QF=70) and resolution as low as 534×503 pixels.",
    "chosen": "This paper presents a method to detect screen-recaptured document images using a chromaticity map adapter (CMA). The CMA extracts chromaticity maps highlighting color artifacts, which are fed into a transformer backbone. Evaluated on a dataset with over 10K samples, the method reduces average EER from 26.82% to 16.78% and shows robustness under JPEG compression and low resolution.",
    "rejected": "This paper introduces a method for detecting screen-recaptured document images by analyzing color artifacts using a chromaticity map adapter (CMA) and transformer backbone. The proposed approach improves upon existing state-of-the-art methods, reducing the average Equal Error Rate (EER) from 26.82% to 16.78%, even under JPEG compression and low-resolution conditions."
  },
  {
    "prompt": "summary: Passive depth estimation based on stereo or defocus relies on the presence of the texture on an object to resolve its depth. Hence, recovering the depth of a textureless object— for example, a large white wall—is not just hard but perhaps even impossible. Or is it? We show that spatial coherence, a property of natural light sources, can be used to resolve the depth of a scene point even when it is textureless. Our approach relies on the idea that natural light scattered off a scene point is locally coherent with itself, while incoherent with the light scattered from other surface points; we use this insight to design an optical setup that uses selfinterference as a texture feature for estimating depth. Our lab prototype is capable of resolving depths of textureless objects in sunlight as well as indoor lights.",
    "chosen": "This paper introduces a novel approach for passive 3D reconstruction of textureless objects using spatial coherence of natural light. The method leverages self-interference as a texture feature, enabling depth estimation of textureless surfaces. A lab prototype demonstrates successful depth resolution of textureless objects in various lighting conditions.",
    "rejected": "The research presents an innovative technique for depth estimation of textureless objects through spatial coherence in natural light sources. By utilizing self-interference as a texture feature, the optical setup can estimate depths accurately under both sunlight and indoor lighting conditions, overcoming traditional limitations of passive depth estimation methods."
  },
  {
    "prompt": "summary: This paper proposes ConsistDreamer – a novel framework that lifts 2D diffusion models with 3D awareness and 3D consistency, thus enabling high-fidelity instructionguided scene editing. To overcome the fundamental limitation of missing 3D consistency in 2D diffusion models, our key insight is to introduce three synergistic strategies that augment the input of the 2D diffusion model to become 3D-aware and to explicitly enforce 3D consistency during the training process. Specifically, we design surrounding views as context-rich input for the 2D diffusion model, and generate 3D-consistent structured noise instead of image-independent noise. Moreover, we introduce selfsupervised consistency-enforcing training within the perscene editing procedure. Extensive evaluation shows that our ConsistDreamer achieves state-of-the-art performance for instruction-guided scene editing across various scenes and editing instructions, particularly in complicated largescale indoor scenes from ScanNet++, with significantly improved sharpness and fine-grained textures. Notably, ConsistDreamer stands as the first work capable of successfully editing complex (e.g., plaid/checkered) patterns. Our",
    "chosen": "This paper proposes ConsistDreamer, a framework enhancing 2D diffusion models with 3D awareness for high-fidelity scene editing. The approach introduces three strategies: using surrounding views as context, generating 3D-consistent noise, and self-supervised consistency training. ConsistDreamer achieves state-of-the-art performance in instruction-guided scene editing, particularly in complex indoor scenes.",
    "rejected": "ConsistDreamer is introduced as a novel framework that enhances 2D diffusion models with 3D awareness and consistency, enabling high-fidelity instruction-guided scene editing. It achieves state-of-the-art performance across various scenes, particularly improving sharpness and fine-grained textures in large-scale indoor scenes."
  },
  {
    "prompt": "summary: In this paper, we democratise caricature generation, empowering individuals to effortlessly craft personalised caricatures with just a photo and a conceptual sketch. Our objective is to strike a delicate balance between abstraction and identity, while preserving the creativity and subjectivity inherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing alongside single-image personalisation, selectively applying nuanced edits to cross-attention layers for a seamless merge of identity and style. Additionally, we propose Random Mask Reconstruction to enhance robustness, directing the model to focus on distinctive identity and style features. Crucially, our aim is not to replace artists but to eliminate accessibility barriers, allowing enthusiasts to engage in the artistry.",
    "chosen": "This paper presents a method for generating personalized caricatures from a photo and a conceptual sketch. The approach combines Explicit Rank-1 Model Editing and single-image personalization, focusing on cross-attention layers. Random Mask Reconstruction is used to enhance robustness, ensuring a balance between identity and style.",
    "rejected": "A new method for generating personalized caricatures from photos and conceptual sketches is proposed, balancing abstraction and identity while preserving artistic creativity. Techniques like Explicit Rank-1 Model Editing and Random Mask Reconstruction are employed to enhance robustness and ensure seamless integration of identity and style features."
  },
  {
    "prompt": "summary: We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusionbased co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpaintingbased sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms the superiority of DiffSHEG over prior approaches. By enabling the real-time generation of expressive and synchronized motions, DiffSHEG showcases its potential for various applications in the development of digital humans and embodied agents.",
    "chosen": "This paper proposes DiffSHEG, a diffusion-based approach for speech-driven holistic 3D expression and gesture generation. The method enables uni-directional information flow from expression to gesture and introduces an outpainting-based sampling strategy for generating sequences of arbitrary length. Evaluated on public datasets, DiffSHEG achieves state-of-the-art performance.",
    "rejected": "DiffSHEG is a diffusion-based model designed for speech-driven holistic 3D expression and gesture generation, capable of producing synchronized expressions and gestures from arbitrary length audio inputs. Evaluation on public datasets shows superior qualitative and quantitative performance, offering potential applications in digital human development and embodied agents."
  },
  {
    "prompt": "summary: We present a method for large-mask pluralistic image inpainting based on the generative framework of discrete latent codes. Our method learns latent priors, discretized as tokens, by only performing computations at the visible locations of the image. This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional transformer that infers the missing labels by only looking at these tokens, and a dedicated synthesis network that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings. Experiments on public benchmarks validate our design choices as the proposed method outperforms strong baselines in both visual quality and diversity metrics.",
    "chosen": "This paper presents a method for large-mask pluralistic image inpainting using discrete latent codes. The approach includes a restrictive partial encoder, a bidirectional transformer, and a synthesis network. Experiments show the method outperforms baselines in visual quality and diversity metrics.",
    "rejected": "This paper describes a pluralistic image inpainting method based on discrete latent codes, which generates coherent and diverse complete images even under extreme mask settings. The approach outperforms strong baselines in visual quality and diversity metrics, demonstrating its effectiveness in handling challenging inpainting tasks."
  },
  {
    "prompt": "summary: We present DRESS , a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction ﬁnetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multiturn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and reﬁnement. The critique NLF identiﬁes the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The reﬁnement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs– which focuses on LVLMs’ ability to reﬁne responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVLMs.",
    "chosen": "This paper introduces DRESS, a large vision-language model that uses natural language feedback (NLF) for enhanced alignment and interaction. The model categorizes NLF into critique and refinement types, improving response quality and multi-turn interaction ability. Experimental results demonstrate DRESS's superiority over existing LVLMs.",
    "rejected": "DRESS, a large vision language model, leverages Natural Language Feedback (NLF) to improve alignment and multi-turn interaction capabilities. It categorizes NLF into critique and refinement types to better align responses with human preferences and improve interaction abilities. Experimental results show significant improvements in helpfulness, honesty, and harmlessness compared to state-of-the-art LVLMs."
  },
  {
    "prompt": "summary: Self-supervised denoising has attracted widespread attention due to its ability to train without clean images. However, noise in real-world scenarios is often spatially correlated, which causes many self-supervised algorithms that assume pixel-wise independent noise to perform poorly. Recent works have attempted to break noise correlation with downsampling or neighborhood masking. However, denoising on downsampled subgraphs can lead to aliasing effects and loss of details due to a lower sampling rate. Furthermore, the neighborhood masking methods either come with high computational complexity or do not consider local spatial preservation during inference. Through the analysis of existing methods, we point out that the key to obtaining high-quality and texture-rich results in real-world selfsupervised denoising tasks is to train at the original input resolution structure and use asymmetric operations during training and inference. Based on this, we propose Asymmetric Tunable Blind-Spot Network (AT-BSN), where the blindspot size can be freely adjusted, thus better balancing noise correlation suppression and image local spatial destruction during training and inference. In addition, we regard the pre-trained AT-BSN as a meta-teacher network capable of generating various teacher networks by sampling different blind-spots. We propose a blind-spot based multi-teacher distillation strategy to distill a lightweight network, significantly improving performance. Experimental results on multiple datasets prove that our method achieves state-ofthe-art, and is superior to other self-supervised algorithms in terms of computational overhead and visual effects.",
    "chosen": "This paper proposes an asymmetric tunable blind-spot network (AT-BSN) for self-supervised denoising in real-world scenarios. The method balances noise correlation suppression and image local spatial preservation, using asymmetric operations during training and inference. The approach achieves state-of-the-art performance with lower computational overhead.",
    "rejected": "AT-BSN is introduced as a solution for real-world self-supervised denoising, addressing the challenge of spatially correlated noise. This asymmetric tunable blind-spot network balances noise correlation suppression and local spatial preservation, achieving state-of-the-art performance in terms of computational overhead and visual effects."
  },
  {
    "prompt": "summary: In this paper we propose an efﬁcient data-driven solution to self-localization within a ﬂoorplan. Floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel probabilistic model consisting of an observation and a novel temporal ﬁltering module. Operating internally with an efﬁcient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to beneﬁt from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing methods [16, 17, 20, 28] that often demand upright images. Our full system meets real-time requirements, while outperforming the state-of-the-art [20, 28] by a signiﬁcant margin.",
    "chosen": "This paper presents F3Loc, a method for self-localization within a floorplan using a probabilistic model with observation and temporal filtering modules. The method operates on consumer hardware and meets real-time requirements, outperforming existing approaches.",
    "rejected": "An efficient data-driven solution for camera localization within floorplans is proposed, utilizing a probabilistic model with observation and temporal filtering modules. The method operates on conventional consumer hardware and meets real-time requirements, significantly outperforming current state-of-the-art methods."
  },
  {
    "prompt": "summary: In this paper, we explore a novel challenging generation task, i.e. Handwritten Mathematical Expression Generation (HMEG) from symbolic sequences. Since symbolic sequences are naturally graph-structured data, we formulate HMEG as a graph-to-image (G2I) generation problem. Unlike the generation of natural images, HMEG requires critic layout clarity for synthesizing correct and recognizable formulas, but has no real masks available to supervise the learning process. To alleviate this challenge, we propose a novel end-to-end G2I generation pipeline (i.e. graph →layout →mask →image), which requires no real masks or nondifferentiable alignment between layouts and masks. Technically, to boost the capacity of predicting detailed relations among adjacent symbols, we propose a Less-is-More (LiM) learning strategy. In addition, we design a differentiable layout refinement module, which maps bounding boxes to pixel-level soft masks, so as to further alleviate ambiguous layout areas. Our whole model, including layout prediction, mask refinement, and image generation, can be jointly optimized in an end-to-end manner. Experimental results show that, our model can generate highquality HME images, and outperforms previous generative methods. Besides, a series of ablations study demonstrate effectiveness of the proposed techniques. Finally, we validate that our generated images promisingly boosts the performance of HME recognition models, through data augmentation. Our",
    "chosen": "This paper explores generating handwritten mathematical expressions from symbolic sequences, formulating it as a graph-to-image generation problem. The proposed end-to-end pipeline includes layout prediction, mask refinement, and image generation. Experimental results show the model generates high-quality images and boosts recognition performance through data augmentation.",
    "rejected": "The paper explores Handwritten Mathematical Expression Generation (HMEG) as a graph-to-image generation problem, proposing an end-to-end pipeline with layout prediction, mask refinement, and image generation stages. The Less-is-More (LiM) learning strategy and differentiable layout refinement module improve detailed relation prediction among symbols, resulting in high-quality HME images."
  },
  {
    "prompt": "summary: In recent years, Neural Radiance Field (NeRF) has demonstrated remarkable capabilities in representing 3D scenes. To expedite the rendering process, learnable explicit representations have been introduced for combination with implicit NeRF representation, which however results in a large storage space requirement. In this paper, we introduce the Context-based NeRF Compression (CNC) framework, which leverages highly efﬁcient context models to provide a storage-friendly NeRF representation. Speciﬁcally, we excavate both level-wise and dimension-wise context dependencies to enable probability prediction for information entropy reduction. Additionally, we exploit hash collision and occupancy grids as strong prior knowledge for better context modeling. To the best of our knowledge, we are the ﬁrst to construct and exploit context models for NeRF compression. We achieve a size reduction of 100⇥and 70⇥with improved ﬁdelity against the baseline Instant-NGP on Synthesic-NeRF and Tanks and Temples datasets, respectively. Additionally, we attain 86.7% and 82.3% storage size reduction against the SOTA NeRF compression method BiRF. Our",
    "chosen": "This paper presents CNC, a framework for compressing NeRF representations using context models to reduce storage requirements. The approach leverages level-wise and dimension-wise context dependencies and utilizes hash collisions and occupancy grids for better context modeling. Experiments show significant size reductions and improved fidelity compared to baseline methods.",
    "rejected": "In recent years, Neural Radiance Field (NeRF) has shown remarkable 3D scene representation abilities but faced challenges with storage space due to explicit representations. This paper presents Context-based NeRF Compression (CNC), which utilizes context models for efficient compression while improving fidelity. CNC achieves significant size reduction compared to Instant-NGP and BiRF methods."
  },
  {
    "prompt": "summary: Diffusion models have shown an impressive ability to model complex data distributions, with several key advantages over GANs, such as stable training, better coverage of the training distribution’s modes, and the ability to solve inverse problems without extra training. However, most diffusion models learn the distribution of fixed-resolution images. We propose to learn the distribution of continuous images by training diffusion models on image neural fields, which can be rendered at any resolution, and show its advantages over fixed-resolution models. To achieve this, a key challenge is to obtain a latent space that represents photorealistic image neural fields. We propose a simple and effective method, inspired by several recent techniques but with key changes to make the image neural fields photorealistic. Our method can be used to convert existing latent diffusion autoencoders into image neural field autoencoders. We show that image neural field diffusion models can be trained using mixed-resolution image datasets, outperform fixed-resolution diffusion models followed by super-resolution models, and can solve inverse problems with conditions applied at different scales efficiently.",
    "chosen": "This paper introduces Image Neural Field Diffusion models (INFD), which learn the distribution of continuous images by training diffusion models on image neural fields. The method converts existing latent diffusion autoencoders into image neural field autoencoders and demonstrates advantages over fixed-resolution models, including better performance on mixed-resolution datasets and efficient solving of inverse problems.",
    "rejected": "Diffusion models have demonstrated advantages over GANs in modeling complex data distributions. This work proposes Image Neural Field Diffusion models (INFD), capable of rendering images at any resolution and solving inverse problems efficiently without extra training. INFD outperforms fixed-resolution diffusion models followed by super-resolution models."
  },
  {
    "prompt": "summary: The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, visionlanguage tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models. † This work is done when they are interns at Shanghai AI Laboratory; B corresponding author (daijifeng@tsinghua.edu.cn)",
    "chosen": "This paper presents InternVL, a large-scale vision-language foundation model with 6 billion parameters. The model is progressively aligned with large language models (LLMs) using web-scale image-text data and achieves state-of-the-art performance on 32 visual-linguistic benchmarks. It offers powerful visual capabilities and can serve as an alternative to ViT-22B.",
    "rejected": "This paper introduces a large-scale vision-language foundation model named InternVL, aligning with large language models using web-scale image-text data. It performs well on various visual-linguistic benchmarks, offering a promising alternative to ViT-22B. The model bridges the gap between vision models and language models."
  },
  {
    "prompt": "summary: Speech-preserving facial expression manipulation (SPFEM) aims to modify facial emotions while meticulously maintaining the mouth animation associated with spoken content. Current works depend on inaccessible paired training samples for the person, where two aligned frames exhibit the same speech content yet differ in emotional expression, limiting the SPFEM applications in real-world scenarios. In this work, we discover that speakers who convey the same content with different emotions exhibit highly correlated local facial animations, providing valuable supervision for SPFEM. To capitalize on this insight, we propose a novel adaptive spatial coherent correlation learning (ASCCL) algorithm, which models the aforementioned correlation as an explicit metric and integrates the metric to supervise manipulating facial expression and meanwhile better preserving the facial animation of spoken contents. To this end, it first learns a spatial coherent correlation metric, ensuring the visual disparities of adjacent local regions of the image belonging to one emotion are similar to those of the corresponding counterpart of the image belonging to another emotion. Recognizing that visual disparities are not uniform across all regions, we have also crafted a disparity-aware adaptive strategy that prioritizes regions that present greater challenges. During SPFEM model training, we construct the adaptive spatial coherent correlation metric between corresponding local regions of the input and output images as addition loss to supervise the generation *Zhijing Yang is the corresponding author. Tianshui Chen, Jianman Lin, and Zhijing Yang are with Guangdong University of Technology. Chunmei Qing is with South China University of Technology. Liang Lin is with Sun Yat-Sen University. This work was supported in part by National Natural Science Foundation of China (NSFC) under Grant No. 62206060, in Part by Natural Science Foundation of Guangdong Province (2022A1515011555, 2023A1515012568, 2023A1515012561), Guangdong Provincial Key Laboratory of Human Digital Twin (2022B1212010004), and in part by Guangzhou Basic and Applied Basic Research Foundation under Grant No. SL2022A04J01626. Reference Source NED ASCCL Figure  Several examples are generated by the current advanced NED with and without the proposed ASCCL algorithm. Incorporating the ASCCL can better manipulate the expressions and meanwhile preserve mouth shapes. process. We conduct extensive experiments on variant datasets, and the results demonstrate the effectiveness of the proposed ASCCL algorithm.",
    "chosen": "This paper proposes a novel adaptive spatial coherent correlation learning (ASCCL) algorithm for speech-preserving facial expression manipulation (SPFEM). The algorithm models the correlation of local facial animations and integrates it as a metric to supervise the manipulation process, ensuring better preservation of mouth animations associated with spoken content.",
    "rejected": "Speech-preserving facial expression manipulation aims to modify facial emotions while preserving mouth animations. This paper proposes an adaptive spatial coherent correlation learning (ASCCL) algorithm that improves expression manipulation while better preserving speech-related facial animations. Extensive experiments validate the effectiveness of ASCCL."
  },
  {
    "prompt": "summary: Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability to perceive and understand multi-modal signals. However, most of the existing MLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text pairs, leading to insufficient extraction and reasoning of visual knowledge. To address this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model (LION), which empowers the MLLM by injecting visual knowledge in two levels. 1) Progressive incorporation of fine-grained spatialaware visual knowledge. We design a vision aggregator cooperated with region-level vision-language (VL) tasks to incorporate fine-grained spatial-aware visual knowledge into the MLLM. To alleviate the conflict between imagelevel and region-level VL tasks during incorporation, we devise a dedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This progressive incorporation scheme contributes to the mutual promotion between these two kinds of VL tasks. 2) Soft prompting of high-level semantic visual evidence. We facilitate the MLLM with highlevel semantic visual evidence by leveraging diverse image tags. To mitigate the potential influence caused by imperfect predicted tags, we propose a soft prompting method by embedding a learnable token into the tailored text instruction. Comprehensive experiments on several multi-modal benchmarks demonstrate the superiority of our model (e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps over InstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2).",
    "chosen": "This paper introduces LION, a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model. LION incorporates fine-grained spatial-aware visual knowledge and soft prompts of high-level semantic visual evidence into the multimodal large language model (MLLM), improving performance on various multi-modal benchmarks.",
    "rejected": "Multimodal Large Language Models (MLLMs) face challenges in extracting and reasoning visual knowledge. This work presents LION, enhancing MLLMs by injecting visual knowledge at two levels: progressive incorporation of fine-grained spatial-aware knowledge and soft prompting of high-level semantic visual evidence. LION shows superior performance on several multi-modal benchmarks."
  },
  {
    "prompt": "summary: Recent progress in Large Multimodal Models (LMM) has opened up great possibilities for various applications in the ﬁeld of human-machine interactions. However, developing LMMs that can comprehend, reason, and plan in complex and diverse 3D environments remains a challenging topic, especially considering the demand for understanding permutation-invariant point cloud representations of the 3D scene. Existing works seek help from multi-view images by projecting 2D features to 3D space, which inevitably leads to huge computational overhead and performance degradation. In this paper, we present LL3DA, a Large Language 3D Assistant that takes point cloud as the direct input and responds to both text instructions and visual interactions. The additional visual interaction enables LMMs to better comprehend human interactions with the 3D environment and further remove the ambiguities within plain texts. Experiments show that LL3DA achieves remarkable results and surpasses various 3D vision-language models on both 3D Dense Captioning and 3D Question Answering.",
    "chosen": "This paper presents LL3DA, a Large Language 3D Assistant that takes point cloud as direct input and responds to both text instructions and visual interactions. The model enables better comprehension of human interactions with 3D environments and removes ambiguities in plain texts, achieving remarkable results on 3D Dense Captioning and 3D Question Answering.",
    "rejected": "Large Multimodal Models (LMM) struggle with understanding permutation-invariant point cloud representations of 3D scenes. LL3DA is introduced as a solution, taking point clouds directly as input and responding to text instructions and visual interactions. Experiments demonstrate LL3DA's superior performance in 3D Dense Captioning and 3D Question Answering tasks."
  },
  {
    "prompt": "summary: For image super-resolution (SR), bridging the gap between the performance on synthetic datasets and real-world degradation scenarios remains a challenge. This work introduces a novel ”Low-Res Leads the Way” (LWay) training framework, merging Supervised Pre-training with Selfsupervised Learning to enhance the adaptability of SR models to real-world images. Our approach utilizes a lowresolution (LR) reconstruction network to extract degradation embeddings from LR images, merging them with superresolved outputs for LR reconstruction. Leveraging unseen LR images for self-supervised learning guides the model to adapt its modeling space to the target domain, facilitating fine-tuning of SR models without requiring paired high-resolution (HR) images. The integration of Discrete Wavelet Transform (DWT) further refines the focus on highfrequency details. Extensive evaluations show that our method significantly improves the generalization and detail restoration capabilities of SR models on unseen realworld datasets, outperforming existing methods. Our training regime is universally compatible, requiring no network architecture modifications, making it a practical solution for real-world SR applications.",
    "chosen": "This paper introduces a novel \"Low-Res Leads the Way\" (LWay) training framework for image super-resolution (SR). The framework combines Supervised Pre-training with Self-supervised Learning, utilizing a low-resolution reconstruction network to extract degradation embeddings and guide model adaptation to real-world images without requiring paired high-resolution images.",
    "rejected": "Image super-resolution (SR) faces challenges bridging synthetic datasets' performance and real-world degradation scenarios. A novel \"Low-Res Leads the Way\" (LWay) training framework merges supervised pre-training with self-supervised learning, significantly improving SR models' generalization and detail restoration capabilities on unseen real-world datasets."
  },
  {
    "prompt": "summary: Pose regression networks predict the camera pose of a query image relative to a known environment. Within this family of methods, absolute pose regression (APR) has recently shown promising accuracy in the range of a few centimeters in position error. APR networks en",
    "chosen": "This paper presents map-relative pose regression (marepo), a new approach to pose regression for visual re-localization. The method conditions the pose regressor on a scene-specific map representation, allowing training across hundreds of scenes and immediate application to new map representations. The approach outperforms previous methods on public datasets.",
    "rejected": "Pose regression networks predict camera poses from query images but require vast amounts of training data. This paper introduces map-relative pose regression (marepo), conditioning pose regressors on scene-specific maps, enabling training across multiple scenes. Marepo outperforms previous methods on public indoor and outdoor datasets."
  },
  {
    "prompt": "summary: We introduce Mind Artist (MindArt), a novel and efficient neural decoding architecture to snap artistic photographs from our mind in a controllable manner. Recently, progress has been made in image reconstruction with non-invasive brain recordings, but it’s still difficult to generate realistic images with high semantic fidelity due to the scarcity of data annotations. Unlike previous methods, this work casts the neural decoding into optimal transport (OT) and representation decoupling problems. Specifically, under discrete OT theory, we design a graph matching-guided neural representation learning framework to seek the underlying correspondences between conceptual semantics and neural signals, which yields a natural and meaningful selfsupervisory task. Moreover, the proposed MindArt, structured with multiple stand-alone modal branches, enables the seamless incorporation of semantic representation into any visual style information, thus leaving it to have multimodal reconstruction and training-free semantic editing ca†Corresponding author: Yu Qi. pabilities. By doing so, the reconstructed images of MindArt have phenomenal realism both in terms of semantics and appearance. We compare our MindArt with leading alternatives, and achieve SOTA performance in different decoding tasks. Importantly, our approach can directly generate a series of stylized “mind snapshots” w/o extra optimizations, which may open up more potential applications.",
    "chosen": "This paper introduces Mind Artist (MindArt), a neural decoding architecture for generating artistic photographs from human thoughts. The method casts neural decoding into optimal transport and representation decoupling problems, enabling multimodal reconstruction and training-free semantic editing capabilities. MindArt achieves state-of-the-art performance in different decoding tasks.",
    "rejected": "Mind Artist (MindArt) proposes a novel neural decoding architecture for generating artistic photographs from brain recordings. Utilizing discrete optimal transport theory, MindArt designs a graph matching-guided framework, achieving phenomenal realism in reconstructed images. It supports multimodal reconstruction and training-free semantic editing capabilities."
  },
  {
    "prompt": "summary: Learning-based stereo matching techniques have made significant progress. However, existing methods inevitably lose geometrical structure information during the feature channel generation process, resulting in edge detail mismatches. In this paper, the Motif Channel Attention Stereo Matching Network (MoCha-Stereo) is designed to address this problem. We provide the Motif Channel Correlation Volume (MCCV) to determine more accurate edge matching costs. MCCV is achieved by projecting motif channels, which capture common geometric structures in feature channels, onto feature maps and cost volumes. In addition, edge variations in the reconstruction error map also affect details matching, we propose the Reconstruction Error Motif Penalty (REMP) module to further refine the fullresolution disparity estimation. REMP integrates the frequency information of typical channel features from the reconstruction error. MoCha-Stereo ranks 1st on the KITTI2015 and KITTI-2012 Reflective leaderboards. Our structure also shows excellent performance in Multi-View Stereo. *Co-first author. †Corresponding author.",
    "chosen": "This paper proposes MoCha-Stereo, a motif channel attention stereo matching network. The approach introduces the Motif Channel Correlation Volume (MCCV) and Reconstruction Error Motif Penalty (REMP) module to improve edge matching and full-resolution disparity estimation. MoCha-Stereo ranks 1st on KITTI-2015 and KITTI-2012 Reflective leaderboards and shows excellent performance in Multi-View Stereo.",
    "rejected": "For image super-resolution, this study develops a new training framework combining supervised learning on synthetic data with self-supervised learning on real test data, termed \"Low-Res Leads the Way\". This approach enhances adaptability and fine-tuning of SR models without paired HR images, showing improved generalization on real-world datasets."
  },
  {
    "prompt": "summary: Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multi-viewconsistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks.",
    "chosen": "This paper enhances the quality and functionality of generative diffusion models for creating controllable, photorealistic human avatars from a single image. By integrating a 3D morphable model into a multi-view-consistent diffusion approach, the framework improves novel view synthesis and enables accurate facial expression and body pose control. Extensive evaluations show advantages over existing avatar creation models.",
    "rejected": "Recent advances have enabled the generation of 3D human avatars from single images using generative diffusion models. This work integrates a 3D morphable model into a multi-view-consistent diffusion approach, enhancing photorealism and controllability of facial expressions and body poses. The proposed method creates fully 3D-consistent, animatable avatars with superior performance compared to existing avatar creation models."
  },
  {
    "prompt": "summary: Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work, we propose a novel paradigm, namely “Heuristics-Guided Segmentation” (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes.",
    "chosen": "This paper proposes NeRF-HuGS, a novel paradigm combining heuristics and segmentation models to improve NeRF's performance in non-static scenes. The method fuses Structure-from-Motion (SfM)-based heuristics and color residual heuristics, effectively separating static scenes from transient distractors. Experiments demonstrate the method's superiority in mitigating artifacts caused by moving objects or shadows.",
    "rejected": "NeRF has shown excellence in novel view synthesis but struggles with transient distractors. A new paradigm named Heuristics-Guided Segmentation (HuGS) combines hand-crafted heuristics and state-of-the-art segmentation models to improve separation between static scenes and transient distractors, significantly mitigating artifacts in NeRFs trained in non-static scenes."
  },
  {
    "prompt": "summary: Absolute Pose Regression (APR) methods use deep neural networks to directly regress camera poses from RGB images. However, the predominant APR architectures only rely on 2D operations during inference, resulting in limited accuracy of pose estimation due to the lack of 3D geometry constraints or priors. In this work, we propose a test-time refinement pipeline that leverages implicit geometric constraints using a robust feature field to enhance the ability of APR methods to use 3D information during inference. We also introduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3D geometric features during training and directly renders dense novel view features at test time to refine APR methods. To enhance the robustness of our model, we introduce a feature fusion module and a progressive training strategy. Our proposed method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets.",
    "chosen": "This paper presents a test-time refinement pipeline for Absolute Pose Regression (APR) methods, enhancing their ability to use 3D information during inference. The proposed Neural Feature Synthesizer (NeFeS) model encodes 3D geometric features during training and renders dense novel view features at test time. The method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets.",
    "rejected": "APR methods for camera pose estimation face limitations due to lack of 3D geometry constraints during inference. This work proposes a test-time refinement pipeline that leverages implicit geometric constraints through a robust feature field, introducing a Neural Feature Synthesizer (NeFeS) to encode 3D features and refine APR accuracy."
  },
  {
    "prompt": "summary: We explore the boundaries of scaling up a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. Our model advances the state-of-the-art on most vision-and-language benchmarks considered (20+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.",
    "chosen": "This paper explores scaling up a multilingual vision and language model, achieving new performance levels on various complex tasks. The model advances the state-of-the-art on most vision-and-language benchmarks and shows emerging capabilities like complex counting and multilingual object detection.",
    "rejected": "Scaling up multilingual vision and language models achieves improved performance across various tasks, including image captioning, question answering, and object detection. PaLI-X demonstrates enhanced capabilities like complex counting and multilingual object detection by scaling both visual and language components, resulting in balanced parameter allocation and state-of-the-art results on multiple benchmarks."
  },
  {
    "prompt": "summary: Due to the resource-intensive nature of training visionlanguage models on expansive video data, a majority of studies have centered on adapting pre-trained imagelanguage models to the video domain. Dominant pipelines propose to tackle the visual discrepancies with additional temporal learners while overlooking the substantial discrepancy for web-scaled descriptive narratives and concise action category names, leading to less distinct semantic space and potential performance limitations. In this work, we prioritize the refinement of text knowledge to facilitate generalizable video recognition. To address the limitations of the less distinct semantic space of category names, we prompt a large language model (LLM) to augment action class names into Spatio-Temporal Descriptors thus bridging the textual discrepancy and serving as a knowledge base for general recognition. Moreover, to assign the best descriptors with different video instances, we propose Optimal Descriptor Solver, forming the video recognition problem as solving the optimal matching flow across frame-level representations and descriptors. Comprehensive evaluations in zero-shot, few-shot, and fully supervised video recognition highlight the effectiveness of our approach. Our best model achieves a state-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.",
    "chosen": "This paper proposes refining text knowledge with optimal spatio-temporal descriptors for generalizable video recognition. The method prompts a large language model to augment action class names and proposes an Optimal Descriptor Solver. Evaluations show effectiveness in zero-shot, few-shot, and fully supervised video recognition.",
    "rejected": "To address discrepancies in adapting pre-trained image-language models to video recognition, this paper proposes augmenting action class names into Spatio-Temporal Descriptors using a large language model (LLM). An Optimal Descriptor Solver is introduced to match frame-level representations with descriptors, leading to significant improvements in zero-shot, few-shot, and fully supervised video recognition."
  },
  {
    "prompt": "summary: Recently, some large kernel convnets strike back with appealing performance and efﬁciency. However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem. Due to these issues, current CNNs compromise to scale up to 51 × 51 in the form of stripe convolution (i.e., 51 × 5 + 5 × 51) and start to saturate as the kernel size continues growing. In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains. Inspired by human vision, we propose a human-like peripheral convolution that efﬁciently reduces over 90% parameter count of dense grid convolution through parameter sharing, and manage to scale up kernel size to extremely large. Our peripheral convolution behaves highly similar to human, reducing the complexity of convolution from O(K2) to O(log K) without backﬁring performance. Built on this, we propose Parameter-efﬁcient Large Kernel Network (PeLK). Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classiﬁcation, semantic segmentation on ADE20K and object detection on MS COCO. For the ﬁrst time, we successfully scale up the kernel size of CNNs to an unprecedented 101 × 101 and demonstrate consistent improvements.",
    "chosen": "This paper introduces Parameter-efficient Large Kernel Networks (PeLK) with peripheral convolution, inspired by human vision. The network efficiently reduces parameters, scales up kernel size, and outperforms modern vision Transformers and ConvNet architectures on various vision tasks.",
    "rejected": "Large kernel convnets can achieve high performance but suffer from optimization issues as kernel sizes increase. Inspired by human vision, a peripheral convolution method reduces parameter counts by over 90% while scaling up kernel size to extremely large dimensions. The Parameter-efficient Large Kernel Network (PeLK) outperforms modern vision transformers and ConvNet architectures on various vision tasks, successfully scaling kernels to an unprecedented 101x101 size."
  },
  {
    "prompt": "summary: Domain Generalization (DG) aims to resolve distribution shifts between source and target domains, and current DG methods are default to the setting that data from source and target domains share identical categories. Nevertheless, there exists unseen classes from target domains in practical scenarios. To address this issue, Open Set Domain Generalization (OSDG) has emerged and several methods have been exclusively proposed. However, most existing methods adopt complex architectures with slight improvement compared with DG methods. Recently, visionlanguage models (VLMs) have been introduced in DG following the fine-tuning paradigm, but consume huge training overhead with large vision models. Therefore, in this paper, we innovate to transfer knowledge from VLMs to lightweight vision models and improve the robustness by introducing Perturbation Distillation (PD) from three perspectives, including Score, Class and Instance (SCI), named SCI-PD. Moreover, previous methods are oriented by the benchmarks with identical and fixed splits, ignoring the divergence between source domains. These methods are revealed to suffer from sharp performance decay with our proposed new benchmark Hybrid Domain Generalization (HDG) and a novel metric H2-CV, which construct various splits to comprehensively assess the robustness of algorithms. Extensive experiments demonstrate that our method outperforms state-of-the-art algorithms on multiple datasets, especially improving the robustness when confronting data scarcity.",
    "chosen": "This paper addresses Open Set Domain Generalization (OSDG) by transferring knowledge from vision-language models to lightweight vision models and introducing Perturbation Distillation (PD) from three perspectives. The method improves robustness and outperforms state-of-the-art algorithms on multiple datasets.",
    "rejected": "Domain Generalization (DG) faces challenges when dealing with unseen classes in target domains. Open Set Domain Generalization (OSDG) addresses this issue, but existing methods often require complex architectures. This paper introduces Perturbation Distillation (PD) from three perspectives (Score, Class, Instance), named SCI-PD, which transfers knowledge from vision-language models to lightweight vision models, improving robustness and performance on multiple datasets, especially under data scarcity."
  },
  {
    "prompt": "summary: Weakly-supervised Video Anomaly Detection (wVAD) aims to detect frame-level anomalies using only videolevel labels in training. Due to the limitation of coarsegrained labels, Multi-Instance Learning (MIL) is prevailing in wVAD. However, MIL suffers from insufficiency of binary supervision to model diverse abnormal patterns. Besides, the coupling between abnormality and its context hinders the learning of clear abnormal event boundary. In this paper, we propose prompt-enhanced MIL to detect various abnormal events while ensuring clear event boundaries. Concretely, we design the abnormal-aware prompts by using abnormal class annotations together with learnable prompt, which can incorporate semantic priors into video features dynamically. The detector can utilize the semantic-rich features to capture diverse abnormal patterns. In addition, normal context prompt is introduced to amplify the distinction between abnormality and its context, facilitating the generation of clear boundary. With the mutual enhancement of abnormal-aware and normal context prompt, the model can construct discriminative representations to detect divergent anomalies without ambiguous event boundaries. Extensive experiments demonstrate our method achieves SOTA performance on three public benchmarks.",
    "chosen": "This paper proposes prompt-enhanced Multi-Instance Learning (MIL) for weakly supervised video anomaly detection. The method uses abnormal-aware prompts and normal context prompts to capture diverse abnormal patterns and generate clear event boundaries. Experiments show state-of-the-art performance on public benchmarks.",
    "rejected": "Weakly-supervised Video Anomaly Detection (wVAD) aims to detect anomalies using only video-level labels. This paper proposes prompt-enhanced MIL, incorporating abnormal-aware prompts and normal context prompts to generate clear event boundaries and capture diverse abnormal patterns. Extensive experiments show that this method achieves state-of-the-art performance on three public benchmarks."
  },
  {
    "prompt": "summary: Segment Anything Model (SAM) has emerged as a transformative approach in image segmentation, acclaimed for its robust zero-shot segmentation capabilities and flexible prompting system. Nonetheless, its performance is challenged by images with degraded quality. Addressing this limitation, we propose the Robust Segment Anything Model (RobustSAM), which enhances SAM’s performance on lowquality images while preserving its promptability and zeroshot generalization. Our method leverages the pre-trained SAM model with only marginal parameter increments and computational requirements. The additional parameters of RobustSAM can be optimized within 30 hours on eight GPUs, demonstrating its feasibility and practicality for typical research laboratories. We also introduce the RobustSeg dataset, a collection of 688K image-mask pairs with different degradations designed to train and evaluate our model optimally. Extensive experiments across various segmentation tasks and datasets confirm RobustSAM’s superior performance, especially under zero-shot conditions, underscoring its potential for extensive real-world application. Additionally, our method has been shown to effectively improve the performance of SAM-based downstream tasks such as single image dehazing and deblurring.",
    "chosen": "This paper introduces RobustSAM, enhancing Segment Anything Model (SAM) for better performance on low-quality images. The method preserves SAM's promptability and zero-shot generalization with minimal parameter increments. Experiments confirm superior performance across various segmentation tasks.",
    "rejected": "The Segment Anything Model (SAM) excels in zero-shot segmentation but struggles with low-quality images. RobustSAM enhances SAM’s performance on such images with minimal additional parameters, preserving its promptability and zero-shot capabilities. It uses a pre-trained SAM model and can be optimized within 30 hours on eight GPUs. The RobustSeg dataset, comprising 688K image-mask pairs with various degradations, supports training and evaluation. Experiments confirm RobustSAM's superior performance, especially under zero-shot conditions, making it ideal for real-world applications like image dehazing and deblurring."
  },
  {
    "prompt": "summary: We propose SceneTex, a novel method for effectively generating high-quality and style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike previous methods that either iteratively warp 2D views onto a mesh surface or distillate diffusion latent features without accurate geometric and style cues, SceneTex formulates the texture synthesis task as an optimization problem in the RGB space where style and geometry consistency are properly reflected. At its core, SceneTex proposes a multiresolution texture field to implicitly en",
    "chosen": "This paper proposes SceneTex, a method for generating high-quality, style-consistent textures for indoor scenes using depth-to-image diffusion priors. The method formulates texture synthesis as an optimization problem and introduces a cross-attention decoder for style consistency. Results show significant improvements in visual quality and prompt fidelity.",
    "rejected": "SceneTex is a novel method for generating high-quality, style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike previous methods, SceneTex formulates texture synthesis as an RGB-space optimization problem, ensuring geometry and style consistency. It employs a multi-resolution texture field and score-distillation-based objective function for optimization, and a cross-attention decoder for style consistency across views. SceneTex shows significant improvements in visual quality and prompt fidelity over prior methods, enabling accurate texture synthesis for 3D-FRONT scenes."
  },
  {
    "prompt": "summary: Recent works on text-to-3d generation show that using only 2D diffusion supervision for 3D generation tends to produce results with inconsistent appearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals with extra legs). Existing methods mainly address this issue by retraining diffusion models with images rendered from 3D data to ensure multi-view consistency while struggling to balance 2D generation quality with 3D consistency. In this paper, we present a new framework Sculpt3D that equips the current pipeline with explicit injection of 3D priors from retrieved reference objects without re-training the 2D diffusion model. Specifically, we demonstrate that high-quality and diverse 3D geometry can be guaranteed by  key points supervision through a sparse ray sampling approach. Moreover, to ensure accurate appearances of different views, we further modulate the output of the 2D diffusion model to the correct patterns of the template views without altering the generated object’s style. These two decoupled designs effectively harness 3D information from reference objects to generate 3D objects while preserving the generation quality of the 2D diffusion model. Extensive experiments show our method can largely improve the multi-view consistency while retaining fidelity and diversity. Our",
    "chosen": "This paper presents Sculpt3D, a framework for text-to-3D generation that improves multi-view consistency by injecting 3D priors from reference objects without retraining the 2D diffusion model. The method uses  key points supervision and modulates the 2D diffusion model's output to ensure accurate appearances across different views. Experiments show significant improvements in multi-view consistency while maintaining fidelity and diversity.",
    "rejected": "Recent research in text-to-3D generation has struggled with multi-view consistency and shape accuracy. Sculpt3D proposes a novel framework that integrates 3D priors from reference objects into the 2D diffusion model without retraining, enhancing the quality and diversity of generated 3D objects."
  },
  {
    "prompt": "summary: Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE). Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms. Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts. Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models. To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters. Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model. Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods.",
    "chosen": "This paper introduces SD-Pose, a self-distillation method for improving the performance of small transformer-based models in human pose estimation. The method features a Multi-Cycled Transformer module and a self-distillation scheme, achieving state-of-the-art performance among small models on the MSCOCO validation dataset.",
    "rejected": "SDPose introduces a self-distillation method to improve small transformer-based models' performance on human pose estimation tasks. By employing a Multi-Cycled Transformer module and self-distillation scheme, SDPose significantly boosts model efficiency and accuracy, achieving state-of-the-art results among tiny neural network methods."
  },
  {
    "prompt": "summary: Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of capturing this variation. To address this issue, we present SecondPose, a novel approach integrating object-specific geometric features with semantic category priors from DINOv2. Leveraging the advantage of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin.",
    "chosen": "This paper presents SecondPose, a novel approach for category-level object pose estimation that integrates object-specific geometric features with semantic category priors from DINOv2. The method extracts SE(3)-invariant geometric features and aligns them with DINOv2 features, achieving significant performance improvements on multiple datasets.",
    "rejected": "SecondPose addresses intra-class shape variation in category-level object pose estimation by integrating geometric features with semantic category priors from DINOv2. This approach enhances the consistency and accuracy of pose predictions, demonstrating superior performance on NOCS-REAL275 and HouseCat6D datasets."
  },
  {
    "prompt": "summary: We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos. Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associations when any one pair does not. We show our approach can successfully discover how the long tail of human actions sound from egocentric video, outperforming an array of recent multimodal embedding techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal tasks.",
    "chosen": "This paper proposes SoundingActions, a self-supervised embedding method for learning how actions sound from narrated egocentric videos. The method reinforces audio-visual-language associations when all modalities agree, outperforming recent techniques on multiple datasets and tasks.",
    "rejected": "This paper presents MC3 embedding, a self-supervised multimodal learning method for associating actions with sounds in egocentric videos. MC3 outperforms existing techniques in discovering how sounding actions cluster based on visual and audio cues, advancing cross-modal tasks like activity recognition and content generation."
  },
  {
    "prompt": "summary: Learning generalizable visual representations from Internet data has yielded promising results for robotics. Yet, prevailing approaches focus on pre-training 2D representations, being sub-optimal to deal with occlusions and accurately localize objects in complex 3D scenes. Meanwhile, 3D representation learning has been limited to single-object understanding. To address these limitations, we introduce a novel 3D pre-training framework for robotics named SUGAR that captures semantic, geometric and affordance properties of objects through 3D point clouds. We underscore the importance of cluttered scenes in 3D representation learning, and automatically construct a multi-object dataset benefiting from cost-free supervision in simulation. SUGAR employs a versatile transformer-based model to jointly address five pre-training tasks, namely cross-modal knowledge distillation for semantic learning, masked point modeling to understand geometry structures, grasping pose synthesis for object affordance, 3D instance segmentation and referring expression grounding to analyze cluttered scenes. We evaluate our learned representation on three robotic-related tasks, namely, zero-shot 3D object recognition, referring expression grounding, and language-driven robotic manipulation. Experimental results show that SUGAR’s 3D representation outperforms state-of-the-art 2D and 3D representations.",
    "chosen": "This paper introduces SUGAR, a 3D pre-training framework for robotics that captures semantic, geometric, and affordance properties of objects through 3D point clouds. The framework includes five pre-training tasks and demonstrates superior performance on robotic-related tasks compared to state-of-the-art 2D and 3D representations.",
    "rejected": "SUGAR is a 3D pre-training framework designed to learn semantic, geometric, and affordance properties of objects through point clouds. It excels in handling occlusions and accurately localizing objects in cluttered scenes, showing improved performance over state-of-the-art 2D and 3D representations in various robotic tasks."
  },
  {
    "prompt": "summary: Automatic text-to-3D generation that combines Score Distillation Sampling (SDS) with the optimization of volume rendering has achieved remarkable progress in synthesizing realistic 3D objects. Yet most existing text-to-3D methods by SDS and volume rendering suffer from inaccurate geometry, e.g., the Janus issue, since it is hard to explicitly integrate 3D priors into implicit 3D representations. Besides, it is usually time-consuming for them to generate elaborate 3D models with rich colors. In response, this paper proposes GSGEN, a novel method that adopts Gaussian Splatting, a recent stateof-the-art representation, to text-to-3D generation. GSGEN aims at generating high-quality 3D objects and addressing existing shortcomings by exploiting the explicit nature of Gaussian Splatting that enables the incorporation of 3D prior. Specifically, our method adopts a progressive optimization strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimiza†Corresponding author tion, a coarse representation is established under 3D point cloud diffusion prior along with the ordinary 2D SDS optimization, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative appearance refinement to enrich texture details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D assets with delicate details and accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components. Our",
    "chosen": "This paper proposes GSGEN, a text-to-3D generation method using Gaussian Splatting. The method includes geometry optimization and appearance refinement stages, generating high-quality 3D objects with accurate geometry and rich details. Evaluations show its effectiveness, particularly in capturing high-frequency components.",
    "rejected": "GSGEN proposes using Gaussian Splatting for text-to-3D generation to address geometry inaccuracies and time consumption issues. Through progressive optimization stages including geometry establishment and appearance refinement, GSGEN generates high-quality 3D objects with accurate geometries and rich colors."
  },
  {
    "prompt": "summary: Federated learning facilitates the collaborative learning of a global model across multiple distributed medical institutions without centralizing data. Nevertheless, the expensive cost of annotation on local clients remains an obstacle to effectively utilizing local data. To mitigate this issue, federated active learning methods suggest leveraging local and global model predictions to select a relatively small amount of informative local data for annotation. However, existing methods mainly focus on all local data sampled from the same domain, making them unreliable in realistic medical scenarios with domain shifts among different clients. In this paper, we make the first attempt to assess the informativeness of local data derived from diverse domains and propose a novel methodology termed Federated Evidential Active Learning (FEAL) to calibrate the data evaluation under domain shift. Specifically, we introduce a Dirichlet prior distribution in both local and global models to treat the prediction as a distribution over the probability simplex and capture both aleatoric and epistemic uncertainties by using the Dirichlet-based evidential model. Then we employ the epistemic uncertainty to calibrate the aleatoric uncertainty. Afterward, we design a diversity relaxation strategy to reduce data redundancy and maintain data diversity. Extensive experiments and analysis on five real multi-center medical image datasets demonstrate the superiority of FEAL over the state-of-the-art active learning methods in federated scenarios with domain shifts.",
    "chosen": "This paper presents FEAL, a federated evidential active learning method for medical imaging that addresses domain shifts among clients. The method uses a Dirichlet prior distribution to capture uncertainties and a diversity relaxation strategy to maintain data diversity. Experiments on multi-center datasets demonstrate its superiority over existing methods.",
    "rejected": "FEAL introduces federated evidential active learning to calibrate data evaluation under domain shifts in medical federated learning scenarios. By leveraging Dirichlet-based evidential models, FEAL effectively selects informative local data for annotation, improving model performance across diverse domains."
  },
  {
    "prompt": "summary: Despite many attempts to leverage pre-trained text-toimage models (T2I) like Stable Diffusion (SD) [25] for controllable image editing, producing good predictable results remains a challenge. Previous approaches have focused on either fine-tuning pre-trained T2I models on specific datasets to generate certain kinds of images (e.g., with a specific object or person), or on optimizing the weights, text prompts, and/or learning features for each input image in an attempt to coax the image generator to produce the desired result. However, these approaches all have shortcomings and fail to produce good results in a predictable and controllable manner. To address this problem, we present TiNO-Edit, an SD-based method that focuses on optimizing the noise patterns and diffusion timesteps during editing, something previously unexplored in the literature. With this simple change, we are able to generate results that both better align with the original images and reflect the desired result. Furthermore, we propose a set of new loss functions that operate in the latent domain of SD, greatly speeding up the optimization when compared to prior losses, which operate in the pixel domain. Our method can be easily applied to variations of SD including Textual Inversion [13] and DreamBooth [27] that en",
    "chosen": "This paper introduces TiNO-Edit, a method for robust diffusion-based image editing that optimizes noise patterns and diffusion timesteps. The method also proposes new loss functions in the latent domain, improving optimization speed and enabling various image-editing capabilities.",
    "rejected": "TiNO-Edit focuses on optimizing noise patterns and diffusion timesteps during image editing using Stable Diffusion models. Introducing new latent domain loss functions, this method achieves better alignment with original images and desired edits, facilitating realistic and high-quality image synthesis."
  },
  {
    "prompt": "summary: We present a method to generate full-body selfies from photographs originally taken at arms length. Because self-captured photos are typically taken close up, they have limited field of view and exaggerated perspective that distorts facial shapes. We instead seek to generate the photo some one else would take of you from a few feet away. Our approach takes as input four selfies of your face and body, a background image, and generates a full-body selfie in a desired target pose. We introduce a novel diffusion-based approach to combine all of this information into high-quality, well-composed photos of you with the desired pose and background.",
    "chosen": "This paper presents a method to generate full-body selfies from arm's length photographs using a diffusion-based approach. The method takes four selfies, a background image, and generates a well-composed full-body selfie in a desired pose.",
    "rejected": "This work presents a diffusion-based method to generate full-body selfies from close-up photographs. Utilizing multiple input selfies and a background image, it creates well-composed photos of subjects in desired poses, overcoming limitations of traditional selfies such as distorted perspectives and limited field of view."
  },
  {
    "prompt": "summary: Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation. However, success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys. We have ascertained that generative AI models, e.g., Diffusion Models, can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demographics, imaging protocols, and healthcare facilities.",
    "chosen": "This paper discusses generalizable tumor synthesis in medical images using diffusion models, focusing on early-stage tumors with similar imaging characteristics across organs. The method creates realistic synthetic tumors and trains AI models for detecting real tumors across different domains.",
    "rejected": "The paper discusses tumor synthesis for training AI models in detecting and segmenting tumors across different organs and imaging domains. Successful tumor synthesis requires creating visually realistic and generalizable tumors, ensuring AI models can effectively detect real tumors in varied medical images."
  },
  {
    "prompt": "summary: Pretrained diffusion models and their outputs are widely accessible due to their exceptional capacity for synthesizing high-quality images and their open-source nature. The users, however, may face litigation risks owing to the models’ tendency to memorize and regurgitate training data during inference. To address this, we introduce Anti-Memorization Guidance (AMG), a novel framework employing three targeted guidance strategies for the main causes of memorization: image and caption duplication, and highly specific user prompts. Consequently, AMG ensures memorization-free outputs while maintaining high image quality and text alignment, leveraging the synergy of its guidance methods, each indispensable in its own right. AMG also features an innovative automatic detection system for potential memorization during each step of inference process, allows selective application of guidance strategies, minimally interfering with the original sampling process to preserve output utility. We applied AMG to pretrained Denoising Diffusion Probabilistic Models (DDPM) and Stable Diffusion across various generation tasks. The results demonstrate that AMG is the first approach to successfully eradicates all instances of memorization with no or marginal impacts on image quality and text-alignment, as evidenced by FID and CLIP scores.",
    "chosen": "This paper introduces Anti-Memorization Guidance (AMG), a framework to eliminate memorization in diffusion models while maintaining image quality and text alignment. AMG uses three guidance strategies targeting image and caption duplication and specific user prompts. It includes an automatic detection system for potential memorization during inference. Experiments show AMG successfully removes memorization with minimal impact on quality and alignment.",
    "rejected": "Towards Memorization-Free Diffusion Models. This research proposes a novel approach to mitigate memorization in diffusion models, focusing on enhancing generalization while maintaining sample quality. By integrating specific regularization techniques, the model can achieve superior performance on diverse datasets without overfitting."
  },
  {
    "prompt": "summary: 3D pose transfer that aims to transfer the desired pose to a target mesh is one of the most challenging 3D generation tasks. Previous attempts rely on well-defined parametric human models or skeletal joints as driving pose sources. However, to obtain those clean pose sources, cumbersome but necessary pre-processing pipelines are inevitable, hindering implementations of the real-time applications. This work is driven by the intuition that the robustness of the model can be enhanced by introducing adversarial samples into the training, leading to a more invulnerable model to the noisy inputs, which even can be further extended to directly handling the real-world data like raw point clouds/scans without intermediate processing. Furthermore, we propose a novel 3D pose Masked Autoencoder (3D-PoseMAE), a customized MAE that effectively learns 3D extrinsic presentations (i.e., pose). 3D-PoseMAE facilitates learning from the aspect of extrinsic attributes by simultaneously generating adversarial samples that perturb the model and learning the arbitrary raw noisy poses via a multi-scale masking strategy. Both qualitative and quantitative studies show that the transferred meshes given by our network result in much better quality. Besides, we demonstrate the strong generalizability of our method on various poses, different domains, and even raw scans. Experimental results also show meaningful insights that the intermediate adversarial samples generated in the training can successfully attack the existing pose transfer models.",
    "chosen": "This paper presents a 3D pose transfer method enhanced by adversarial samples and a 3D pose Masked Autoencoder (3D-PoseMAE). The method improves robustness to noisy inputs and generalizes well to raw point clouds/scans. The multi-scale masking strategy helps learn arbitrary raw noisy poses. Results show better quality transferred meshes and strong generalizability.",
    "rejected": "Towards Robust 3D Pose Transfer with Adversarial Learning. The study introduces an adversarial learning framework to improve the robustness of 3D pose transfer across different subjects. Through comprehensive experiments, it demonstrates significant improvements in accuracy and stability under various conditions."
  },
  {
    "prompt": "summary: Audio-visual segmentation (AVS) is a challenging task that involves accurately segmenting sounding objects based on audio-visual cues. The effectiveness of audio-visual learning critically depends on achieving accurate crossmodal alignment between sound and visual objects. Successful audio-visual learning requires two essential components: 1) a challenging dataset with high-quality pixel-level multi-class annotated images associated with audio files, and 2) a model that can establish strong links between audio information and its corresponding visual object. However, these requirements are only partially addressed by current methods, with training sets containing biased audiovisual data, and models that generalise poorly beyond this biased training set. In this work, we propose a new costeffective strategy to build challenging and relatively unbiased high-quality audio-visual segmentation benchmarks. We also propose a new informative sample mining method for audio-visual supervised contrastive learning to leverage discriminative contrastive samples to enforce cross-modal understanding. We show empirical results that demonstrate the effectiveness of our benchmark. Furthermore, experiments conducted on existing AVS datasets and on our new benchmark show that our method achieves state-of-the-art (SOTA) segmentation accuracy1.",
    "chosen": "This paper proposes a cost-effective strategy to build unbiased audio-visual segmentation benchmarks and a supervised contrastive learning method for audio-visual segmentation. The method mines informative samples to better constrain audio-visual embeddings. Experiments demonstrate state-of-the-art segmentation accuracy on existing datasets and the new benchmark.",
    "rejected": "Unraveling Instance Associations A Closer Look for Audio-Visual Segmentation. This work investigates the challenge of associating instances in audio-visual scenes. It presents an innovative method that enhances segmentation accuracy by leveraging multimodal information, achieving state-of-the-art results."
  },
  {
    "prompt": "summary: Significant progress in image deblurring has been achieved by deep learning methods, especially the remarkable performance of supervised models on paired synthetic data. However, real-world quality degradation is more complex than synthetic datasets, and acquiring paired data in real-world scenarios poses significant challenges. To address these challenges, we propose a novel unsupervised image deblurring framework based on self-enhancement. The framework progressively generates improved pseudosharp and blurry image pairs without the need for real paired datasets, and the generated image pairs with higher qualities can be used to enhance the performance of the reconstructor. To ensure the generated blurry images are closer to the real blurry images, we propose a novel re-degradation principal component consistency loss, which enforces the principal components of the generated low-quality images to be similar to those of re-degraded images from the original sharp ones. Furthermore, we introduce the self-enhancement strategy that significantly improves deblurring performance without increasing the computational complexity of network during inference. Through extensive experiments on multiple real-world blurry datasets, we demonstrate the superiority of our approach over other state-of-the-art unsupervised methods.",
    "chosen": "This paper presents an unsupervised image deblurring framework based on self-enhancement. The framework generates improved pseudo-sharp and blurry image pairs progressively. A re-degradation principal component consistency loss ensures generated blurry images resemble real ones. The self-enhancement strategy improves deblurring performance without added inference complexity.",
    "rejected": "Unsupervised Blind Image Deblurring Based on Self-Enhancement. The paper addresses the problem of blind image deblurring using unsupervised learning methods. It introduces a self-enhancement technique that iteratively refines the image, leading to clearer outcomes without prior knowledge of blur kernels."
  },
  {
    "prompt": "summary: Phase unwrapping (PU) is a technique to reconstruct original phase images from their noisy wrapped counterparts, finding many applications in scientific imaging. Although supervised learning has shown promise in PU, its utility is limited in ground-truth (GT) scarce scenarios. This paper presents an unsupervised learning approach that eliminates the need for GTs during end-to-end training. Our approach leverages the insight that both the gradients and wrapped gradients of wrapped phases serve as noisy labels for GT phase gradients, along with sparse outliers induced by the wrapping operation. A recorruption-based self-reconstruction loss in the gradient domain is proposed to mitigate the adverse effects of label noise, complemented with a self-distillation loss for improved generalization. Additionally, by unfolding a variational model of PU that utilizes wrapped gradients of wrapped phases for its datafitting term, we develop a deep unrolling network that encodes physics of phase wrapping and incorporates special treatments on outliers. In the experiments on three types of phase data, our approach outperforms existing GT-free methods and competes well against the supervised ones.",
    "chosen": "This paper introduces an unsupervised learning approach for phase unwrapping (PU) that doesn't require ground-truth during training. The approach uses gradients and wrapped gradients of wrapped phases as noisy labels, with a self-reconstruction loss and self-distillation loss. It also develops a deep unrolling network encoding physics of phase wrapping. The method outperforms existing GT-free methods and competes with supervised ones.",
    "rejected": "Unsupervised Deep Unrolling Networks for Phase Unwrapping. This paper develops an unsupervised deep unrolling network designed to solve phase unwrapping problems efficiently. The proposed method combines traditional optimization algorithms with deep learning, offering faster computation times and improved accuracy."
  },
  {
    "prompt": "summary: Recent Large Language Models (LLMs) have been enhanced with vision capabilities, enabling them to comprehend images, videos, and interleaved vision-language content. However, the learning methods of these large multimodal models (LMMs) typically treat videos as predetermined clips, rendering them less effective and efficient at handling streaming video inputs. In this paper, we propose a novel Learning-In-Video-Stream (LIVE) framework, which enables temporally aligned, long-context, and realtime dialogue within a continuous video stream. Our LIVE framework comprises comprehensive approaches to achieve video streaming dialogue, encompassing: (1) a training objective designed to perform language modeling for continuous streaming inputs, (2) a data generation scheme that converts offline temporal annotations into a streaming dialogue format, and (3) an optimized inference pipeline to speed up interactive chat in real-world video streams. With BCorresponding Author. our LIVE framework, we develop a simplified model called VideoLLM-online and demonstrate its significant advantages in processing streaming videos. For instance, our VideoLLM-online-7B model can operate at over 10 FPS on an A100 GPU for a 5-minute video clip from Ego4D narration. Moreover, VideoLLM-online also showcases state-of-the-art performance on public offline video benchmarks, such as recognition, captioning, and forecasting.",
    "chosen": "This paper proposes the LIVE framework for temporally aligned, long-context, and real-time dialogue within continuous video streams. LIVE includes a training objective for streaming inputs, a data generation scheme, and an optimized inference pipeline. The VideoLLM-online model demonstrates significant advantages in processing streaming videos and shows state-of-the-art performance on public offline video benchmarks.",
    "rejected": "VideoLLM-online Online Video Large Language Model for Streaming Video. This contribution presents VideoLLM, a large language model tailored for streaming video analysis. Capable of processing online videos in real-time, it showcases remarkable performance in understanding and generating textual descriptions synchronized with video content."
  },
  {
    "prompt": "summary: Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTaminL significantly outperforms ViT-L by 2.0% ImageNet zeroshot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, openvocabulary detection and segmentation, and large multimodal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).",
    "chosen": "This paper aims to build an evaluation protocol for vision models in the vision-language era under the CLIP framework. It introduces ViTamin, a new vision model tailored for VLMs. ViTamin-L outperforms ViT-L in ImageNet zero-shot accuracy and presents promising results on diverse benchmarks. ViTamin-XL surpasses models with ten times more parameters.",
    "rejected": "ViTamin Designing Scalable Vision Models in the Vision-Language Era. Focusing on scalable vision models, this paper introduces ViTamin, which integrates advancements in vision-language models. ViTamin is designed to handle large-scale visual recognition tasks more effectively, demonstrating its superiority in multiple benchmarks."
  },
  {
    "prompt": "summary: Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our",
    "chosen": "This paper introduces VP3D, a Visual Prompt-guided text-to-3D diffusion model. VP3D uses a 2D visual prompt generated from input text to strengthen SDS optimization. It couples SDS with a differentiable reward function for better visual alignment. Experiments show VP3D significantly improves visual fidelity and texture details.",
    "rejected": "VP3D Unleashing 2D Visual Prompt for Text-to-3D Generation. VP3D is introduced as a method to generate 3D models from text prompts using 2D visual cues. This technique enhances the realism and detail of generated 3D objects, making significant contributions to the field of computer graphics."
  },
  {
    "prompt": "summary: Spatio-temporal grounding describes the task of localizing events in space and time, e.g., in video data, based on verbal descriptions only. Models for this task are usually trained with human-annotated sentences and bounding box supervision. This work addresses this task from a multimodal supervision perspective, proposing a framework for spatio-temporal action grounding trained on loose video and subtitle supervision only, without human annotation. To this end, we combine local representation learning, which focuses on leveraging ﬁne-grained spatial information, with a global representation encoding that captures higher-level representations and incorporates both in a joint approach. To evaluate this challenging task in a reallife setting, a new benchmark dataset is proposed, providing dense spatio-temporal grounding annotations in long, untrimmed, multi-action instructional videos for over 5K events. We evaluate the proposed approach and other methods on the proposed and standard downstream tasks, showing that our method improves over current baselines in various settings, including spatial, temporal, and untrimmed multi-action spatio-temporal grounding.",
    "chosen": "This paper addresses spatio-temporal grounding from a multimodal supervision perspective, proposing a framework trained on loose video and subtitle supervision. It combines local and global representation learning. A new benchmark dataset with dense annotations in long instructional videos is proposed. The method improves over baselines in various settings.",
    "rejected": "What When and Where Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action. This study explores self-supervised learning for spatio-temporal grounding in long, untrimmed videos containing multiple actions. It proposes a framework that accurately localizes and recognizes actions within complex scenes without manual annotations."
  },
  {
    "prompt": "summary: Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures. Instead, these methods directly output animations from speech without control over the expressed emotion. To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent diffusion. Our observation is that content (i.e., gestures related to speech rhythm and word utterances), emotion, and personal style are separable. To account for this, AMUSE maps the driving audio to three disentangled latent vectors: one for content, one for emotion, and one for personal style. A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative, quantitative, and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences. Compared to the state of the art, the generated gestures are better synchronized with the speech content, and better represent the emotion expressed by the input speech. Our",
    "chosen": "This paper presents AMUSE, an emotional speech-driven body animation model based on latent diffusion. AMUSE disentangles content, emotion, and personal style in latent vectors. It synthesizes 3D human gestures from speech with control over emotion and style. Evaluations show realistic gesture sequences better synchronized with speech content and emotion.",
    "rejected": "Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion. The paper discusses a method for generating realistic 3D body animations driven by emotional speech. Using disentangled latent diffusion models, it achieves nuanced and contextually appropriate animations, significantly advancing the state of the art in non-verbal communication synthesis."
  },
  {
    "prompt": "summary: We introduce a novel approach to single image denoising based on the Blind Spot Denoising principle, which we call MAsked and SHuffled Blind Spot Denoising (MASH). We focus on the case of correlated noise, which often plagues real images. MASH is the result of a careful analysis to determine the relationships between the level of blindness (masking) of the input and the (unknown) noise correlation. Moreover, we introduce a shuffling technique to weaken the local correlation of noise, which in turn yields an additional denoising performance improvement. We evaluate MASH via extensive experiments on real-world noisy image datasets. We demonstrate state-of-the-art results compared to existing self-supervised denoising methods.",
    "chosen": "This paper introduces MASH, a novel approach to single image denoising based on Blind Spot Denoising principles. MASH focuses on correlated noise in real images and introduces a shuffling technique to weaken local noise correlation. Extensive experiments demonstrate state-of-the-art results compared to existing self-supervised denoising methods.",
    "rejected": "We introduce MASH, a novel Blind Spot Denoising technique that handles correlated noise in images by utilizing masked and shuffled input data. This method improves denoising performance on real-world noisy images compared to existing self-supervised methods."
  },
  {
    "prompt": "summary: Referring Image Segmentation (RIS) is a challenging task that requires an algorithm to segment objects referred by free-form language expressions. Despite significant progress in recent years, most state-of-the-art (SOTA) methods still suffer from considerable language-image modality gap at the pixel and word level. These methods generally 1) rely on sentence-level language features for language-image alignment and 2) lack explicit training supervision for fine-grained visual grounding. Consequently, they exhibit weak object-level correspondence between visual and language features. Without well-grounded features, prior methods struggle to understand complex expressions that require strong reasoning over relationships among multiple objects, especially when dealing with rarely used or ambiguous clauses. To tackle this challenge, we introduce a novel Mask Grounding auxiliary task that significantly improves visual grounding within language features, by explicitly teaching the model to learn fine-grained correspondence between masked textual tokens and their matching visual objects. Mask Grounding can be directly used on prior RIS methods and consistently bring improvements. Furthermore, to holistically address the modality gap, we also design a cross-modal alignment loss and an accompanying alignment module. These additions work synergistically with Mask Grounding. With all these techniques, our comprehensive approach culminates in MagNet (Mask grounded Network), an architecture that significantly outperforms prior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating our method’s effectiveness in addressing current limitations of RIS algorithms. Our",
    "chosen": "This paper presents MagNet, a mask-grounded network for referring image segmentation (RIS). The method introduces a Mask Grounding auxiliary task to improve visual grounding within language features and designs a cross-modal alignment loss and alignment module. MagNet significantly outperforms prior arts on key benchmarks.",
    "rejected": "This paper presents MagNet, an advanced referring image segmentation network incorporating Mask Grounding and cross-modal alignment loss to address language-image modality gaps, enhancing object-level correspondence between visual and language features for improved segmentation accuracy."
  },
  {
    "prompt": "summary: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer the pixel-wise knowledge from the labeled source domain to the unlabeled target domain. However, current UDA methods typically assume a shared label space between source and target, limiting their applicability in real-world scenarios where novel categories may emerge in the target domain. In this paper, we introduce Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) for the first time, where the target domain includes unknown classes. We identify two major problems in the OSDA-SS scenario as follows: 1) the existing UDA methods struggle to predict the exact boundary of the unknown classes, and 2) they fail to accurately predict the shape of the unknown classes. To address these issues, we propose Boundary and Unknown Shape-Aware openset domain adaptation, coined BUS. Our BUS can accurately discern the boundaries between known and unknown classes in a contrastive manner using a novel dilationerosion-based contrastive loss. In addition, we propose OpenReMix, a new domain mixing augmentation method that guides our model to effectively learn domain and sizeinvariant features for improving the shape detection of the known and unknown classes. Through extensive experiments, we demonstrate that our proposed BUS effectively detects unknown classes in the challenging OSDA-SS scenario compared to the previous methods by a large margin.",
    "chosen": "This paper addresses open-set domain adaptation for semantic segmentation, where the target domain includes unknown classes. The proposed BUS method accurately discerns boundaries between known and unknown classes and improves shape detection. The method demonstrates effective unknown class detection in challenging scenarios.",
    "rejected": "The authors propose BUS, an open-set domain adaptation method for semantic segmentation capable of detecting unknown classes by using dilation-erosion-based contrastive loss and OpenReMix augmentation, significantly outperforming previous methods in identifying boundaries and shapes of unknown classes."
  },
  {
    "prompt": "summary: This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework, where the input and output of the system are multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key advantages can be brought: 1) We can perform real-like conversations with individuals worldwide in a virtual meeting by utilizing our own primary languages. In contrast to Speech-to-Speech Translation (A2A), which solely translates between audio modalities, the proposed AV2AV directly translates between audio-visual speech. This capability enhances the dialogue experience by presenting synchronized lip movements along with the translated speech. 2) We can improve the robustness of the spoken language translation system. By employing the complementary information of audio-visual speech, the system can effectively translate spoken language even in the presence of acoustic noise, showcasing robust performance. To mitigate the problem of the absence of a parallel AV2AV translation dataset, we propose to train our spoken language translation system with the audio-only dataset of A2A. This is done by learning unified audio-visual speech representations through self-supervised learning in advance to train the translation system. Moreover, we propose an AV-Renderer that can generate raw audio and video in parallel. It is designed with zero-shot speaker modeling, thus the speaker in source audio-visual speech can be maintained at the target translated audio-visual speech. The effectiveness of AV2AV is evaluated with extensive experiments in a many-to-many language translation setting.",
    "chosen": "This paper proposes a direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework. The framework translates between multimodal audio-visual speech inputs and outputs, enhancing dialogue experiences with synchronized lip movements and improving translation robustness in noisy environments.",
    "rejected": "This work introduces AV2AV, a direct audio-visual speech translation framework that enhances dialogue experience by presenting synchronized lip movements along with translated speech, improving robustness against acoustic noise through self-supervised learning of unified audio-visual speech representations."
  },
  {
    "prompt": "summary: We address the problem of generalized category discovery (GCD) that aims to partition a partially labeled collection of images; only a small part of the collection is labeled and the total number of target classes is unknown. To address this generalized image clustering problem, we revisit the mean-shift algorithm, i.e., a classic, powerful technique for mode seeking, and incorporate it into a contrastive learning framework. The proposed method, dubbed Contrastive Mean-Shift (CMS) learning, trains an embedding network to produce representations with better clustering properties by an iterative process of mean shift and contrastive update. Experiments demonstrate that our method, both in settings with and without the total number of clusters being known, achieves state-of-the-art performance on six public GCD benchmarks without bells and whistles.",
    "chosen": "This paper introduces Contrastive Mean-Shift (CMS) learning for generalized category discovery (GCD). The method integrates mean shift into a contrastive learning framework, achieving state-of-the-art performance on GCD benchmarks.",
    "rejected": "The paper discusses Contrastive Mean-Shift (CMS) learning, a method for generalized category discovery that integrates mean shift into a contrastive learning framework, achieving state-of-the-art performance on multiple benchmarks without requiring total cluster number information."
  },
  {
    "prompt": "summary: Photometric stereo leverages variations in illumination conditions to reconstruct surface normals. Display photometric stereo, which employs a conventional monitor as an illumination source, has the potential to overcome limitations often encountered in bulky and difficult-to-use conventional setups. In this paper, we present differentiable display photometric stereo (DDPS), addressing an often overlooked challenge in display photometric stereo: the design of display patterns. Departing from using heuristic display patterns, DDPS learns the display patterns that yield accurate normal reconstruction for a target system in an endto-end manner. To this end, we propose a differentiable framework that couples basis-illumination image formation with analytic photometric-stereo reconstruction. The differentiable framework facilitates the effective learning of display patterns via auto-differentiation. Also, for training supervision, we propose to use 3D printing for creating a real-world training dataset, enabling accurate reconstruction on the target real-world setup. Finally, we exploit that conventional LCD monitors emit polarized light, which allows for the optical separation of diffuse and specular reflections when combined with a polarization camera, leading to accurate normal reconstruction. Extensive evaluation of DDPS shows improved normal-reconstruction accuracy compared to heuristic patterns and demonstrates compelling properties such as robustness to pattern initialization, calibration errors, and simplifications in image formation and reconstruction.",
    "chosen": "This paper presents differentiable display photometric stereo (DDPS), which learns display patterns for accurate normal reconstruction in an end-to-end manner. The framework couples basis-illumination image formation with analytic photometric-stereo reconstruction, showing improved normal-reconstruction accuracy.",
    "rejected": "DDPS is presented as a differentiable display photometric stereo framework that learns optimal illumination patterns via auto-differentiation for accurate surface normal reconstruction, demonstrating superior reconstruction accuracy and robustness against various errors."
  },
  {
    "prompt": "summary: This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with contentbased features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through further analysis, we also validate the importance of using temporal changes of style latent vectors to improve the generality of deepfake video detection.",
    "chosen": "This paper proposes a new approach for fake video detection by analyzing style latent vectors and their temporal changes. The framework utilizes a StyleGRU module and a style attention module to detect visual and temporal artifacts, demonstrating superiority in cross-dataset and cross-manipulation scenarios.",
    "rejected": "A new approach for fake video detection is introduced, focusing on the analysis of style latent vectors' abnormal behavior in temporal changes. Utilizing StyleGRU and style attention modules, this method shows superiority in cross-dataset and cross-manipulation scenarios."
  },
  {
    "prompt": "summary: Advancements in neural signed distance fields (SDFs) have enabled modeling 3D surface geometry from a set of 2D images of real-world scenes. Baking neural SDFs can extract explicit mesh with appearance baked into texture maps as neural features. The baked meshes still have a large memory footprint and require a powerful GPU for real-time rendering. Neural optimization of such large meshes with differentiable rendering pose significant challenges. We propose a method to produce optimized meshes for large unbounded scenes with low triangle budget and high fidelity of geometry and appearance. We achieve this by combining advancements in baking neural SDFs with classical mesh simplification techniques and proposing a joint appearance-geometry refinement step. The visual quality is comparable to or better than state-of-the-art neural meshing and baking methods with high geometric accuracy despite significant reduction in triangle count, making the produced meshes efficient for storage, transmission, and rendering on mobile hardware. We validate the effectiveness of the proposed method on large unbounded scenes from mip-NeRF 360, Tanks & Temples, and Deep Blending datasets, achieving at-par rendering quality with 73× reduced triangles and 11× reduction in memory footprint.",
    "chosen": "This paper presents LTM, a method for producing optimized meshes for large unbounded scenes with low triangle budget and high fidelity. The method combines baking neural SDFs with classical mesh simplification techniques, achieving significant reduction in triangle count and memory footprint while maintaining visual quality.",
    "rejected": "The research proposes a method for optimizing large unbounded scene meshes derived from neural SDFs with classical mesh simplification techniques, achieving high fidelity in geometry and appearance while significantly reducing triangle count for efficient storage and transmission."
  },
  {
    "prompt": "summary: Omnidirectional cameras are extensively used in various applications to provide a wide field of vision. However, they face a challenge in synthesizing novel views due to the inevitable presence of dynamic objects, including the photographer, in their wide field of view. In this paper, we introduce a new approach called Omnidirectional Local Radiance Fields (OmniLocalRF) that can render staticonly scene views, removing and inpainting dynamic objects simultaneously. Our approach combines the principles of local radiance fields with the bidirectional optimization of omnidirectional rays. Our input is an omnidirectional video, and we evaluate the mutual observations of the entire angle between the previous and current frames. To reduce ghosting artifacts of dynamic objects and in paint occlusions, we devise a multi-resolution motion mask prediction module. Unlike existing methods that primarily separate dynamic components through the temporal domain, our method uses multi-resolution neural feature planes for precise segmentation, which is more suitable for long 360◦ videos. Our experiments validate that OmniLocalRF outperforms existing methods in both qualitative and quantitative metrics, especially in scenarios with complex realworld scenes. In particular, our approach eliminates the need for manual interaction, such as drawing motion masks by hand and additional pose estimation, making it a highly effective and efficient solution.",
    "chosen": "This paper introduces OmniLocalRF, a method for rendering static-only scene views from omnidirectional videos by removing and inpainting dynamic objects. The approach combines local radiance fields with bidirectional optimization of omnidirectional rays, outperforming existing methods in complex real-world scenes.",
    "rejected": "This paper introduces a novel framework for spatio-temporal action grounding trained solely on video and subtitle supervision, eliminating the need for human annotations. The method combines local representation learning to capture fine-grained spatial information with global representation encoding for higher-level representations. A new benchmark dataset is proposed, featuring dense spatio-temporal grounding annotations in long, untrimmed multi-action instructional videos. Evaluations demonstrate that the proposed approach outperforms current baselines in various settings, including spatial and temporal localization tasks."
  },
  {
    "prompt": "summary: Video Transformers have become the prevalent solution for various video downstream tasks with superior expressive power and flexibility. However, these video transformers suffer from heavy computational costs induced by the massive number of tokens across the entire video frames, which has been the major barrier to train and deploy the model. Further, the patches irrelevant to the main contents, e.g., backgrounds, degrade the generalization performance of models. To tackle these issues, we propose training-free token merging for lightweight video Transformer (vid-TLDR) that aims to enhance the efficiency of video Transformers by merging the background tokens without additional training. For vid-TLDR, we introduce a novel approach to capture the salient regions in videos only with the attention map. Further, we introduce the saliency-aware token merging strat*: Equal contribution, †: Corresponding author. egy by dropping the background tokens and sharpening the object scores. Our experiments show that vid-TLDR significantly mitigates the computational complexity of video Transformers while achieving competitive performance compared to the base model without vid-TLDR.",
    "chosen": "This paper proposes vid-TLDR, a training-free token merging method for lightweight video Transformers. The method enhances efficiency by merging background tokens using a saliency-aware strategy, significantly mitigating computational complexity while maintaining competitive performance.",
    "rejected": "AMUSE is presented as an emotional speech-driven body animation model based on latent diffusion. It addresses the limitation of existing methods by explicitly modeling the impact of emotions on generated gestures. AMUSE maps audio input to three disentangled latent vectors representing content, emotion, and personal style. These vectors condition a latent diffusion model trained to generate gesture motion sequences. AMUSE synthesizes realistic 3D human gestures from speech while allowing control over expressed emotions and style. Comprehensive evaluations show that AMUSE produces more synchronized and emotionally expressive gestures compared to state-of-the-art methods."
  },
  {
    "prompt": "summary: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Fields (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view image and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic GeoReasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified.",
    "chosen": "This paper introduces GSNeRF, a generalizable semantic neural radiance field that produces novel-view images and associated semantic maps for unseen scenes. GSNeRF consists of Semantic Geo-Reasoning and Depth-Guided Visual Rendering stages. Experiments confirm GSNeRF's effectiveness in novel-view synthesis and semantic segmentation.",
    "rejected": "Generalizable Semantic Neural Radiance Fields (GSNeRF) integrates semantic information into novel view synthesis, enabling both image and semantic map generation. It uses a two-stage process: Semantic Geo-Reasoning extracts features from multi-view images, while Depth-Guided Visual rendering generates improved images and semantic renderings. GSNeRF demonstrates superior performance in novel view synthesis and semantic segmentation tasks compared to prior methods."
  },
  {
    "prompt": "summary: Deep Neural Networks (DNNs) are widely used for visual classiﬁcation tasks, but their complex computation process and black-box nature hinder decision transparency and interpretability. Class activation maps (CAMs) and recent variants provide ways to visually explain the DNN decisionmaking process by displaying ‘attention’ heatmaps of the DNNs. Nevertheless, the CAM explanation only offers relative attention information, that is, on an attention heatmap, we can interpret which image region is more or less important than the others. However, these regions cannot be meaningfully compared across classes, and the contribution of each region to the model’s class prediction is not revealed. To address these challenges that ultimately lead to better DNN Interpretation, in this paper, we propose CAPE, a novel reformulation of CAM that provides a uniﬁed and probabilistically meaningful assessment of the contributions of image regions. We quantitatively and qualitatively compare CAPE with state-of-the-art CAM methods on CUB and ImageNet benchmark datasets to demonstrate enhanced interpretability. We also test on a cytology imaging dataset depicting a challenging Chronic Myelomonocytic Leukemia (CMML) diagnosis problem.",
    "chosen": "This paper proposes CAPE, a reformulation of class activation maps (CAM) that provides a unified and probabilistically meaningful assessment of image region contributions. CAPE enhances DNN interpretation by revealing region contributions to class predictions, demonstrated on benchmark datasets and a medical imaging task.",
    "rejected": "CAPE reformulates Class Activation Maps (CAM) to offer a unified and probabilistically meaningful assessment of image region contributions. Unlike traditional CAMs that provide relative attention values, CAPE enables cross-class comparisons and reveals each region's contribution to class prediction. Experiments on CUB, ImageNet, and a cytology dataset demonstrate enhanced interpretability and diagnostic accuracy, making DNN decisions more transparent."
  },
  {
    "prompt": "summary: Music is a universal language that can communicate emotions and feelings. It forms an essential part of the whole spectrum of creative media, ranging from movies to social media posts. Machine learning models that can synthesize music are predominantly conditioned on textual descriptions of it. Inspired by how musicians compose music not just from a movie script, but also through visualizations, we propose MELFUSION, a model that can effectively use cues from a textual description and the corresponding image to synthesize music. MELFUSION is a text-to-music diffusion model with a novel “visual synapse”, which effectively infuses the semantics from the visual modality into the generated music. To facilitate research in this area, we introduce a new dataset MeLBench, and propose a new evaluation metric IMSM. Our exhaustive experimental evaluation suggests that adding visual information to the music synthesis pipeline significantly improves the quality of generated music, measured both objectively and subjectively, with a relative gain of up to 67.98% on the FAD score. We hope that our work will gather attention to this pragmatic, yet relatively under-explored research area.",
    "chosen": "This paper presents MELFUSION, a text-to-music diffusion model with a \"visual synapse\" that infuses visual semantics into music synthesis. The model improves music quality using visual and textual cues, validated on a new dataset and evaluation metric.",
    "rejected": "MELFUSION is a text-to-music diffusion model incorporating visual cues for music synthesis. By integrating a \"visual synapse,\" it effectively blends textual descriptions with corresponding images to produce high-quality music. An evaluation on MeLBench shows significant improvements in generated music quality, achieving up to a 67.98% gain on the FAD score. This approach highlights the potential of multimodal inputs in creative applications."
  },
  {
    "prompt": "summary: Open-vocabulary semantic segmentation presents the challenge of labeling each pixel within an image based on a wide range of text descriptions. In this work, we introduce a novel cost-based approach to adapt vision-language foundation models, notably CLIP, for the intricate task of semantic segmentation. Through aggregating the cosine similarity score, i.e., the cost volume between image and text embeddings, our method potently adapts CLIP for segmenting seen and unseen classes by fine-tuning its encoders, addressing the challenges faced by existing methods in handling unseen classes. Building upon this, we explore methods to effectively aggregate the cost volume considering its multi-modal nature of being established between image and text embeddings. Furthermore, we examine various methods for efficiently fine-tuning CLIP.",
    "chosen": "This paper introduces a cost-based framework for open-vocabulary semantic segmentation, adapting CLIP through cost volume aggregation. The method effectively segments seen and unseen classes, with fine-tuning improving performance on unseen classes.",
    "rejected": "A cost-based approach adapts vision-language models like CLIP for open-vocabulary semantic segmentation by aggregating cosine similarity scores between image and text embeddings. Fine-tuning CLIP encoders improves handling of seen and unseen classes, addressing limitations in existing methods. Cost aggregation proves effective in generalizing to unseen classes, significantly enhancing performance metrics over feature aggregation methods."
  },
  {
    "prompt": "summary: We present a new open-vocabulary detection framework. Our framework uses both image-level labels and detailed detection annotations when available. Our framework proceeds in three steps. We first train a language-conditioned object detector on fully-supervised detection data. This detector gets to see the presence or absence of ground truth classes during training, and conditions prediction on the set of present classes. We use this detector to pseudolabel images with image-level labels. Our detector provides much more accurate pseudo-labels than prior approaches with its conditioning mechanism. Finally, we train an unconditioned open-vocabulary detector on the pseudo-annotated images. The resulting detector, named DECOLA, shows strong zero-shot performance in openvocabulary LVIS benchmark as well as direct zero-shot transfer benchmarks on LVIS, COCO, Object365, and OpenImages. DECOLA outperforms the prior arts by 17.1 APrare and 9.4 mAP on zero-shot LVIS benchmark. DECOLA achieves state-of-the-art results in various model sizes, architectures, and datasets by only training on open-sourced data and academic-scale computing.",
    "chosen": "This paper presents DECOLA, an open-vocabulary detection framework using language-conditioned object detectors. The framework generates accurate pseudo-labels and trains an unconditioned detector, showing strong zero-shot performance on multiple benchmarks.",
    "rejected": "DECOLA, a language-conditioned object detector, leverages image-level labels and detection annotations to generate accurate pseudo-labels for training. It outperforms previous methods in zero-shot detection benchmarks on LVIS, COCO, Object365, and OpenImages datasets. DECOLA achieves state-of-the-art results across various model sizes and architectures, demonstrating strong generalization capabilities in open-vocabulary settings."
  },
  {
    "prompt": "summary: While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with outof-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models. The source",
    "chosen": "This paper proposes OSASIS, a one-shot stylization method using diffusion models. OSASIS disentangles image semantics and structure, offering control over content and style levels. It outperforms other methods, especially on rarely encountered inputs.",
    "rejected": "OSASIS introduces a one-shot stylization method using diffusion models, focusing on robust structure preservation. It disentangles semantics from structure, allowing controlled content and style implementation. OSASIS excels particularly in handling rare input images, offering promising solutions for stylization via diffusion models. The approach surpasses other stylization methods in preserving structural integrity."
  },
  {
    "prompt": "summary: Video Frame Interpolation (VFI), which aims at generating high-frame-rate videos from low-frame-rate inputs, is a highly challenging task. The emergence of bio-inspired sensors known as event cameras, which boast micro-second-level temporal resolution, has ushered in a transformative era for VFI. Nonetheless, the application of event-based VFI techniques in domains with distinct environments from the training data can be problematic. This is mainly because event camera data distribution can undergo substantial variations based on camera settings and scene conditions, presenting challenges for effective adaptation. In this paper, we propose a test-time adaptation method for eventbased VFI to address the gap between the source and target domains. Our approach enables sequential learning in an online manner on the target domain, which only provides low-frame-rate videos. We present an approach that leverages confident pixels as pseudo ground-truths, enabling stable and accurate online learning from low-frame-rate videos. Furthermore, to prevent overfitting during the continuous online process where the same scene is encountered repeatedly, we propose a method of blending historical samples with current scenes. Extensive experiments validate the effectiveness of our method, both in cross-domain and continuous domain shifting setups.",
    "chosen": "This paper proposes a test-time adaptation method for event-based video frame interpolation. The approach uses confident pixels as pseudo ground-truths for stable online learning, blending historical samples to prevent overfitting. Experiments validate its effectiveness in cross-domain and continuous domain shifting setups.",
    "rejected": "TTA-EVF proposes a test-time adaptation method for event-based Video Frame Interpolation (VFI). It leverages confident pixels as pseudo ground-truths for online learning from low-frame-rate videos, blending historical samples to prevent overfitting. Extensive experiments validate its effectiveness in cross-domain and continuous domain shifting setups, enhancing VFI performance without offline data supply."
  },
  {
    "prompt": "summary: Each photo in an image burst can be considered a sample of a complex 3D scene: the product of parallax, diffuse and specular materials, scene motion, and illuminant variation. While decomposing all of these effects from a stack of misaligned images is a highly ill-conditioned task, the conventional align-and-merge burst pipeline takes the other extreme: blending them into a single image. In this work, we propose a versatile intermediate representation: a twolayer alpha-composited image plus flow model constructed with neural spline fields – networks trained to map input coordinates to spline control points. Our method is able to, during test-time optimization, jointly fuse a burst image capture into one high-resolution reconstruction and decompose it into transmission and obstruction layers. Then, by discarding the obstruction layer, we can perform a range of tasks including seeing through occlusions, reflection suppression, and shadow removal. Tested on complex in-thewild captures we find that, with no post-processing steps or learned priors, our generalizable model is able to outperform existing dedicated single-image and multi-view obstruction removal approaches.",
    "chosen": "This paper introduces neural spline fields for burst image fusion and layer separation. The method constructs a two-layer alpha-composited image plus flow model, enabling tasks like occlusion removal and reflection suppression.",
    "rejected": "This work introduces a neural spline fields-based intermediate representation for burst photography, creating a two-layer alpha-composited image plus flow model. During test-time optimization, it fuses burst captures into a high-resolution reconstruction and decomposes them into transmission and obstruction layers. Discarding the obstruction layer facilitates tasks such as seeing through occlusions, reflection suppression, and shadow removal, outperforming existing single-image and multi-view approaches."
  },
  {
    "prompt": "summary: Recent advancements in Spatial Transcriptomics (ST) technology have facilitated detailed gene expression analysis within tissue contexts. However, the high costs and methodological limitations of ST necessitate a more robust predictive model. In response, this paper introduces TRIPLEX, a novel deep learning framework designed to predict spatial gene expression from Whole Slide Images (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing cellular morphology at individual spots, the local context around these spots, and the global tissue organization. By integrating these features through an effective fusion strategy, TRIPLEX achieves accurate gene expression prediction. Our comprehensive benchmark study, conducted on three public ST datasets and supplemented with Visium data from 10X Genomics, demonstrates that TRIPLEX outperforms current state-of-the-art models in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC). The model’s predictions align closely with ground truth gene expression profiles and tumor annotations, underscoring TRIPLEX’s potential in advancing cancer diagnosis and treatment.",
    "chosen": "This paper presents TRIPLEX, a deep learning framework predicting spatial gene expression from whole slide images. TRIPLEX uses multi-resolution features and outperforms state-of-the-art models in prediction accuracy.",
    "rejected": "TRIPLEX introduces a deep learning framework for predicting spatial gene expression from Whole Slide Images (WSIs). By integrating multi-resolution features, it captures cellular morphology at individual spots, local context around these spots, and global tissue organization. TRIPLEX employs an effective fusion strategy to achieve accurate predictions, outperforming state-of-the-art models in MSE, MAE, and PCC on three public ST datasets and Visium data. The model's predictions closely align with ground truth gene expressions and tumor annotations, highlighting its potential for advancing cancer diagnosis and treatment."
  },
  {
    "prompt": "summary: Existing research based on deep learning has extensively explored the problem of daytime image dehazing. However, few studies have considered the characteristics of nighttime hazy scenes. There are two distinctions between nighttime and daytime haze. First, there may be multiple active colored light sources with lower illumination intensity in nighttime scenes, which may cause haze, glow and noise with localized, coupled and frequency inconsistent characteristics. Second, due to the domain discrepancy between simulated and real-world data, unrealistic brightness may occur when applying a dehazing model trained on simulated data to real-world data. To address the above two issues, we propose a semi-supervised model for real-world nighttime dehazing. First, the spatial attention and frequency spectrum ﬁltering are implemented as a spatial-frequency domain information interaction module to handle the ﬁrst issue. Second, a pseudo-label-based retraining strategy and a local window-based brightness loss for semi-supervised training process is designed to suppress haze and glow while achieving realistic brightness. Experiments on public benchmarks validate the effectiveness of the proposed method and its superiority over state-of-the-art methods. The source",
    "chosen": "This paper proposes a semi-supervised model for real-world nighttime dehazing. The method includes a spatial-frequency domain information interaction module and strategies to handle nighttime haze characteristics and domain discrepancies.",
    "rejected": "A novel semi-supervised model is proposed for real-world nighttime dehazing, addressing the unique challenges of nighttime hazy scenes such as localized glow, noise, and frequency inconsistencies due to multiple active light sources. The approach includes a spatial-frequency domain information interaction module and a pseudo-label-based retraining strategy with a local window-based brightness loss to ensure realistic brightness while suppressing haze and glow. Experiments on public benchmarks demonstrate the method's effectiveness and superiority over existing techniques, emphasizing its capability to handle complex nighttime imaging conditions effectively."
  },
  {
    "prompt": "summary: Recent advancements have shown the potential of leveraging both point clouds and images to localize anomalies. Nevertheless, their applicability in industrial manufacturing is often constrained by significant drawbacks, such as the use of memory banks, which leads to a substantial increase in terms of memory footprint and inference times. We propose a novel light and fast framework that learns to map features from one modality to the other on nominal samples and detect anomalies by pinpointing inconsistencies between observed and mapped features. Extensive experiments show that our approach achieves state-of-the-art detection and segmentation performance in both the standard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods. Furthermore, we propose a layer pruning technique to improve memory and time efficiency with a marginal sacrifice in performance.",
    "chosen": "This paper presents a multimodal industrial anomaly detection framework that maps features from one modality to another on nominal samples. The approach detects anomalies by identifying inconsistencies between observed and mapped features. Experiments show state-of-the-art performance on the MVTec 3D-AD dataset with faster inference and lower memory usage than previous methods.",
    "rejected": "Recent research proposes a lightweight framework for anomaly detection in industrial settings that maps features between point clouds and images, detecting inconsistencies to identify anomalies. This method outperforms existing techniques in accuracy and speed on the MVTec 3D-AD dataset while using less memory."
  },
  {
    "prompt": "summary: Lifelong Person Re-identiﬁcation (L-ReID) aims to learn from sequentially collected data to match a person across different scenes. Once an L-ReID model is updated using new data, all historical images in the gallery are required to be re-calculated to obtain new features for testing, known as “re-indexing”. However, it is infeasible when raw images in the gallery are unavailable due to data privacy concerns, resulting in incompatible retrieval between the query and the gallery features calculated by different models, which causes signiﬁcant performance degradation. In this paper, we focus on a new task called Re-indexing Free Lifelong Person Re-identiﬁcation (RFL-ReID), which requires achieving effective L-ReID without re-indexing raw images in the gallery. To this end, we propose a Continual Compatible Representation (C2R) method, which facilitates the query feature calculated by the continuously updated model to effectively retrieve the gallery feature calculated by the old model in a compatible manner. Speciﬁcally, we design a Continual Compatible Transfer (CCT) network to continuously transfer and consolidate the old gallery feature into the new feature space. Besides, a Balanced Compatible Distillation module is introduced to achieve compatibility by aligning the transferred feature space with the new feature space. Finally, a Balanced Anti-forgetting Distillation module is proposed to eliminate the accumulated forgetting of old knowledge during the continual compatible transfer. Extensive experiments on several benchmark LReID datasets demonstrate the effectiveness of our method against state-of-the-art methods for both RFL-ReID and LReID tasks. The source",
    "chosen": "This paper introduces the Re-indexing Free Lifelong Person Re-identification (RFL-ReID) task and proposes the Continual Compatible Representation (C2R) method. C2R facilitates effective retrieval without re-indexing raw images in the gallery, using a Continual Compatible Transfer (CCT) network and Balanced Compatible Distillation module. Experiments demonstrate effectiveness on benchmark datasets.",
    "rejected": "This paper introduces Re-indexing Free Lifelong Person Re-identification (RFL-ReID), aiming to eliminate the need for re-indexing gallery images during model updates. It presents a Continual Compatible Representation (C2R) approach to maintain retrieval compatibility across model updates without accessing original images, showing improved performance on benchmark datasets."
  },
  {
    "prompt": "summary: Recent advances in instruction tuning have led to the development of State-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these models, the impact of visual adversarial attacks on LMMs has not been thoroughly examined. We conduct a comprehensive study of the robustness of various LMMs against different adversarial attacks, evaluated across tasks including image classification, image captioning, and Visual Question Answer (VQA). We find that in general LMMs are not robust to visual adversarial inputs. However, our findings suggest that context provided to the model via prompts—such as questions in a QA pair—helps to mitigate the effects of visual adversarial inputs. Notably, the LMMs evaluated demonstrated remarkable resilience to such attacks on the ScienceQA task with only an 8.10% drop in performance compared to their visual counterparts which dropped 99.73%. We also propose a new approach to real-world image classification which we term query decomposition. By incorporating existence queries into our input prompt we observe diminished attack effectiveness and improvements in image classification accuracy. This research highlights a previously under explored facet of LMM robustness and sets the stage for future work aimed at strengthening the resilience of multimodal systems in adversarial environments.",
    "chosen": "This paper evaluates the robustness of large multimodal models (LMMs) against visual adversarial attacks across tasks like image classification and VQA. Findings indicate LMMs are not robust, but context via prompts mitigates effects. A query decomposition approach for image classification is proposed.",
    "rejected": "The study examines the robustness of Large Multimodal Models (LMMs) against visual adversarial attacks, finding that LMMs are generally not robust but can be mitigated with contextual prompts. The proposed query decomposition method enhances image classification accuracy under adversarial conditions."
  },
  {
    "prompt": "summary: Acquiring large-scale, well-annotated datasets is essential for training robust scene text detectors, yet the process is often resource-intensive and time-consuming. While some efforts have been made to explore the synthesis of scene text images, a notable gap remains between synthetic and authentic data. In this paper, we introduce a novel method that utilizes Neural Radiance Fields (NeRF) to model real-world scenes and emulate the data collection process by rendering images from diverse camera perspectives, enriching the variability and realism of the synthesized data. A semi-supervised learning framework is proposed to categorize semantic regions within 3D scenes, ensuring consistent labeling of text regions across various viewpoints. Our method also models the pose, and view-dependent appearance of text regions, thereby offering precise control over camera poses and significantly *Corresponding author. E-mail: lianzhouhui@pku.edu.cn This work was supported by National Natural Science Foundation of China (Grant No.: 62372015), Center For Chinese Font Design and Research, and Key Laboratory of Intelligent Press Media Technology. improving the realism of text insertion and editing within scenes. Employing our technique on real-world scenes has led to the creation of a novel scene text image dataset. Compared to other existing benchmarks, the proposed dataset is distinctive in providing not only standard annotations such as bounding boxes and transcriptions but also the information of 3D pose attributes for text regions, enabling a more detailed evaluation of the robustness of text detection algorithms. Through extensive experiments, we demonstrate the effectiveness of our proposed method in enhancing the performance of scene text detectors.",
    "chosen": "This paper proposes TextNeRF, a method using Neural Radiance Fields to synthesize scene text images by modeling real-world scenes and rendering images from diverse perspectives. A semi-supervised learning framework ensures consistent labeling of text regions. The method improves realism of text insertion and editing within scenes.",
    "rejected": "A novel method is introduced for synthesizing scene text images using Neural Radiance Fields (NeRF), providing enriched variability and realism. The technique includes a semi-supervised learning framework for consistent labeling across viewpoints, enhancing the training of scene text detectors through a new dataset."
  },
  {
    "prompt": "summary: The lifting of a 3D structure and camera from 2D landmarks is at the cornerstone of the discipline of computer vision. Traditional methods have been confined to specific rigid objects, such as those in Perspective-n-Point (PnP) problems, but deep learning has expanded our capability to reconstruct a wide range of object classes (e.g. C3DPO [18] and PAUL [24]) with resilience to noise, occlusions, and perspective distortions. However, all these techniques have been limited by the fundamental need to establish correspondences across the 3D training data, significantly limiting their utility to applications where one has an abundance of “in-correspondence” 3D data. Our approach harnesses the inherent permutation equivariance of transformers to manage varying numbers of points per 3D data instance, withstands occlusions, and generalizes *Both authors advised equally. to unseen categories. We demonstrate state-of-the-art performance across 2D-3D lifting task benchmarks. Since our approach can be trained across such a broad class of structures, we refer to it simply as a 3D Lifting Foundation Model (3D-LFM) – the first of its kind.",
    "chosen": "This paper introduces 3D-LFM, a 3D lifting foundation model leveraging transformer permutation equivariance for 2D-3D lifting tasks. The model handles varying points per 3D data instance, withstands occlusions, and generalizes to unseen categories, showing state-of-the-art performance.",
    "rejected": "This work develops a transformer-based approach called 3D Lifting Foundation Model (3D-LFM) for lifting 2D landmarks into 3D structures, demonstrating state-of-the-art performance across various benchmarks. The model's capability to handle occlusions and unseen categories marks a significant advancement in the field."
  },
  {
    "prompt": "summary: Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the common dropout techniques to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this study, we delve into this contrasting phenomenon through the lens of modality bias and uncover that an excessive modality bias towards the audio modality induced by dropout constitutes the fundamental cause. Next, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between the modality bias and the robustness against missing modality in multimodal systems. Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality, maintaining performance and robustness simultaneously. Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies. The effectiveness of our proposed approach is evaluated through comprehensive experiments on the MISP2021 and MISP2022 datasets. Our",
    "chosen": "This paper studies dropout-induced modality bias in Audio-Visual Speech Recognition (AVSR) systems. It presents the Modality Bias Hypothesis (MBH) and proposes a Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on audio, maintaining performance and robustness.",
    "rejected": "The paper explores modality bias in Audio-Visual Speech Recognition (AVSR) systems, proposing a Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on audio modality, ensuring robustness against missing video frames without compromising performance on complete data."
  },
  {
    "prompt": "summary: Retrieval tasks play central roles in real-world machine learning systems such as search engine, recommender system, and retrieval-augmented generation (RAG). Achieving decent performance in these tasks often requires fine-tuning various pretrained models on specific datasets and selecting the best candidate, a process that can be both time and resource consuming. To tackle the problem, we introduce a novel and efficient method, called RetMMD, that leverages Maximum Mean Discrepancy (MMD) and kernel methods to assess the transferability of pretrained models in retrieval tasks. RetMMD is calculated on pretrained model and target dataset without any fine-tuning involved. Specifically, given some query, we quantify the distribution discrepancy between relevant and irrelevant document embeddings, by estimating the similarities within their mappings in the finetuned embedding space through kernel method. This discrepancy is averaged over multiple queries, taking into account the distribution characteristics of the target dataset. Experiments suggest that the proposed metric calculated on pretrained models closely aligns with retrieval performance post fine-tuning. The observation holds across a variety of datasets, including image, text, and multi-modal domains, indicating the potential of using MMD and kernel methods for transfer learning evaluation in retrieval scenarios. In addition, we also design a way of evaluating dataset transferability for retrieval tasks, with experimental results demonstrating the effectiveness of the proposed approach.",
    "chosen": "This paper introduces RetMMD, a method using Maximum Mean Discrepancy (MMD) and kernel methods to assess pretrained model transferability in retrieval tasks. Experiments show the metric aligns with retrieval performance post fine-tuning across various datasets.",
    "rejected": "A novel metric named RetMMD is introduced for evaluating transferability of pretrained models in retrieval tasks, leveraging Maximum Mean Discrepancy (MMD) and kernel methods. Experiments demonstrate that RetMMD aligns closely with retrieval performance post fine-tuning, offering a promising evaluation approach across various domains."
  },
  {
    "prompt": "summary: It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO. In this paper, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc. The scalability of inputs may accommodate users’ choices for both high tracking accuracy and easy-to-wear. A lightweight temporalspatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents online body shape estimation to improve the position accuracy of body joints. Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-theart results in both accuracy and real-time performance. We also build a new free-dancing motion dataset to evaluate HMD-Poser’s on-device performance and investigate the performance gap between synthetic data and real-captured sensor data. Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our",
    "chosen": "This paper proposes HMD-Poser, a unified approach for real-time full-body motion tracking on VR HMDs using sparse observations from HMD and body-worn IMUs. The approach supports various input scenarios and includes a lightweight network and online body shape estimation.",
    "rejected": "HMD-Poser is presented as the first unified approach for real-time full-body motion tracking using sparse observations from HMD and body-worn IMUs. The lightweight temporal-spatial feature learning network ensures real-time operation on standalone VR headsets, achieving state-of-the-art results in both accuracy and performance."
  },
  {
    "prompt": "summary: Generative models have been very popular in the recent years for their image generation capabilities. GAN-based models are highly regarded for their disentangled latent space, which is a key feature contributing to their success in controlled image editing. On the other hand, diffusion models have emerged as powerful tools for generating highquality images. However, the latent space of diffusion models is not as thoroughly explored or understood. Existing methods that aim to explore the latent space of diffusion models usually relies on text prompts to pinpoint specific semantics. However, this approach may be restrictive in areas such as art, fashion, or specialized fields like medicine, where suitable text prompts might not be available or easy to conceive thus limiting the scope of existing work. In this paper, we propose an unsupervised method to discover latent semantics in text-to-image diffusion models without reThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 24209",
    "chosen": "This paper presents NoiseCLR, a contrastive learning approach for unsupervised discovery of interpretable semantics in the latent space of diffusion models without relying on text prompts.",
    "rejected": "This paper explores the latent space of text-to-image diffusion models, aiming to discover latent semantics without relying on restrictive text prompts. Existing methods often depend on text inputs to pinpoint specific semantics, which can be limiting in specialized fields like art, fashion, or medicine where suitable text prompts may not be readily available. The proposed unsupervised method aims to address this limitation by discovering meaningful latent variables directly from the data, enhancing the applicability and flexibility of diffusion models in generating high-quality images across various domains."
  },
  {
    "prompt": "summary: We introduce a new family of minimal problems for reconstruction from multiple views. Our primary focus is a novel approach to autocalibration, a long-standing problem in computer vision. Traditional approaches to this problem, such as those based on Kruppa’s equations or the modulus constraint, rely explicitly on the knowledge of multiple fundamental matrices or a projective reconstruction. In contrast, we consider a novel formulation involving constraints on image points, the unknown depths of 3D points, and a partially specified calibration matrix K. For 2 and 3 views, we present a comprehensive taxonomy of minimal autocalibration problems obtained by relaxing some of these constraints. These problems are organized into classes according to the number of views and any assumed prior knowledge of K. Within each class, we determine problems with the fewest—or a relatively small number of—solutions. From this zoo of problems, we devise three practical solvers. Experiments with synthetic and real data and interfacing our solvers with COLMAP demonstrate that we achieve superior accuracy compared to stateof-the-art calibration methods.",
    "chosen": "This paper introduces a new family of minimal problems for reconstruction from multiple views, focusing on autocalibration. The approach involves constraints on image points, unknown depths, and a partially specified calibration matrix K, with practical solvers developed for minimal autocalibration problems.",
    "rejected": "The research introduces a novel family of minimal problems for autocalibration in multi-view reconstruction, focusing on a new approach that relaxes traditional constraints such as those based on Kruppa’s equations or modulus constraint. By formulating the problem with constraints on image points, unknown depths of 3D points, and partially specified calibration matrices, the study presents a comprehensive taxonomy of minimal autocalibration problems for 2 and 3 views. Practical solvers developed from this framework demonstrate superior accuracy compared to state-of-the-art methods when tested on both synthetic and real datasets. The approach also integrates seamlessly with existing tools like COLMAP, showcasing its practical utility and robustness."
  },
  {
    "prompt": "summary: In this work, we tackle the problem of domain generalization for object detection, specifically focusing on the scenario where only a single source domain is available. We propose an effective approach that involves two key steps: diversifying the source domain and aligning detections based on class prediction confidence and localization. Firstly, we demonstrate that by carefully selecting a set of augmentations, a base detector can outperform existing methods for single domain generalization by a good margin. This highlights the importance of domain diversification in improving the performance of object detectors. Secondly, we introduce a method to align detections from multiple views, considering both classification and localization outputs. This alignment procedure leads to better generalized and well-calibrated object detector models, which are crucial for accurate decision-making in safety-critical applications. Our approach is detector-agnostic and can be seamlessly applied to both single-stage and two-stage detectors. To validate the effectiveness of our proposed methods, we conduct extensive experiments and ablations on challenging domain-shift scenarios. The results consistently demonstrate the superiority of our approach compared to existing methods. Our",
    "chosen": "This paper proposes a method to improve domain generalization for object detection from a single source domain. The approach involves diversifying the source domain through careful augmentation selection and aligning detections based on class prediction confidence and localization. Experiments show superior performance compared to existing methods.",
    "rejected": "In this work, the authors tackle domain generalization for object detection with a single source domain. They introduce diversification through augmentations and align detections based on class prediction confidence and localization, leading to better-generalized models. The approach is validated on challenging domain-shift scenarios, showing superior performance."
  },
  {
    "prompt": "summary: The existing facial datasets, while having plentiful images at near frontal views, lack images with extreme head poses, leading to the downgraded performance of deep learning models when dealing with proﬁle or pitched faces. This work aims to address this gap by introducing a novel dataset named Extreme Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k highquality images of faces at extreme poses. To produce such a massive dataset, we utilize a novel and meticulous dataset processing pipeline to curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many high-resolution face videos captured in various settings. Our dataset can complement existing datasets on various facial-related tasks, such as facial synthesis with 2D/3Daware GAN, diffusion-based text-to-image face generation, and face reenactment. Speciﬁcally, training with EFHQ helps models generalize well across diverse poses, significantly improving performance in scenarios involving extreme views, conﬁrmed by extensive experiments. Additionally, we utilize EFHQ to deﬁne a challenging cross-view face veriﬁcation benchmark, in which the performance of SOTA face recognition models drops 5-37% compared to frontal-to-frontal scenarios, aiming to stimulate studies on face recognition under severe pose conditions in the wild.",
    "chosen": "This paper introduces EFHQ, a large-scale dataset of extreme pose face images. Created by curating existing datasets, EFHQ helps improve model performance on profile and pitched faces. The dataset also defines a challenging cross-view face verification benchmark.",
    "rejected": "This paper introduces EFHQ, a dataset containing 450k high-quality images of faces at extreme poses, curated from existing datasets. Training with EFHQ improves model generalizability across diverse poses, especially in extreme view scenarios. Additionally, a cross-view face verification benchmark using EFHQ reveals significant drops in SOTA models' performance compared to frontal-face scenarios."
  },
  {
    "prompt": "summary: Modeling and visualizing relationships between tasks or datasets is an important step towards solving various meta-tasks such as dataset discovery, multi-tasking, and transfer learning. However, many relationships, such as containment and transferability, are naturally asymmetric and current approaches for representation and visualization (e.g., t-SNE [44]) do not readily support this. We propose TASK2BOX, an approach to represent tasks using box embeddings—axis-aligned hyper rectangles in low dimensional spaces—that can capture asymmetric relationships between them through volumetric overlaps. We show that TASK2BOX accurately predicts unseen hierarchical relationships between nodes in ImageNet and iNaturalist datasets, as well as transferability between tasks in the Taskonomy benchmark. We also show that box embeddings estimated from task representations (e.g., CLIP [36], Task2Vec [4], or attribute based [15]) can be used to predict relationships between unseen tasks more accurately than classifiers trained on the same representations, as well as handcrafted asymmetric distances (e.g., KL divergence). This suggests that low-dimensional box embeddings can effectively capture these task relationships and have the added advantage of being interpretable. We use the approach to visualize relationships among publicly available image classification datasets on popular dataset hosting platform called Hugging Face.",
    "chosen": "This paper presents TASK2BOX, an approach using box embeddings to model asymmetric relationships between tasks. The method accurately predicts hierarchical relationships and transferability between tasks, offering an interpretable way to visualize task relationships.",
    "rejected": "TASK2BOX proposes representing tasks as box embeddings to capture asymmetric relationships between them. This method accurately predicts unseen hierarchical and transferability relationships among tasks, outperforming classifiers trained on the same representations. Visualization of image classification datasets on Hugging Face demonstrates its effectiveness."
  },
  {
    "prompt": "summary: Reconstructing dynamic objects from monocular videos is a severely under-constrained and challenging problem, and recent work has approached it in various directions. However, owing to the ill-posed nature of this problem, there has been no solution that can provide consistent, highquality novel views from camera positions that are significantly different from the training views. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on this challenge by imposing a two-stage approach: first, we fit a low-rank neural deformation model, which then is used as regularization for non-rigid reconstruction in the second stage. The first stage learns the object’s deformations such that it preserves consistency in novel views. The second stage obtains high reconstruction quality by optimizing 3D Gaussians that are driven by the coarse model. To this end, we introduce a local 3D Gaussian representation, where temporally shared Gaussians are anchored in and deformed by local oriented volumes. The resulting combined model can be rendered as radiance fields, resulting in high-quality photo-realistic reconstructions of the non-rigidly deforming objects. We demonstrate that NPGs achieve superior results compared to previous works, especially in challenging scenarios with few multi-view cues.1 1",
    "chosen": "This paper proposes Neural Parametric Gaussians (NPGs) for reconstructing dynamic objects from monocular videos. The two-stage approach first fits a deformation model, then refines it with 3D Gaussians, achieving high-quality reconstructions of non-rigidly deforming objects.",
    "rejected": "NPGs address dynamic object reconstruction from monocular videos by imposing a two-stage approach: fitting a low-rank neural deformation model followed by optimizing 3D Gaussians. This method achieves high-quality photo-realistic reconstructions, demonstrating superiority in scenarios with few multi-view cues."
  },
  {
    "prompt": "summary: Even the best current algorithms for estimating body 3D shape and pose yield results that include body self-intersections. In this paper, we present CLOAF, which exploits the diffeomorphic nature of Ordinary Differential Equations to eliminate such self-intersections while still imposing body shape constraints. We show that, unlike earlier approaches to addressing this issue, ours completely eliminates the self-intersections without compromising the accuracy of the reconstructions. Being differentiable, CLOAF can be used to ﬁne-tune pose and shape estimation baselines to improve their overall performance and eliminate self-intersections in their predictions. Furthermore, we demonstrate how our CLOAF strategy can be applied to practically any motion ﬁeld induced by the user. CLOAF also makes it possible to edit motion to interact with the environment without worrying about potential collision or loss of body-shape prior.",
    "chosen": "This paper presents CLOAF, a method using Ordinary Differential Equations to eliminate self-intersections in human body 3D shape and pose estimation. The differentiable approach improves reconstruction accuracy without explicit detection steps.",
    "rejected": "CLOAF uses differential equations to eliminate body self-intersections while preserving shape constraints during 3D human pose estimation. Unlike iterative methods that minimize interpenetration loss, CLOAF ensures no self-intersections without compromising accuracy, making it suitable for realistic animation and robotics applications."
  },
  {
    "prompt": "summary: We present 3D Paintbrush, a technique for automatically texturing local semantic regions on meshes via text descriptions. Our method is designed to operate directly on meshes, producing texture maps which seamlessly integrate into standard graphics pipelines. We opt to simultaneously produce a localization map (to specify the edit region) and a texture map which conforms to it. This approach improves the quality of both the localization and the stylization. To enhance the details and resolution of the textured area, we leverage multiple stages of a cascaded diffusion model to supervise our local editing technique with generative priors learned from images at different resolutions. Our technique, referred to as Cascaded Score Distillation (CSD), simultaneously distills scores at multiple resolutions in a cascaded fashion, enabling control over both the granularity and global understanding of the supervision. We demonstrate the effectiveness of 3D Paintbrush to locally texture different semantic regions on a variety of shapes.",
    "chosen": "This paper introduces 3D Paintbrush, a technique for texturing local semantic regions on meshes via text descriptions. Using cascaded diffusion models, the method produces high-quality texture maps that integrate into standard graphics pipelines.",
    "rejected": "3D Paintbrush presents a technique for texturing local semantic regions on meshes via text descriptions. By leveraging cascaded diffusion models, it simultaneously produces localization and texture maps, enhancing detail and resolution. The method allows for intuitive control over edits, seamlessly integrating into standard graphics pipelines."
  },
  {
    "prompt": "summary: When building classification systems with demographic fairness considerations, there are two objectives to satisfy: 1) maximizing utility for the specific task and 2) ensuring fairness w.r.t. a known demographic attribute. These objectives often compete, so optimizing both can lead to a trade-off between utility and fairness. While existing works acknowledge the trade-offs and study their limits, two questions remain unanswered: 1) What are the optimal tradeoffs between utility and fairness? and 2) How can we numerically quantify these trade-offs from data for a desired prediction task and demographic attribute of interest? This paper addresses these questions. We introduce two utilityfairness trade-offs: the Data-Space and Label-Space Tradeoff. The trade-offs reveal three regions within the utilityfairness plane, delineating what is fully and partially possible and impossible. We propose U-FaTE, a method to numerically quantify the trade-offs for a given prediction task and group fairness definition from data samples. Based on the trade-offs, we introduce a new scheme for evaluating representations. An extensive evaluation of fair representation learning methods and representations from over 1000 pre-trained models revealed that most current approaches are far from the estimated and achievable fairness-utility trade-offs across multiple datasets and prediction tasks.",
    "chosen": "This paper addresses the utility-fairness trade-offs in classification systems with demographic fairness considerations. The authors introduce methods to quantify these trade-offs and evaluate fair representation learning methods.",
    "rejected": "The paper discusses utility-fairness trade-offs in classification systems considering demographic fairness. It introduces Data-Space and Label-Space Trade-offs, delineating what is possible, partially possible, and impossible. U-FaTE quantifies these trade-offs for given tasks and demographic attributes, revealing that most current approaches fall short of achievable fairness-utility balances."
  },
  {
    "prompt": "summary: Estimating large, extreme inter-image rotations is critical for numerous computer vision domains involving images related by limited or non-overlapping fields of view. In this work, we propose an attention-based approach with a pipeline of novel algorithmic components. First, as rotation estimation pertains to image pairs, we introduce an inter-image distillation scheme using Decoders to improve embeddings. Second, whereas contemporary methods compute a 4D correlation volume (4DCV) encoding inter-image relationships, we propose an Encoder-based cross-attention approach between activation maps to compute an enhanced equivalent of the 4DCV. Finally, we present a cascaded Decoder-based technique for alternately refining the cross-attention and the rotation query. Our approach outperforms current state-of-the-art methods on extreme rotation estimation. We make our",
    "chosen": "This paper proposes an attention-based approach with novel components for estimating extreme 3D image rotations. The method uses inter-image distillation and cross-attention to outperform state-of-the-art rotation estimation techniques.",
    "rejected": "An attention-based approach for estimating large inter-image rotations is proposed, involving inter-image distillation and Encoder-based cross-attention to enhance 4D correlation volume computation. A cascaded Decoder-based refinement technique alternately refines cross-attention and rotation query, outperforming state-of-the-art methods on extreme rotation estimation."
  },
  {
    "prompt": "summary: Existing 3D scene understanding methods are heavily focused on 3D semantic and instance segmentation. However, identifying objects and their parts only constitutes an intermediate step towards a more ﬁne-grained goal, which is effectively interacting with the functional interactive elements (e.g., handles, knobs, buttons) in the scene to accomplish diverse tasks. To this end, we introduce SceneFun3D, a large-scale dataset with more than 14.8k highly accurate interaction annotations for 710 high-resolution realworld 3D indoor scenes. We accompany the annotations with motion parameter information, describing how to interact with these elements, and a diverse set of natural language descriptions of tasks that involve manipulating them in the scene context. To showcase the value of our dataset, we introduce three novel tasks, namely functionality segmentation, task-driven affordance grounding and 3D motion estimation, and adapt existing state-of-the-art methods to tackle them. Our experiments show that solving these tasks in real 3D scenes remains challenging despite recent progress in closed-set and open-set 3D scene understanding methods.",
    "chosen": "This paper introduces SceneFun3D, a dataset with detailed interaction annotations for 3D indoor scenes. The dataset supports tasks like functionality segmentation and 3D motion estimation, highlighting challenges in real-world scene understanding.",
    "rejected": "SceneFun3D introduces a dataset with interaction annotations for real-world 3D indoor scenes, including motion parameters and natural language task descriptions. Novel tasks such as functionality segmentation and task-driven affordance grounding are introduced, showcasing the complexity of interacting with functional elements in 3D environments despite recent progress in scene understanding."
  },
  {
    "prompt": "summary: Recently, building on the foundation of neural radiance ﬁeld, various techniques have emerged to learn unsigned distance ﬁelds (UDF) to reconstruct 3D non-watertight models from multi-view images. Yet, a central challenge in UDF-based volume rendering is formulating a proper way to convert unsigned distance values into volume density, ensuring that the resulting weight function remains unbiased and sensitive to occlusions. Falling short on these requirements often results in incorrect topology or large reconstruction errors in resulting models. This paper addresses this challenge by presenting a novel two-stage algorithm, 2S-UDF, for learning a high-quality UDF from multi-view images. Initially, the method applies an easily trainable density function that, while slightly biased and transparent, aids in coarse reconstruction. The subsequent stage then reﬁnes the geometry and appearance of the object to achieve a high-quality reconstruction by directly adjusting the weight function used in volume rendering to ensure that it is unbiased and occlusion-aware. Decoupling density and weight in two stages makes our training stable and robust, distinguishing our technique from existing UDF learning approaches. Evaluations on the DeepFashion3D, DTU, and BlendedMVS datasets validate the robustness and effectiveness of our proposed approach. In both quantitative metrics and visual quality, the results indicate our superior performance over other UDF learning techniques in reconstructing 3D non-watertight models from multi-view images. Our",
    "chosen": "This paper presents 2S-UDF, a two-stage algorithm for learning unsigned distance fields from multi-view images. The method improves reconstruction of non-watertight models by decoupling density and weight functions, ensuring unbiased and occlusion-aware results.",
    "rejected": "Building on neural radiance fields, this work explores unsigned distance fields (UDFs) for reconstructing non-watertight 3D models. Challenges in converting unsigned distances to volume density are addressed to ensure unbiased weight functions sensitive to occlusions, aiming to improve topology correctness and reduce reconstruction errors."
  }
]